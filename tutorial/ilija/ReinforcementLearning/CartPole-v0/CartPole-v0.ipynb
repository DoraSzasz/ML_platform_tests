{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving CartPole OpenAI environment using DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first install rl package - not done in the container as it requires a bit older version of keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-rl\n",
      "  Downloading keras-rl-0.3.1.tar.gz\n",
      "Collecting keras<2.0.7,>=1.0.7 (from keras-rl)\n",
      "  Downloading Keras-2.0.6.tar.gz (228kB)\n",
      "\u001b[K    100% |################################| 235kB 2.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting theano (from keras<2.0.7,>=1.0.7->keras-rl)\n",
      "  Downloading Theano-0.9.0.tar.gz (3.1MB)\n",
      "\u001b[K    100% |################################| 3.1MB 408kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras<2.0.7,>=1.0.7->keras-rl)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from keras<2.0.7,>=1.0.7->keras-rl)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from theano->keras<2.0.7,>=1.0.7->keras-rl)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from theano->keras<2.0.7,>=1.0.7->keras-rl)\n",
      "Building wheels for collected packages: keras-rl, keras, theano\n",
      "  Running setup.py bdist_wheel for keras-rl ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8b/3f/0e/d0dbbcddddf6d14b412935b2286098872de5464123fdaeb7d9\n",
      "  Running setup.py bdist_wheel for keras ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/80/ba/2beab8c2131e2dcc391ee8a2f55e648af66348115c245e0839\n",
      "  Running setup.py bdist_wheel for theano ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d5/5b/93/433299b86e3e9b25f0f600e4e4ebf18e38eb7534ea518eba13\n",
      "Successfully built keras-rl keras theano\n",
      "Installing collected packages: theano, keras, keras-rl\n",
      "  Found existing installation: Keras 2.0.8\n",
      "    Uninstalling Keras-2.0.8:\n",
      "      Successfully uninstalled Keras-2.0.8\n",
      "Successfully installed keras-2.0.6 keras-rl-0.3.1 theano-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open environment\n",
    "extract the number of actions. \n",
    "There are two discrete actions - move left and move Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure and compile our agent. \n",
    "You can use every built-in Keras optimizer and even the metrics!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, \n",
    "               policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual learning \n",
    "Visualization is OFF untill we figure out how to export display correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate our algorithm for 5 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    23/50000: episode: 1, duration: 0.888s, episode steps: 23, steps per second: 26, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.082 [-2.106, 1.171], loss: 0.480381, mean_absolute_error: 0.504366, mean_q: 0.034857\n",
      "    33/50000: episode: 2, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.157 [-1.129, 1.948], loss: 0.424156, mean_absolute_error: 0.511362, mean_q: 0.114185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    69/50000: episode: 3, duration: 0.275s, episode steps: 36, steps per second: 131, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.134 [-0.535, 0.891], loss: 0.266964, mean_absolute_error: 0.553619, mean_q: 0.425473\n",
      "    82/50000: episode: 4, duration: 0.113s, episode steps: 13, steps per second: 115, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.085 [-1.000, 1.519], loss: 0.114552, mean_absolute_error: 0.623867, mean_q: 0.845792\n",
      "    97/50000: episode: 5, duration: 0.133s, episode steps: 15, steps per second: 113, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.087 [-1.870, 1.028], loss: 0.077943, mean_absolute_error: 0.693653, mean_q: 1.107926\n",
      "   112/50000: episode: 6, duration: 0.130s, episode steps: 15, steps per second: 115, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.100 [-0.979, 1.748], loss: 0.042188, mean_absolute_error: 0.708470, mean_q: 1.267657\n",
      "   133/50000: episode: 7, duration: 0.170s, episode steps: 21, steps per second: 124, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.041 [-2.262, 1.600], loss: 0.033151, mean_absolute_error: 0.772814, mean_q: 1.498734\n",
      "   143/50000: episode: 8, duration: 0.095s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.570, 2.499], loss: 0.036129, mean_absolute_error: 0.822767, mean_q: 1.557098\n",
      "   188/50000: episode: 9, duration: 0.328s, episode steps: 45, steps per second: 137, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.013 [-0.860, 1.140], loss: 0.035296, mean_absolute_error: 0.905708, mean_q: 1.767635\n",
      "   201/50000: episode: 10, duration: 0.132s, episode steps: 13, steps per second: 98, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.087 [-0.804, 1.381], loss: 0.035862, mean_absolute_error: 1.002638, mean_q: 2.021916\n",
      "   212/50000: episode: 11, duration: 0.091s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.110 [-2.148, 1.342], loss: 0.041480, mean_absolute_error: 1.074251, mean_q: 2.109220\n",
      "   225/50000: episode: 12, duration: 0.101s, episode steps: 13, steps per second: 128, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.107 [-1.138, 1.948], loss: 0.044119, mean_absolute_error: 1.103132, mean_q: 2.179727\n",
      "   236/50000: episode: 13, duration: 0.098s, episode steps: 11, steps per second: 113, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.122 [-1.131, 1.758], loss: 0.066068, mean_absolute_error: 1.170821, mean_q: 2.286110\n",
      "   251/50000: episode: 14, duration: 0.137s, episode steps: 15, steps per second: 109, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.088 [-1.493, 0.811], loss: 0.060579, mean_absolute_error: 1.220735, mean_q: 2.385423\n",
      "   276/50000: episode: 15, duration: 0.232s, episode steps: 25, steps per second: 108, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.057 [-0.999, 1.884], loss: 0.073825, mean_absolute_error: 1.323072, mean_q: 2.558878\n",
      "   295/50000: episode: 16, duration: 0.163s, episode steps: 19, steps per second: 117, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.074 [-1.786, 0.997], loss: 0.091436, mean_absolute_error: 1.394463, mean_q: 2.665284\n",
      "   316/50000: episode: 17, duration: 0.198s, episode steps: 21, steps per second: 106, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.106 [-1.186, 0.433], loss: 0.070544, mean_absolute_error: 1.481919, mean_q: 2.903131\n",
      "   333/50000: episode: 18, duration: 0.150s, episode steps: 17, steps per second: 113, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.075 [-1.770, 2.760], loss: 0.116937, mean_absolute_error: 1.551081, mean_q: 2.970392\n",
      "   353/50000: episode: 19, duration: 0.168s, episode steps: 20, steps per second: 119, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.106 [-1.161, 2.141], loss: 0.105410, mean_absolute_error: 1.643697, mean_q: 3.172234\n",
      "   393/50000: episode: 20, duration: 0.358s, episode steps: 40, steps per second: 112, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.096 [-1.664, 0.443], loss: 0.130516, mean_absolute_error: 1.775850, mean_q: 3.401484\n",
      "   431/50000: episode: 21, duration: 0.316s, episode steps: 38, steps per second: 120, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.059 [-1.347, 2.169], loss: 0.133581, mean_absolute_error: 1.932964, mean_q: 3.709701\n",
      "   451/50000: episode: 22, duration: 0.178s, episode steps: 20, steps per second: 112, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.047 [-2.289, 1.547], loss: 0.192765, mean_absolute_error: 2.053558, mean_q: 3.896082\n",
      "   460/50000: episode: 23, duration: 0.084s, episode steps: 9, steps per second: 107, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.338, 2.315], loss: 0.137900, mean_absolute_error: 2.116991, mean_q: 4.056845\n",
      "   472/50000: episode: 24, duration: 0.102s, episode steps: 12, steps per second: 117, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-0.964, 1.524], loss: 0.149561, mean_absolute_error: 2.175057, mean_q: 4.184625\n",
      "   491/50000: episode: 25, duration: 0.159s, episode steps: 19, steps per second: 120, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.044 [-1.386, 2.201], loss: 0.133720, mean_absolute_error: 2.228780, mean_q: 4.307006\n",
      "   500/50000: episode: 26, duration: 0.083s, episode steps: 9, steps per second: 108, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.134 [-1.162, 1.966], loss: 0.238837, mean_absolute_error: 2.282193, mean_q: 4.351767\n",
      "   512/50000: episode: 27, duration: 0.116s, episode steps: 12, steps per second: 103, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.092 [-1.576, 2.466], loss: 0.219899, mean_absolute_error: 2.317382, mean_q: 4.316128\n",
      "   524/50000: episode: 28, duration: 0.106s, episode steps: 12, steps per second: 113, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.099 [-1.230, 2.097], loss: 0.207448, mean_absolute_error: 2.321544, mean_q: 4.275835\n",
      "   538/50000: episode: 29, duration: 0.114s, episode steps: 14, steps per second: 122, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.121 [-0.938, 1.686], loss: 0.222838, mean_absolute_error: 2.432093, mean_q: 4.542125\n",
      "   554/50000: episode: 30, duration: 0.122s, episode steps: 16, steps per second: 132, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.103 [-1.140, 2.111], loss: 0.213523, mean_absolute_error: 2.480207, mean_q: 4.674201\n",
      "   599/50000: episode: 31, duration: 0.343s, episode steps: 45, steps per second: 131, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.167 [-0.973, 0.710], loss: 0.213097, mean_absolute_error: 2.577207, mean_q: 4.860440\n",
      "   613/50000: episode: 32, duration: 0.132s, episode steps: 14, steps per second: 106, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.096 [-1.034, 1.823], loss: 0.237295, mean_absolute_error: 2.688414, mean_q: 5.098756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   625/50000: episode: 33, duration: 0.116s, episode steps: 12, steps per second: 104, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.115 [-1.544, 2.522], loss: 0.272199, mean_absolute_error: 2.751459, mean_q: 5.245492\n",
      "   650/50000: episode: 34, duration: 0.212s, episode steps: 25, steps per second: 118, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.069 [-1.131, 2.025], loss: 0.202475, mean_absolute_error: 2.793996, mean_q: 5.315974\n",
      "   663/50000: episode: 35, duration: 0.114s, episode steps: 13, steps per second: 114, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.102 [-2.371, 1.540], loss: 0.270679, mean_absolute_error: 2.822477, mean_q: 5.306091\n",
      "   699/50000: episode: 36, duration: 0.288s, episode steps: 36, steps per second: 125, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.002 [-1.583, 2.399], loss: 0.208883, mean_absolute_error: 2.940639, mean_q: 5.633710\n",
      "   833/50000: episode: 37, duration: 1.124s, episode steps: 134, steps per second: 119, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.137 [-1.465, 1.355], loss: 0.192481, mean_absolute_error: 3.278823, mean_q: 6.354373\n",
      "   853/50000: episode: 38, duration: 0.182s, episode steps: 20, steps per second: 110, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.573, 1.078], loss: 0.387156, mean_absolute_error: 3.547467, mean_q: 6.818481\n",
      "   869/50000: episode: 39, duration: 0.144s, episode steps: 16, steps per second: 111, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.100 [-2.320, 1.373], loss: 0.324435, mean_absolute_error: 3.692883, mean_q: 7.143034\n",
      "   884/50000: episode: 40, duration: 0.144s, episode steps: 15, steps per second: 104, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.090 [-0.601, 0.977], loss: 0.182886, mean_absolute_error: 3.730724, mean_q: 7.300741\n",
      "   915/50000: episode: 41, duration: 0.239s, episode steps: 31, steps per second: 130, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.007 [-1.848, 1.179], loss: 0.181708, mean_absolute_error: 3.823561, mean_q: 7.555189\n",
      "   972/50000: episode: 42, duration: 0.439s, episode steps: 57, steps per second: 130, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.018 [-1.640, 1.117], loss: 0.243776, mean_absolute_error: 3.999979, mean_q: 7.885211\n",
      "  1007/50000: episode: 43, duration: 0.309s, episode steps: 35, steps per second: 113, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.126 [-0.513, 0.937], loss: 0.239068, mean_absolute_error: 4.153470, mean_q: 8.221423\n",
      "  1053/50000: episode: 44, duration: 0.396s, episode steps: 46, steps per second: 116, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.031 [-1.073, 0.815], loss: 0.210908, mean_absolute_error: 4.361914, mean_q: 8.663843\n",
      "  1069/50000: episode: 45, duration: 0.141s, episode steps: 16, steps per second: 114, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.114 [-0.548, 1.093], loss: 0.451868, mean_absolute_error: 4.520779, mean_q: 8.977947\n",
      "  1115/50000: episode: 46, duration: 0.368s, episode steps: 46, steps per second: 125, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.030 [-0.845, 0.618], loss: 0.346658, mean_absolute_error: 4.625588, mean_q: 9.149490\n",
      "  1140/50000: episode: 47, duration: 0.218s, episode steps: 25, steps per second: 115, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.143 [-0.741, 1.167], loss: 0.348297, mean_absolute_error: 4.762135, mean_q: 9.434134\n",
      "  1161/50000: episode: 48, duration: 0.176s, episode steps: 21, steps per second: 119, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.089 [-1.471, 0.943], loss: 0.336395, mean_absolute_error: 4.840405, mean_q: 9.613131\n",
      "  1241/50000: episode: 49, duration: 0.704s, episode steps: 80, steps per second: 114, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.120 [-1.375, 0.964], loss: 0.440735, mean_absolute_error: 5.094745, mean_q: 10.125135\n",
      "  1260/50000: episode: 50, duration: 0.174s, episode steps: 19, steps per second: 109, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.105 [-1.276, 0.580], loss: 0.467137, mean_absolute_error: 5.281781, mean_q: 10.521479\n",
      "  1288/50000: episode: 51, duration: 0.226s, episode steps: 28, steps per second: 124, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.081 [-0.552, 1.277], loss: 0.596044, mean_absolute_error: 5.361003, mean_q: 10.653383\n",
      "  1306/50000: episode: 52, duration: 0.161s, episode steps: 18, steps per second: 112, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-1.131, 0.562], loss: 0.460963, mean_absolute_error: 5.459152, mean_q: 10.872778\n",
      "  1485/50000: episode: 53, duration: 1.503s, episode steps: 179, steps per second: 119, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: 0.043 [-1.431, 1.478], loss: 0.561880, mean_absolute_error: 5.864525, mean_q: 11.674911\n",
      "  1657/50000: episode: 54, duration: 1.496s, episode steps: 172, steps per second: 115, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.194 [-1.296, 0.885], loss: 0.486796, mean_absolute_error: 6.631365, mean_q: 13.341207\n",
      "  1808/50000: episode: 55, duration: 1.248s, episode steps: 151, steps per second: 121, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.392 [-1.156, 2.310], loss: 0.472349, mean_absolute_error: 7.348260, mean_q: 14.872879\n",
      "  1950/50000: episode: 56, duration: 1.191s, episode steps: 142, steps per second: 119, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.322 [-1.646, 0.935], loss: 0.917456, mean_absolute_error: 8.085100, mean_q: 16.238068\n",
      "  2136/50000: episode: 57, duration: 1.500s, episode steps: 186, steps per second: 124, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.329 [-2.074, 1.088], loss: 0.819206, mean_absolute_error: 8.687321, mean_q: 17.532290\n",
      "  2331/50000: episode: 58, duration: 1.606s, episode steps: 195, steps per second: 121, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.346 [-2.389, 1.083], loss: 0.602123, mean_absolute_error: 9.402234, mean_q: 19.049774\n",
      "  2531/50000: episode: 59, duration: 1.633s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.087 [-1.155, 1.208], loss: 0.803811, mean_absolute_error: 10.201118, mean_q: 20.661013\n",
      "  2731/50000: episode: 60, duration: 1.661s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.202 [-1.537, 1.152], loss: 0.853166, mean_absolute_error: 11.111094, mean_q: 22.608788\n",
      "  2928/50000: episode: 61, duration: 1.634s, episode steps: 197, steps per second: 121, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.341 [-3.174, 1.532], loss: 1.051578, mean_absolute_error: 12.014831, mean_q: 24.435671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3128/50000: episode: 62, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.286 [-1.959, 1.190], loss: 1.078630, mean_absolute_error: 12.793987, mean_q: 26.051723\n",
      "  3328/50000: episode: 63, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.297 [-1.974, 1.000], loss: 1.116100, mean_absolute_error: 13.697876, mean_q: 27.912025\n",
      "  3509/50000: episode: 64, duration: 1.493s, episode steps: 181, steps per second: 121, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.385 [-2.427, 1.048], loss: 1.461792, mean_absolute_error: 14.460250, mean_q: 29.414192\n",
      "  3709/50000: episode: 65, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.291 [-2.062, 1.283], loss: 1.488432, mean_absolute_error: 15.038794, mean_q: 30.632471\n",
      "  3892/50000: episode: 66, duration: 1.558s, episode steps: 183, steps per second: 117, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.335 [-2.057, 1.200], loss: 1.380598, mean_absolute_error: 15.957238, mean_q: 32.472740\n",
      "  4092/50000: episode: 67, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.237 [-1.637, 0.831], loss: 1.185765, mean_absolute_error: 16.660374, mean_q: 33.921234\n",
      "  4292/50000: episode: 68, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.049 [-1.025, 1.129], loss: 1.756055, mean_absolute_error: 17.525196, mean_q: 35.670200\n",
      "  4492/50000: episode: 69, duration: 1.640s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.150 [-1.101, 0.860], loss: 1.635805, mean_absolute_error: 18.404173, mean_q: 37.406116\n",
      "  4692/50000: episode: 70, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.148 [-1.107, 1.150], loss: 1.841439, mean_absolute_error: 19.222837, mean_q: 39.031288\n",
      "  4892/50000: episode: 71, duration: 1.597s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.118 [-1.167, 1.159], loss: 2.594737, mean_absolute_error: 20.171070, mean_q: 40.904385\n",
      "  5092/50000: episode: 72, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.095 [-0.929, 1.143], loss: 3.110992, mean_absolute_error: 20.956837, mean_q: 42.406181\n",
      "  5292/50000: episode: 73, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.241 [-1.658, 1.123], loss: 2.300580, mean_absolute_error: 21.661827, mean_q: 44.005783\n",
      "  5492/50000: episode: 74, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.062 [-1.004, 0.959], loss: 3.901691, mean_absolute_error: 22.383745, mean_q: 45.328732\n",
      "  5692/50000: episode: 75, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.182 [-1.352, 0.884], loss: 2.847599, mean_absolute_error: 23.209951, mean_q: 46.960831\n",
      "  5892/50000: episode: 76, duration: 1.640s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.074 [-0.903, 1.007], loss: 3.702725, mean_absolute_error: 23.732056, mean_q: 47.963520\n",
      "  6092/50000: episode: 77, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.270 [-1.874, 0.846], loss: 3.211821, mean_absolute_error: 24.567520, mean_q: 49.690029\n",
      "  6292/50000: episode: 78, duration: 1.623s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.001 [-0.937, 0.887], loss: 4.145136, mean_absolute_error: 25.296253, mean_q: 51.102020\n",
      "  6492/50000: episode: 79, duration: 1.658s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.259 [-1.785, 0.861], loss: 3.971937, mean_absolute_error: 25.695118, mean_q: 51.950596\n",
      "  6692/50000: episode: 80, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.318 [-2.937, 1.550], loss: 3.976125, mean_absolute_error: 26.337339, mean_q: 53.186832\n",
      "  6892/50000: episode: 81, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.078 [-1.124, 1.255], loss: 3.356560, mean_absolute_error: 26.791348, mean_q: 54.083572\n",
      "  7092/50000: episode: 82, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.158 [-1.164, 1.056], loss: 4.068361, mean_absolute_error: 27.377129, mean_q: 55.302124\n",
      "  7292/50000: episode: 83, duration: 1.474s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.146 [-1.328, 0.893], loss: 4.584283, mean_absolute_error: 28.086069, mean_q: 56.637276\n",
      "  7492/50000: episode: 84, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.060 [-1.238, 1.305], loss: 3.644188, mean_absolute_error: 28.486624, mean_q: 57.563736\n",
      "  7692/50000: episode: 85, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.261 [-1.795, 0.791], loss: 4.484247, mean_absolute_error: 29.108875, mean_q: 58.720291\n",
      "  7892/50000: episode: 86, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.245 [-1.669, 1.344], loss: 4.034356, mean_absolute_error: 29.417881, mean_q: 59.361847\n",
      "  8092/50000: episode: 87, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.170 [-1.187, 1.193], loss: 4.353328, mean_absolute_error: 29.854048, mean_q: 60.179649\n",
      "  8292/50000: episode: 88, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.187 [-1.323, 0.878], loss: 3.548939, mean_absolute_error: 30.312733, mean_q: 61.303925\n",
      "  8492/50000: episode: 89, duration: 1.596s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.204 [-1.418, 1.038], loss: 5.489159, mean_absolute_error: 30.847145, mean_q: 62.227833\n",
      "  8692/50000: episode: 90, duration: 1.517s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.225 [-1.569, 0.816], loss: 3.261078, mean_absolute_error: 31.254498, mean_q: 63.194687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8892/50000: episode: 91, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.342 [-2.910, 1.059], loss: 4.266553, mean_absolute_error: 31.698809, mean_q: 64.039505\n",
      "  9092/50000: episode: 92, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.256 [-1.816, 0.983], loss: 3.675431, mean_absolute_error: 31.975489, mean_q: 64.714554\n",
      "  9292/50000: episode: 93, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.230 [-1.568, 1.051], loss: 4.738186, mean_absolute_error: 32.398472, mean_q: 65.401604\n",
      "  9492/50000: episode: 94, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.251 [-1.721, 0.978], loss: 4.872155, mean_absolute_error: 32.938698, mean_q: 66.406960\n",
      "  9692/50000: episode: 95, duration: 1.675s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-0.938, 1.100], loss: 4.756370, mean_absolute_error: 33.323349, mean_q: 67.158943\n",
      "  9892/50000: episode: 96, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.159 [-1.458, 1.089], loss: 5.793964, mean_absolute_error: 33.515625, mean_q: 67.527550\n",
      " 10092/50000: episode: 97, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.253 [-1.735, 0.798], loss: 4.784479, mean_absolute_error: 33.902206, mean_q: 68.383125\n",
      " 10292/50000: episode: 98, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.102 [-1.006, 0.973], loss: 4.042869, mean_absolute_error: 34.380966, mean_q: 69.392784\n",
      " 10492/50000: episode: 99, duration: 1.586s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.148 [-1.021, 0.974], loss: 4.077186, mean_absolute_error: 34.752113, mean_q: 70.188133\n",
      " 10689/50000: episode: 100, duration: 1.604s, episode steps: 197, steps per second: 123, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.331 [-3.601, 2.263], loss: 5.975301, mean_absolute_error: 35.084206, mean_q: 70.696526\n",
      " 10889/50000: episode: 101, duration: 1.637s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.293 [-2.052, 0.905], loss: 5.097949, mean_absolute_error: 35.182663, mean_q: 70.921036\n",
      " 11089/50000: episode: 102, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.128 [-1.520, 1.170], loss: 4.683949, mean_absolute_error: 35.439259, mean_q: 71.572868\n",
      " 11289/50000: episode: 103, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.069 [-0.860, 0.928], loss: 4.696220, mean_absolute_error: 35.438046, mean_q: 71.511497\n",
      " 11489/50000: episode: 104, duration: 1.611s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.252 [-1.834, 0.954], loss: 8.602753, mean_absolute_error: 35.914444, mean_q: 72.310051\n",
      " 11689/50000: episode: 105, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.177 [-1.238, 0.867], loss: 3.701153, mean_absolute_error: 36.425339, mean_q: 73.548668\n",
      " 26178/50000: episode: 178, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.096 [-1.345, 1.141], loss: 9.550345, mean_absolute_error: 44.257469, mean_q: 88.675819\n",
      " 26378/50000: episode: 179, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-1.110, 1.278], loss: 8.029411, mean_absolute_error: 44.384644, mean_q: 89.212181\n",
      " 26578/50000: episode: 180, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.088 [-1.281, 1.350], loss: 9.765857, mean_absolute_error: 44.492901, mean_q: 89.202820\n",
      " 26778/50000: episode: 181, duration: 1.579s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.116 [-1.127, 1.050], loss: 5.726660, mean_absolute_error: 44.412998, mean_q: 89.222679\n",
      " 26978/50000: episode: 182, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.121 [-1.673, 1.495], loss: 6.655134, mean_absolute_error: 44.614048, mean_q: 89.509209\n",
      " 27178/50000: episode: 183, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-1.374, 1.394], loss: 9.447154, mean_absolute_error: 44.641415, mean_q: 89.420029\n",
      " 27378/50000: episode: 184, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.071 [-1.007, 1.114], loss: 9.334984, mean_absolute_error: 44.893486, mean_q: 90.109436\n",
      " 27578/50000: episode: 185, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.122 [-1.224, 1.231], loss: 9.898459, mean_absolute_error: 44.993603, mean_q: 90.280624\n",
      " 27778/50000: episode: 186, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.072 [-0.966, 1.054], loss: 10.740861, mean_absolute_error: 44.905079, mean_q: 89.990227\n",
      " 27978/50000: episode: 187, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.037 [-1.319, 1.157], loss: 6.477836, mean_absolute_error: 44.723370, mean_q: 89.857147\n",
      " 28178/50000: episode: 188, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-1.225, 1.144], loss: 11.522593, mean_absolute_error: 44.468056, mean_q: 89.204063\n",
      " 28378/50000: episode: 189, duration: 1.657s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.078 [-1.161, 0.944], loss: 11.186758, mean_absolute_error: 44.621334, mean_q: 89.465889\n",
      " 28578/50000: episode: 190, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.138 [-1.321, 1.429], loss: 7.806023, mean_absolute_error: 44.659157, mean_q: 89.676453\n",
      " 28778/50000: episode: 191, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.124 [-1.273, 1.345], loss: 9.422634, mean_absolute_error: 44.790577, mean_q: 89.870453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28978/50000: episode: 192, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.126 [-1.221, 1.154], loss: 5.862950, mean_absolute_error: 44.906303, mean_q: 90.220940\n",
      " 29178/50000: episode: 193, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.131 [-1.466, 1.322], loss: 9.148564, mean_absolute_error: 44.982044, mean_q: 90.212784\n",
      " 29378/50000: episode: 194, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-1.330, 1.309], loss: 13.605254, mean_absolute_error: 45.165493, mean_q: 90.328964\n",
      " 29578/50000: episode: 195, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-1.434, 1.385], loss: 6.778162, mean_absolute_error: 44.576572, mean_q: 89.530487\n",
      " 29778/50000: episode: 196, duration: 1.619s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.146 [-1.691, 1.578], loss: 8.401999, mean_absolute_error: 44.458290, mean_q: 89.127449\n",
      " 29978/50000: episode: 197, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.132 [-1.763, 1.693], loss: 8.902694, mean_absolute_error: 44.459415, mean_q: 89.143028\n",
      " 30178/50000: episode: 198, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.148 [-1.350, 1.307], loss: 7.846933, mean_absolute_error: 44.422588, mean_q: 89.273514\n",
      " 30378/50000: episode: 199, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.146 [-1.454, 1.446], loss: 9.814214, mean_absolute_error: 44.501999, mean_q: 89.281044\n",
      " 30578/50000: episode: 200, duration: 1.594s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.160 [-1.667, 1.941], loss: 6.312714, mean_absolute_error: 44.486275, mean_q: 89.306374\n",
      " 30735/50000: episode: 201, duration: 1.172s, episode steps: 157, steps per second: 134, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.278 [-1.593, 2.170], loss: 7.513536, mean_absolute_error: 44.565586, mean_q: 89.513176\n",
      " 30876/50000: episode: 202, duration: 1.170s, episode steps: 141, steps per second: 120, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.239 [-1.731, 1.848], loss: 7.571375, mean_absolute_error: 44.403587, mean_q: 89.279938\n",
      " 31076/50000: episode: 203, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.165 [-1.735, 1.710], loss: 9.772402, mean_absolute_error: 44.570362, mean_q: 89.301193\n",
      " 31255/50000: episode: 204, duration: 1.424s, episode steps: 179, steps per second: 126, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.217 [-1.574, 2.482], loss: 11.602152, mean_absolute_error: 44.270935, mean_q: 88.742020\n",
      " 31403/50000: episode: 205, duration: 1.176s, episode steps: 148, steps per second: 126, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.242 [-1.584, 1.869], loss: 7.247386, mean_absolute_error: 44.357189, mean_q: 89.064522\n",
      " 31603/50000: episode: 206, duration: 1.661s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.195 [-1.544, 1.572], loss: 10.669141, mean_absolute_error: 44.454079, mean_q: 89.216698\n",
      " 31672/50000: episode: 207, duration: 0.592s, episode steps: 69, steps per second: 117, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.108 [-1.449, 1.391], loss: 14.965694, mean_absolute_error: 44.592010, mean_q: 89.215485\n",
      " 31872/50000: episode: 208, duration: 1.620s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.151 [-1.443, 1.332], loss: 6.276244, mean_absolute_error: 43.770576, mean_q: 87.843185\n",
      " 32072/50000: episode: 209, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.196 [-1.532, 1.567], loss: 7.659140, mean_absolute_error: 43.841637, mean_q: 87.977814\n",
      " 32149/50000: episode: 210, duration: 0.641s, episode steps: 77, steps per second: 120, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.106 [-1.058, 1.460], loss: 9.722555, mean_absolute_error: 44.271576, mean_q: 88.868622\n",
      " 32298/50000: episode: 211, duration: 1.149s, episode steps: 149, steps per second: 130, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: 0.150 [-1.530, 1.719], loss: 14.389722, mean_absolute_error: 43.884491, mean_q: 87.588814\n",
      " 32498/50000: episode: 212, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.176 [-1.956, 2.048], loss: 9.498340, mean_absolute_error: 43.520340, mean_q: 87.123184\n",
      " 32698/50000: episode: 213, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.119 [-1.350, 1.367], loss: 8.768002, mean_absolute_error: 43.292355, mean_q: 86.771202\n",
      " 32898/50000: episode: 214, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.145 [-1.582, 1.541], loss: 4.980330, mean_absolute_error: 43.410763, mean_q: 87.219879\n",
      " 33077/50000: episode: 215, duration: 1.383s, episode steps: 179, steps per second: 129, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.180 [-1.814, 2.127], loss: 9.318861, mean_absolute_error: 43.668777, mean_q: 87.520058\n",
      " 33277/50000: episode: 216, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-1.616, 1.528], loss: 6.599746, mean_absolute_error: 43.123116, mean_q: 86.480705\n",
      " 33477/50000: episode: 217, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.099 [-1.927, 1.885], loss: 9.575774, mean_absolute_error: 43.007870, mean_q: 85.976074\n",
      " 33677/50000: episode: 218, duration: 1.570s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-1.455, 1.473], loss: 7.300550, mean_absolute_error: 42.966782, mean_q: 86.179123\n",
      " 33877/50000: episode: 219, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.120 [-1.993, 1.923], loss: 7.841733, mean_absolute_error: 42.514938, mean_q: 85.233475\n",
      " 34077/50000: episode: 220, duration: 1.548s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.093 [-1.510, 1.593], loss: 7.723604, mean_absolute_error: 42.928787, mean_q: 85.926697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34181/50000: episode: 221, duration: 0.775s, episode steps: 104, steps per second: 134, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.089 [-1.538, 1.785], loss: 9.704447, mean_absolute_error: 43.207153, mean_q: 86.547768\n",
      " 34381/50000: episode: 222, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.067 [-1.764, 1.521], loss: 4.814377, mean_absolute_error: 42.531528, mean_q: 85.288162\n",
      " 34581/50000: episode: 223, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.040 [-1.723, 1.575], loss: 7.417551, mean_absolute_error: 42.578915, mean_q: 85.215775\n",
      " 34781/50000: episode: 224, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.033 [-1.746, 1.512], loss: 6.504939, mean_absolute_error: 42.559357, mean_q: 85.271721\n",
      " 34981/50000: episode: 225, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.061 [-1.550, 1.447], loss: 7.238333, mean_absolute_error: 42.610943, mean_q: 85.396423\n",
      " 35181/50000: episode: 226, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.073 [-1.996, 1.909], loss: 8.284748, mean_absolute_error: 42.446308, mean_q: 85.019035\n",
      " 35381/50000: episode: 227, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.052 [-1.863, 1.748], loss: 7.244917, mean_absolute_error: 42.421341, mean_q: 85.009842\n",
      " 35581/50000: episode: 228, duration: 1.531s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.013 [-1.540, 1.328], loss: 10.495950, mean_absolute_error: 42.745922, mean_q: 85.434387\n",
      " 35781/50000: episode: 229, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-1.813, 1.739], loss: 7.163285, mean_absolute_error: 42.573429, mean_q: 85.254013\n",
      " 35981/50000: episode: 230, duration: 1.643s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-1.662, 1.491], loss: 7.742366, mean_absolute_error: 42.339478, mean_q: 84.836578\n",
      " 36181/50000: episode: 231, duration: 1.533s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.025 [-1.595, 1.476], loss: 8.121723, mean_absolute_error: 42.455761, mean_q: 84.978722\n",
      " 36381/50000: episode: 232, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.052 [-1.832, 1.568], loss: 8.676441, mean_absolute_error: 42.391773, mean_q: 84.959190\n",
      " 36581/50000: episode: 233, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.037 [-1.547, 1.426], loss: 6.698259, mean_absolute_error: 42.516640, mean_q: 85.161369\n",
      " 36781/50000: episode: 234, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.057 [-1.781, 1.788], loss: 10.875158, mean_absolute_error: 42.285858, mean_q: 84.466599\n",
      " 36981/50000: episode: 235, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.052 [-1.678, 1.504], loss: 9.100025, mean_absolute_error: 42.381310, mean_q: 84.918007\n",
      " 37181/50000: episode: 236, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.051 [-2.042, 1.862], loss: 3.706640, mean_absolute_error: 42.536449, mean_q: 85.460724\n",
      " 37381/50000: episode: 237, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.020 [-1.563, 1.258], loss: 7.597517, mean_absolute_error: 42.548740, mean_q: 85.177109\n",
      " 37581/50000: episode: 238, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-1.729, 1.490], loss: 8.219135, mean_absolute_error: 42.459538, mean_q: 84.954300\n",
      " 37781/50000: episode: 239, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.016 [-1.907, 1.691], loss: 11.556870, mean_absolute_error: 42.341934, mean_q: 84.608612\n",
      " 37981/50000: episode: 240, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.022 [-1.465, 1.479], loss: 5.166625, mean_absolute_error: 42.412159, mean_q: 84.968636\n",
      " 38181/50000: episode: 241, duration: 1.593s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.006 [-1.648, 1.341], loss: 8.381668, mean_absolute_error: 42.276348, mean_q: 84.736115\n",
      " 38381/50000: episode: 242, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-1.339, 1.335], loss: 11.091989, mean_absolute_error: 42.061451, mean_q: 84.272224\n",
      " 38581/50000: episode: 243, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-1.118, 1.073], loss: 9.390310, mean_absolute_error: 42.468781, mean_q: 85.135490\n",
      " 38781/50000: episode: 244, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-1.677, 1.630], loss: 10.003736, mean_absolute_error: 42.152985, mean_q: 84.503021\n",
      " 38981/50000: episode: 245, duration: 1.621s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-1.222, 1.163], loss: 7.857733, mean_absolute_error: 42.176502, mean_q: 84.640114\n",
      " 39181/50000: episode: 246, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.005 [-1.492, 1.119], loss: 9.345205, mean_absolute_error: 42.415703, mean_q: 85.101837\n",
      " 39381/50000: episode: 247, duration: 1.685s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.012 [-1.274, 1.219], loss: 10.132733, mean_absolute_error: 42.426723, mean_q: 85.057518\n",
      " 39581/50000: episode: 248, duration: 1.575s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-1.552, 1.345], loss: 15.206707, mean_absolute_error: 42.440899, mean_q: 85.097145\n",
      " 39781/50000: episode: 249, duration: 1.611s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.015 [-0.997, 0.915], loss: 7.657604, mean_absolute_error: 42.630089, mean_q: 85.755959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39981/50000: episode: 250, duration: 1.614s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-1.244, 1.253], loss: 7.819655, mean_absolute_error: 42.755814, mean_q: 85.880684\n",
      " 40102/50000: episode: 251, duration: 0.927s, episode steps: 121, steps per second: 131, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.017 [-1.534, 1.435], loss: 7.563911, mean_absolute_error: 42.735138, mean_q: 85.934830\n",
      " 40302/50000: episode: 252, duration: 1.597s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.020 [-1.639, 1.715], loss: 11.181011, mean_absolute_error: 42.948872, mean_q: 86.051338\n",
      " 40502/50000: episode: 253, duration: 1.609s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.042 [-1.677, 1.579], loss: 11.956221, mean_absolute_error: 43.055138, mean_q: 86.170784\n",
      " 40702/50000: episode: 254, duration: 1.596s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.020 [-1.677, 1.680], loss: 11.052578, mean_absolute_error: 42.986969, mean_q: 86.122597\n",
      " 40902/50000: episode: 255, duration: 1.597s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.011 [-1.655, 1.381], loss: 7.994956, mean_absolute_error: 43.009834, mean_q: 86.383751\n",
      " 41102/50000: episode: 256, duration: 1.583s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.011 [-1.445, 1.668], loss: 10.140103, mean_absolute_error: 43.287140, mean_q: 86.933151\n",
      " 41302/50000: episode: 257, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.027 [-1.454, 1.475], loss: 7.522456, mean_absolute_error: 43.396523, mean_q: 87.101425\n",
      " 41502/50000: episode: 258, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.025 [-1.528, 1.361], loss: 6.707426, mean_absolute_error: 43.307987, mean_q: 87.148689\n",
      " 41702/50000: episode: 259, duration: 1.628s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.014 [-1.358, 1.195], loss: 11.382033, mean_absolute_error: 43.488663, mean_q: 87.312256\n",
      " 41902/50000: episode: 260, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-1.249, 1.285], loss: 14.747253, mean_absolute_error: 43.672558, mean_q: 87.529465\n",
      " 42102/50000: episode: 261, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.034 [-1.670, 1.515], loss: 8.204867, mean_absolute_error: 43.418102, mean_q: 87.274925\n",
      " 42302/50000: episode: 262, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.009 [-1.372, 1.586], loss: 9.159341, mean_absolute_error: 43.236446, mean_q: 86.814568\n",
      " 42502/50000: episode: 263, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-1.510, 1.519], loss: 12.684354, mean_absolute_error: 43.536888, mean_q: 87.181084\n",
      " 42702/50000: episode: 264, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.017 [-1.343, 1.131], loss: 5.326567, mean_absolute_error: 43.169777, mean_q: 86.997620\n",
      " 42902/50000: episode: 265, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.029 [-1.492, 1.524], loss: 9.647020, mean_absolute_error: 43.290558, mean_q: 86.990921\n",
      " 43102/50000: episode: 266, duration: 1.576s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-1.525, 1.618], loss: 6.365539, mean_absolute_error: 43.264496, mean_q: 87.002098\n",
      " 43302/50000: episode: 267, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-1.361, 1.572], loss: 10.302962, mean_absolute_error: 43.612473, mean_q: 87.550545\n",
      " 43502/50000: episode: 268, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.014 [-1.468, 1.465], loss: 7.858966, mean_absolute_error: 43.896347, mean_q: 88.138222\n",
      " 43702/50000: episode: 269, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-1.725, 1.511], loss: 10.905591, mean_absolute_error: 43.737110, mean_q: 87.516266\n",
      " 43902/50000: episode: 270, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.008 [-1.500, 1.341], loss: 8.504398, mean_absolute_error: 43.462372, mean_q: 87.278633\n",
      " 44102/50000: episode: 271, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.004 [-1.136, 1.299], loss: 6.755809, mean_absolute_error: 43.885204, mean_q: 88.154083\n",
      " 44302/50000: episode: 272, duration: 1.576s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-1.651, 1.370], loss: 12.828403, mean_absolute_error: 43.485859, mean_q: 87.225761\n",
      " 44502/50000: episode: 273, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-1.188, 1.229], loss: 14.203910, mean_absolute_error: 43.646748, mean_q: 87.556328\n",
      " 44702/50000: episode: 274, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-1.376, 1.622], loss: 7.939977, mean_absolute_error: 43.270905, mean_q: 87.017471\n",
      " 44902/50000: episode: 275, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.015 [-1.123, 1.069], loss: 8.991491, mean_absolute_error: 43.589981, mean_q: 87.710167\n",
      " 45102/50000: episode: 276, duration: 1.551s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-1.379, 1.567], loss: 16.110693, mean_absolute_error: 43.857159, mean_q: 87.873634\n",
      " 45302/50000: episode: 277, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.015 [-1.223, 1.217], loss: 11.749481, mean_absolute_error: 43.580845, mean_q: 87.658318\n",
      " 45502/50000: episode: 278, duration: 1.593s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.005 [-1.266, 1.312], loss: 12.249150, mean_absolute_error: 43.451172, mean_q: 87.358849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45702/50000: episode: 279, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.004 [-1.584, 1.317], loss: 17.094990, mean_absolute_error: 43.585098, mean_q: 87.273415\n",
      " 45902/50000: episode: 280, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.004 [-1.133, 1.261], loss: 15.279386, mean_absolute_error: 43.371414, mean_q: 87.185440\n",
      " 46102/50000: episode: 281, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.007 [-1.240, 1.478], loss: 9.960597, mean_absolute_error: 43.395546, mean_q: 87.231369\n",
      " 46292/50000: episode: 282, duration: 1.531s, episode steps: 190, steps per second: 124, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.287 [-1.992, 1.591], loss: 12.990545, mean_absolute_error: 43.099674, mean_q: 86.617477\n",
      " 46492/50000: episode: 283, duration: 1.650s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.023 [-1.602, 1.298], loss: 15.007224, mean_absolute_error: 43.241653, mean_q: 86.846619\n",
      " 46692/50000: episode: 284, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.010 [-1.305, 1.136], loss: 9.980779, mean_absolute_error: 43.517002, mean_q: 87.482109\n",
      " 46892/50000: episode: 285, duration: 1.640s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.001 [-1.320, 1.392], loss: 15.036836, mean_absolute_error: 43.266251, mean_q: 86.587044\n",
      " 47092/50000: episode: 286, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.007 [-1.568, 1.698], loss: 11.703453, mean_absolute_error: 43.211281, mean_q: 86.513298\n",
      " 47292/50000: episode: 287, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.000 [-1.542, 1.306], loss: 12.207749, mean_absolute_error: 43.029396, mean_q: 86.344307\n",
      " 47492/50000: episode: 288, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.023 [-1.290, 1.326], loss: 11.780591, mean_absolute_error: 43.245560, mean_q: 86.785301\n",
      " 47692/50000: episode: 289, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.009 [-1.124, 1.009], loss: 10.505725, mean_absolute_error: 43.006981, mean_q: 86.463516\n",
      " 47892/50000: episode: 290, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.008 [-1.158, 1.218], loss: 12.428069, mean_absolute_error: 43.172276, mean_q: 86.568581\n",
      " 48078/50000: episode: 291, duration: 1.500s, episode steps: 186, steps per second: 124, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.181 [-3.006, 3.132], loss: 9.643704, mean_absolute_error: 43.011105, mean_q: 86.497536\n",
      " 48278/50000: episode: 292, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-1.816, 1.516], loss: 11.906681, mean_absolute_error: 42.886162, mean_q: 86.123398\n",
      " 48478/50000: episode: 293, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-1.577, 1.563], loss: 17.427589, mean_absolute_error: 42.879757, mean_q: 85.941017\n",
      " 48678/50000: episode: 294, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.001 [-1.412, 1.461], loss: 14.117014, mean_absolute_error: 43.117565, mean_q: 86.467751\n",
      " 48878/50000: episode: 295, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.008 [-2.035, 1.805], loss: 8.560978, mean_absolute_error: 42.751652, mean_q: 85.944672\n",
      " 49078/50000: episode: 296, duration: 1.653s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.024 [-1.750, 1.547], loss: 20.093878, mean_absolute_error: 42.835850, mean_q: 85.480995\n",
      " 49278/50000: episode: 297, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.010 [-1.568, 1.561], loss: 10.777095, mean_absolute_error: 42.558670, mean_q: 85.490471\n",
      " 49358/50000: episode: 298, duration: 0.613s, episode steps: 80, steps per second: 130, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.058 [-1.427, 1.405], loss: 9.387686, mean_absolute_error: 42.576180, mean_q: 85.429153\n",
      " 49558/50000: episode: 299, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-1.727, 1.424], loss: 12.159245, mean_absolute_error: 42.638927, mean_q: 85.362617\n",
      " 49721/50000: episode: 300, duration: 1.269s, episode steps: 163, steps per second: 128, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.166 [-2.816, 2.851], loss: 12.007652, mean_absolute_error: 42.889950, mean_q: 85.835030\n",
      " 49921/50000: episode: 301, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.028 [-1.841, 1.533], loss: 15.362122, mean_absolute_error: 42.661591, mean_q: 85.476952\n",
      "done, took 405.392 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01f31210b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
