{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.693607, acc: 0.529297]  [A loss: 1.174153, acc: 0.000000]\n",
      "1: [D loss: 0.651737, acc: 0.917969]  [A loss: 1.089066, acc: 0.000000]\n",
      "2: [D loss: 0.552805, acc: 1.000000]  [A loss: 1.318614, acc: 0.000000]\n",
      "3: [D loss: 0.360224, acc: 1.000000]  [A loss: 1.797575, acc: 0.000000]\n",
      "4: [D loss: 0.184098, acc: 1.000000]  [A loss: 0.022982, acc: 1.000000]\n",
      "5: [D loss: 0.156791, acc: 0.996094]  [A loss: 3.613575, acc: 0.000000]\n",
      "6: [D loss: 0.117008, acc: 0.966797]  [A loss: 0.068707, acc: 1.000000]\n",
      "7: [D loss: 0.037135, acc: 1.000000]  [A loss: 0.023453, acc: 1.000000]\n",
      "8: [D loss: 0.029366, acc: 1.000000]  [A loss: 0.013513, acc: 1.000000]\n",
      "9: [D loss: 0.022627, acc: 1.000000]  [A loss: 0.015812, acc: 1.000000]\n",
      "10: [D loss: 0.022074, acc: 1.000000]  [A loss: 0.012944, acc: 1.000000]\n",
      "11: [D loss: 0.020586, acc: 1.000000]  [A loss: 0.011108, acc: 1.000000]\n",
      "12: [D loss: 0.019451, acc: 1.000000]  [A loss: 0.017755, acc: 1.000000]\n",
      "13: [D loss: 0.020047, acc: 0.998047]  [A loss: 0.016404, acc: 1.000000]\n",
      "14: [D loss: 0.020596, acc: 1.000000]  [A loss: 0.014780, acc: 1.000000]\n",
      "15: [D loss: 0.020093, acc: 1.000000]  [A loss: 0.042528, acc: 1.000000]\n",
      "16: [D loss: 0.029234, acc: 1.000000]  [A loss: 0.032262, acc: 1.000000]\n",
      "17: [D loss: 0.018979, acc: 1.000000]  [A loss: 0.006340, acc: 1.000000]\n",
      "18: [D loss: 0.016341, acc: 0.998047]  [A loss: 0.000131, acc: 1.000000]\n",
      "19: [D loss: 0.014144, acc: 1.000000]  [A loss: 0.001642, acc: 1.000000]\n",
      "20: [D loss: 0.011624, acc: 1.000000]  [A loss: 0.000699, acc: 1.000000]\n",
      "21: [D loss: 0.009069, acc: 1.000000]  [A loss: 0.002689, acc: 1.000000]\n",
      "22: [D loss: 0.010298, acc: 0.998047]  [A loss: 0.000281, acc: 1.000000]\n",
      "23: [D loss: 0.007861, acc: 1.000000]  [A loss: 0.007179, acc: 1.000000]\n",
      "24: [D loss: 0.008396, acc: 1.000000]  [A loss: 0.002067, acc: 1.000000]\n",
      "25: [D loss: 0.006272, acc: 1.000000]  [A loss: 0.000415, acc: 1.000000]\n",
      "26: [D loss: 0.004766, acc: 1.000000]  [A loss: 0.002435, acc: 1.000000]\n",
      "27: [D loss: 0.007168, acc: 0.998047]  [A loss: 0.000005, acc: 1.000000]\n",
      "28: [D loss: 0.005793, acc: 1.000000]  [A loss: 0.001618, acc: 1.000000]\n",
      "29: [D loss: 0.003271, acc: 1.000000]  [A loss: 0.000671, acc: 1.000000]\n",
      "30: [D loss: 0.002761, acc: 1.000000]  [A loss: 0.000226, acc: 1.000000]\n",
      "31: [D loss: 0.002237, acc: 1.000000]  [A loss: 0.000367, acc: 1.000000]\n",
      "32: [D loss: 0.003188, acc: 1.000000]  [A loss: 0.000000, acc: 1.000000]\n",
      "33: [D loss: 0.003485, acc: 1.000000]  [A loss: 0.000151, acc: 1.000000]\n",
      "34: [D loss: 0.001439, acc: 1.000000]  [A loss: 0.003799, acc: 0.996094]\n",
      "35: [D loss: 0.003288, acc: 1.000000]  [A loss: 0.007857, acc: 0.996094]\n",
      "36: [D loss: 0.010670, acc: 1.000000]  [A loss: 9.293716, acc: 0.000000]\n",
      "37: [D loss: 0.786883, acc: 0.677734]  [A loss: 16.118095, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38: [D loss: 6.769485, acc: 0.500000]  [A loss: 0.000015, acc: 1.000000]\n",
      "39: [D loss: 0.039925, acc: 0.996094]  [A loss: 0.000399, acc: 1.000000]\n",
      "40: [D loss: 0.023369, acc: 0.996094]  [A loss: 0.004925, acc: 1.000000]\n",
      "41: [D loss: 0.022527, acc: 1.000000]  [A loss: 0.040002, acc: 0.996094]\n",
      "42: [D loss: 0.027577, acc: 0.996094]  [A loss: 0.097318, acc: 0.964844]\n",
      "43: [D loss: 0.091679, acc: 0.970703]  [A loss: 3.833979, acc: 0.066406]\n",
      "44: [D loss: 2.493423, acc: 0.509766]  [A loss: 16.101875, acc: 0.000000]\n",
      "45: [D loss: 2.602032, acc: 0.500000]  [A loss: 0.024326, acc: 1.000000]\n",
      "46: [D loss: 0.530208, acc: 0.718750]  [A loss: 8.002487, acc: 0.000000]\n",
      "47: [D loss: 0.590704, acc: 0.652344]  [A loss: 9.677189, acc: 0.000000]\n",
      "48: [D loss: 0.172872, acc: 0.970703]  [A loss: 4.136106, acc: 0.000000]\n",
      "49: [D loss: 0.772182, acc: 0.593750]  [A loss: 9.329585, acc: 0.000000]\n",
      "50: [D loss: 0.190000, acc: 0.953125]  [A loss: 2.512659, acc: 0.031250]\n",
      "51: [D loss: 0.961534, acc: 0.525391]  [A loss: 9.630148, acc: 0.000000]\n",
      "52: [D loss: 0.305171, acc: 0.863281]  [A loss: 2.032343, acc: 0.035156]\n",
      "53: [D loss: 0.957617, acc: 0.515625]  [A loss: 9.081152, acc: 0.000000]\n",
      "54: [D loss: 0.365102, acc: 0.822266]  [A loss: 2.096219, acc: 0.011719]\n",
      "55: [D loss: 0.748157, acc: 0.539062]  [A loss: 7.579594, acc: 0.000000]\n",
      "56: [D loss: 0.235009, acc: 0.919922]  [A loss: 2.761888, acc: 0.000000]\n",
      "57: [D loss: 0.518402, acc: 0.650391]  [A loss: 6.449852, acc: 0.000000]\n",
      "58: [D loss: 0.136916, acc: 0.976562]  [A loss: 3.044966, acc: 0.000000]\n",
      "59: [D loss: 0.409927, acc: 0.761719]  [A loss: 5.785859, acc: 0.000000]\n",
      "60: [D loss: 0.139374, acc: 0.970703]  [A loss: 2.917116, acc: 0.000000]\n",
      "61: [D loss: 0.433262, acc: 0.687500]  [A loss: 6.085495, acc: 0.000000]\n",
      "62: [D loss: 0.144192, acc: 0.984375]  [A loss: 2.894666, acc: 0.000000]\n",
      "63: [D loss: 0.360404, acc: 0.773438]  [A loss: 5.430661, acc: 0.000000]\n",
      "64: [D loss: 0.128360, acc: 0.984375]  [A loss: 2.919935, acc: 0.000000]\n",
      "65: [D loss: 0.355145, acc: 0.796875]  [A loss: 5.277598, acc: 0.000000]\n",
      "66: [D loss: 0.133829, acc: 0.984375]  [A loss: 3.032577, acc: 0.000000]\n",
      "67: [D loss: 0.286594, acc: 0.876953]  [A loss: 4.347864, acc: 0.000000]\n",
      "68: [D loss: 0.150393, acc: 0.994141]  [A loss: 3.219695, acc: 0.000000]\n",
      "69: [D loss: 0.268399, acc: 0.904297]  [A loss: 3.999717, acc: 0.000000]\n",
      "70: [D loss: 0.164225, acc: 0.994141]  [A loss: 2.920804, acc: 0.000000]\n",
      "71: [D loss: 0.257891, acc: 0.912109]  [A loss: 3.901842, acc: 0.000000]\n",
      "72: [D loss: 0.159123, acc: 0.992188]  [A loss: 2.611612, acc: 0.000000]\n",
      "73: [D loss: 0.314398, acc: 0.845703]  [A loss: 4.287854, acc: 0.000000]\n",
      "74: [D loss: 0.179155, acc: 0.988281]  [A loss: 2.028364, acc: 0.007812]\n",
      "75: [D loss: 0.450789, acc: 0.712891]  [A loss: 4.918257, acc: 0.000000]\n",
      "76: [D loss: 0.264508, acc: 0.910156]  [A loss: 1.500091, acc: 0.070312]\n",
      "77: [D loss: 0.487409, acc: 0.677734]  [A loss: 4.151285, acc: 0.000000]\n",
      "78: [D loss: 0.234459, acc: 0.935547]  [A loss: 1.579866, acc: 0.039062]\n",
      "79: [D loss: 0.418114, acc: 0.738281]  [A loss: 3.500802, acc: 0.000000]\n",
      "80: [D loss: 0.235567, acc: 0.953125]  [A loss: 1.649480, acc: 0.039062]\n",
      "81: [D loss: 0.389251, acc: 0.761719]  [A loss: 3.340264, acc: 0.000000]\n",
      "82: [D loss: 0.218075, acc: 0.972656]  [A loss: 1.611418, acc: 0.027344]\n",
      "83: [D loss: 0.408799, acc: 0.755859]  [A loss: 3.778063, acc: 0.000000]\n",
      "84: [D loss: 0.327529, acc: 0.894531]  [A loss: 1.058885, acc: 0.218750]\n",
      "85: [D loss: 0.646379, acc: 0.609375]  [A loss: 4.211707, acc: 0.000000]\n",
      "86: [D loss: 0.491049, acc: 0.763672]  [A loss: 0.921295, acc: 0.312500]\n",
      "87: [D loss: 0.541242, acc: 0.632812]  [A loss: 2.659332, acc: 0.000000]\n",
      "88: [D loss: 0.281826, acc: 0.919922]  [A loss: 1.294110, acc: 0.097656]\n",
      "89: [D loss: 0.357133, acc: 0.792969]  [A loss: 2.353273, acc: 0.000000]\n",
      "90: [D loss: 0.265468, acc: 0.941406]  [A loss: 1.532485, acc: 0.039062]\n",
      "91: [D loss: 0.345617, acc: 0.835938]  [A loss: 2.578158, acc: 0.000000]\n",
      "92: [D loss: 0.271892, acc: 0.947266]  [A loss: 1.375897, acc: 0.089844]\n",
      "93: [D loss: 0.451642, acc: 0.732422]  [A loss: 3.419585, acc: 0.000000]\n",
      "94: [D loss: 0.447191, acc: 0.824219]  [A loss: 0.692443, acc: 0.523438]\n",
      "95: [D loss: 0.692148, acc: 0.570312]  [A loss: 2.918572, acc: 0.000000]\n",
      "96: [D loss: 0.409026, acc: 0.845703]  [A loss: 0.915412, acc: 0.308594]\n",
      "97: [D loss: 0.492979, acc: 0.681641]  [A loss: 2.062915, acc: 0.000000]\n",
      "98: [D loss: 0.334660, acc: 0.908203]  [A loss: 1.152756, acc: 0.187500]\n",
      "99: [D loss: 0.447506, acc: 0.716797]  [A loss: 2.123195, acc: 0.000000]\n",
      "100: [D loss: 0.331085, acc: 0.912109]  [A loss: 1.185895, acc: 0.117188]\n",
      "101: [D loss: 0.451041, acc: 0.724609]  [A loss: 2.320405, acc: 0.000000]\n",
      "102: [D loss: 0.366220, acc: 0.898438]  [A loss: 0.911633, acc: 0.292969]\n",
      "103: [D loss: 0.570458, acc: 0.607422]  [A loss: 2.724126, acc: 0.000000]\n",
      "104: [D loss: 0.428290, acc: 0.869141]  [A loss: 0.846327, acc: 0.363281]\n",
      "105: [D loss: 0.611394, acc: 0.582031]  [A loss: 2.308292, acc: 0.000000]\n",
      "106: [D loss: 0.425710, acc: 0.878906]  [A loss: 0.947881, acc: 0.218750]\n",
      "107: [D loss: 0.520495, acc: 0.601562]  [A loss: 1.834536, acc: 0.000000]\n",
      "108: [D loss: 0.416378, acc: 0.886719]  [A loss: 1.079946, acc: 0.121094]\n",
      "109: [D loss: 0.511323, acc: 0.646484]  [A loss: 2.054650, acc: 0.000000]\n",
      "110: [D loss: 0.440987, acc: 0.859375]  [A loss: 0.972766, acc: 0.175781]\n",
      "111: [D loss: 0.547799, acc: 0.574219]  [A loss: 2.169463, acc: 0.000000]\n",
      "112: [D loss: 0.457741, acc: 0.875000]  [A loss: 0.942951, acc: 0.191406]\n",
      "113: [D loss: 0.618021, acc: 0.539062]  [A loss: 2.225428, acc: 0.000000]\n",
      "114: [D loss: 0.497073, acc: 0.841797]  [A loss: 0.721101, acc: 0.492188]\n",
      "115: [D loss: 0.697026, acc: 0.511719]  [A loss: 1.870046, acc: 0.000000]\n",
      "116: [D loss: 0.478459, acc: 0.865234]  [A loss: 0.846377, acc: 0.242188]\n",
      "117: [D loss: 0.598220, acc: 0.537109]  [A loss: 1.501311, acc: 0.000000]\n",
      "118: [D loss: 0.460771, acc: 0.837891]  [A loss: 1.146785, acc: 0.027344]\n",
      "119: [D loss: 0.527545, acc: 0.623047]  [A loss: 1.651486, acc: 0.000000]\n",
      "120: [D loss: 0.460150, acc: 0.835938]  [A loss: 1.137532, acc: 0.046875]\n",
      "121: [D loss: 0.550528, acc: 0.589844]  [A loss: 2.068335, acc: 0.000000]\n",
      "122: [D loss: 0.506889, acc: 0.843750]  [A loss: 0.824793, acc: 0.312500]\n",
      "123: [D loss: 0.711207, acc: 0.505859]  [A loss: 2.191496, acc: 0.000000]\n",
      "124: [D loss: 0.538112, acc: 0.810547]  [A loss: 0.700593, acc: 0.488281]\n",
      "125: [D loss: 0.678268, acc: 0.509766]  [A loss: 1.487973, acc: 0.000000]\n",
      "126: [D loss: 0.502733, acc: 0.826172]  [A loss: 1.079152, acc: 0.031250]\n",
      "127: [D loss: 0.560313, acc: 0.601562]  [A loss: 1.530138, acc: 0.000000]\n",
      "128: [D loss: 0.526626, acc: 0.746094]  [A loss: 1.113534, acc: 0.035156]\n",
      "129: [D loss: 0.557066, acc: 0.613281]  [A loss: 1.732527, acc: 0.000000]\n",
      "130: [D loss: 0.502096, acc: 0.826172]  [A loss: 1.039725, acc: 0.035156]\n",
      "131: [D loss: 0.613401, acc: 0.535156]  [A loss: 2.330336, acc: 0.000000]\n",
      "132: [D loss: 0.562063, acc: 0.779297]  [A loss: 0.539463, acc: 0.835938]\n",
      "133: [D loss: 0.818086, acc: 0.500000]  [A loss: 1.670038, acc: 0.000000]\n",
      "134: [D loss: 0.552608, acc: 0.822266]  [A loss: 0.765650, acc: 0.324219]\n",
      "135: [D loss: 0.632294, acc: 0.519531]  [A loss: 1.285750, acc: 0.000000]\n",
      "136: [D loss: 0.548094, acc: 0.720703]  [A loss: 1.138246, acc: 0.019531]\n",
      "137: [D loss: 0.576180, acc: 0.607422]  [A loss: 1.515175, acc: 0.000000]\n",
      "138: [D loss: 0.548609, acc: 0.720703]  [A loss: 1.132780, acc: 0.023438]\n",
      "139: [D loss: 0.589800, acc: 0.572266]  [A loss: 1.810070, acc: 0.000000]\n",
      "140: [D loss: 0.522768, acc: 0.855469]  [A loss: 0.745503, acc: 0.406250]\n",
      "141: [D loss: 0.740784, acc: 0.505859]  [A loss: 2.420677, acc: 0.000000]\n",
      "142: [D loss: 0.634272, acc: 0.638672]  [A loss: 0.548222, acc: 0.871094]\n",
      "143: [D loss: 0.747191, acc: 0.500000]  [A loss: 1.133589, acc: 0.003906]\n",
      "144: [D loss: 0.576963, acc: 0.732422]  [A loss: 0.929510, acc: 0.085938]\n",
      "145: [D loss: 0.600242, acc: 0.582031]  [A loss: 1.161453, acc: 0.003906]\n",
      "146: [D loss: 0.557757, acc: 0.673828]  [A loss: 1.175473, acc: 0.015625]\n",
      "147: [D loss: 0.558576, acc: 0.648438]  [A loss: 1.272799, acc: 0.003906]\n",
      "148: [D loss: 0.560679, acc: 0.673828]  [A loss: 1.371057, acc: 0.003906]\n",
      "149: [D loss: 0.535656, acc: 0.726562]  [A loss: 1.224706, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150: [D loss: 0.555625, acc: 0.640625]  [A loss: 1.610025, acc: 0.000000]\n",
      "151: [D loss: 0.514939, acc: 0.830078]  [A loss: 0.919972, acc: 0.136719]\n",
      "152: [D loss: 0.662136, acc: 0.537109]  [A loss: 2.588324, acc: 0.000000]\n",
      "153: [D loss: 0.663009, acc: 0.541016]  [A loss: 0.532625, acc: 0.886719]\n",
      "154: [D loss: 0.730069, acc: 0.503906]  [A loss: 1.070947, acc: 0.011719]\n",
      "155: [D loss: 0.574223, acc: 0.751953]  [A loss: 0.871162, acc: 0.140625]\n",
      "156: [D loss: 0.601056, acc: 0.597656]  [A loss: 1.103942, acc: 0.011719]\n",
      "157: [D loss: 0.552151, acc: 0.707031]  [A loss: 1.155707, acc: 0.011719]\n",
      "158: [D loss: 0.532884, acc: 0.679688]  [A loss: 1.436136, acc: 0.000000]\n",
      "159: [D loss: 0.519356, acc: 0.802734]  [A loss: 1.051268, acc: 0.046875]\n",
      "160: [D loss: 0.597501, acc: 0.562500]  [A loss: 2.023158, acc: 0.000000]\n",
      "161: [D loss: 0.595792, acc: 0.691406]  [A loss: 0.574433, acc: 0.789062]\n",
      "162: [D loss: 0.747483, acc: 0.503906]  [A loss: 1.470109, acc: 0.000000]\n",
      "163: [D loss: 0.591904, acc: 0.738281]  [A loss: 0.743544, acc: 0.355469]\n",
      "164: [D loss: 0.656211, acc: 0.517578]  [A loss: 1.107673, acc: 0.007812]\n",
      "165: [D loss: 0.567990, acc: 0.707031]  [A loss: 0.967280, acc: 0.078125]\n",
      "166: [D loss: 0.611658, acc: 0.562500]  [A loss: 1.590452, acc: 0.000000]\n",
      "167: [D loss: 0.548304, acc: 0.783203]  [A loss: 0.750207, acc: 0.386719]\n",
      "168: [D loss: 0.682974, acc: 0.517578]  [A loss: 1.913136, acc: 0.000000]\n",
      "169: [D loss: 0.586129, acc: 0.718750]  [A loss: 0.661146, acc: 0.632812]\n",
      "170: [D loss: 0.653257, acc: 0.513672]  [A loss: 1.204039, acc: 0.000000]\n",
      "171: [D loss: 0.543613, acc: 0.763672]  [A loss: 0.907115, acc: 0.097656]\n",
      "172: [D loss: 0.592578, acc: 0.542969]  [A loss: 1.600445, acc: 0.000000]\n",
      "173: [D loss: 0.531526, acc: 0.861328]  [A loss: 0.701216, acc: 0.453125]\n",
      "174: [D loss: 0.779481, acc: 0.503906]  [A loss: 1.990289, acc: 0.000000]\n",
      "175: [D loss: 0.644551, acc: 0.544922]  [A loss: 0.663405, acc: 0.667969]\n",
      "176: [D loss: 0.674499, acc: 0.511719]  [A loss: 0.970125, acc: 0.031250]\n",
      "177: [D loss: 0.579676, acc: 0.654297]  [A loss: 0.907970, acc: 0.078125]\n",
      "178: [D loss: 0.584158, acc: 0.583984]  [A loss: 1.176571, acc: 0.000000]\n",
      "179: [D loss: 0.551255, acc: 0.687500]  [A loss: 1.086098, acc: 0.007812]\n",
      "180: [D loss: 0.590745, acc: 0.580078]  [A loss: 1.502660, acc: 0.000000]\n",
      "181: [D loss: 0.562385, acc: 0.810547]  [A loss: 0.729655, acc: 0.460938]\n",
      "182: [D loss: 0.713305, acc: 0.505859]  [A loss: 1.656091, acc: 0.000000]\n",
      "183: [D loss: 0.609534, acc: 0.648438]  [A loss: 0.685247, acc: 0.566406]\n",
      "184: [D loss: 0.672721, acc: 0.511719]  [A loss: 1.049233, acc: 0.003906]\n",
      "185: [D loss: 0.570027, acc: 0.705078]  [A loss: 0.915013, acc: 0.105469]\n",
      "186: [D loss: 0.593917, acc: 0.582031]  [A loss: 1.293967, acc: 0.000000]\n",
      "187: [D loss: 0.551673, acc: 0.814453]  [A loss: 0.831821, acc: 0.218750]\n",
      "188: [D loss: 0.658111, acc: 0.529297]  [A loss: 1.726409, acc: 0.000000]\n",
      "189: [D loss: 0.590852, acc: 0.701172]  [A loss: 0.640161, acc: 0.636719]\n",
      "190: [D loss: 0.661944, acc: 0.519531]  [A loss: 1.175346, acc: 0.003906]\n",
      "191: [D loss: 0.561034, acc: 0.818359]  [A loss: 0.846091, acc: 0.203125]\n",
      "192: [D loss: 0.588404, acc: 0.578125]  [A loss: 1.312921, acc: 0.000000]\n",
      "193: [D loss: 0.538654, acc: 0.820312]  [A loss: 0.854997, acc: 0.226562]\n",
      "194: [D loss: 0.642349, acc: 0.556641]  [A loss: 1.747524, acc: 0.000000]\n",
      "195: [D loss: 0.586889, acc: 0.687500]  [A loss: 0.643825, acc: 0.617188]\n",
      "196: [D loss: 0.672840, acc: 0.505859]  [A loss: 1.211809, acc: 0.000000]\n",
      "197: [D loss: 0.563091, acc: 0.830078]  [A loss: 0.762079, acc: 0.347656]\n",
      "198: [D loss: 0.623959, acc: 0.525391]  [A loss: 1.272278, acc: 0.000000]\n",
      "199: [D loss: 0.535991, acc: 0.822266]  [A loss: 0.844146, acc: 0.238281]\n",
      "200: [D loss: 0.645643, acc: 0.548828]  [A loss: 1.601202, acc: 0.000000]\n",
      "201: [D loss: 0.587701, acc: 0.712891]  [A loss: 0.692103, acc: 0.554688]\n",
      "202: [D loss: 0.657043, acc: 0.517578]  [A loss: 1.128908, acc: 0.003906]\n",
      "203: [D loss: 0.548309, acc: 0.779297]  [A loss: 0.887176, acc: 0.148438]\n",
      "204: [D loss: 0.601592, acc: 0.564453]  [A loss: 1.428643, acc: 0.000000]\n",
      "205: [D loss: 0.539858, acc: 0.837891]  [A loss: 0.753074, acc: 0.359375]\n",
      "206: [D loss: 0.686661, acc: 0.521484]  [A loss: 1.605385, acc: 0.000000]\n",
      "207: [D loss: 0.592176, acc: 0.718750]  [A loss: 0.694689, acc: 0.515625]\n",
      "208: [D loss: 0.643461, acc: 0.519531]  [A loss: 1.103145, acc: 0.015625]\n",
      "209: [D loss: 0.548333, acc: 0.710938]  [A loss: 1.008682, acc: 0.074219]\n",
      "210: [D loss: 0.591014, acc: 0.603516]  [A loss: 1.461542, acc: 0.000000]\n",
      "211: [D loss: 0.553559, acc: 0.824219]  [A loss: 0.770949, acc: 0.320312]\n",
      "212: [D loss: 0.722037, acc: 0.519531]  [A loss: 1.784411, acc: 0.000000]\n",
      "213: [D loss: 0.617087, acc: 0.621094]  [A loss: 0.660950, acc: 0.574219]\n",
      "214: [D loss: 0.679237, acc: 0.513672]  [A loss: 0.981928, acc: 0.039062]\n",
      "215: [D loss: 0.590558, acc: 0.656250]  [A loss: 0.925885, acc: 0.109375]\n",
      "216: [D loss: 0.591122, acc: 0.609375]  [A loss: 1.254010, acc: 0.000000]\n",
      "217: [D loss: 0.564721, acc: 0.753906]  [A loss: 1.042826, acc: 0.042969]\n",
      "218: [D loss: 0.618848, acc: 0.621094]  [A loss: 1.460389, acc: 0.000000]\n",
      "219: [D loss: 0.574600, acc: 0.775391]  [A loss: 0.669624, acc: 0.578125]\n",
      "220: [D loss: 0.772274, acc: 0.503906]  [A loss: 1.601149, acc: 0.000000]\n",
      "221: [D loss: 0.628052, acc: 0.638672]  [A loss: 0.726309, acc: 0.433594]\n",
      "222: [D loss: 0.656397, acc: 0.525391]  [A loss: 1.005981, acc: 0.035156]\n",
      "223: [D loss: 0.571432, acc: 0.691406]  [A loss: 0.986008, acc: 0.058594]\n",
      "224: [D loss: 0.611061, acc: 0.628906]  [A loss: 1.356058, acc: 0.000000]\n",
      "225: [D loss: 0.580529, acc: 0.785156]  [A loss: 0.793537, acc: 0.285156]\n",
      "226: [D loss: 0.670469, acc: 0.529297]  [A loss: 1.524068, acc: 0.000000]\n",
      "227: [D loss: 0.618883, acc: 0.658203]  [A loss: 0.649850, acc: 0.644531]\n",
      "228: [D loss: 0.677709, acc: 0.511719]  [A loss: 1.103904, acc: 0.000000]\n",
      "229: [D loss: 0.588760, acc: 0.744141]  [A loss: 0.848022, acc: 0.191406]\n",
      "230: [D loss: 0.619997, acc: 0.558594]  [A loss: 1.347055, acc: 0.000000]\n",
      "231: [D loss: 0.576360, acc: 0.798828]  [A loss: 0.765715, acc: 0.332031]\n",
      "232: [D loss: 0.691052, acc: 0.525391]  [A loss: 1.472732, acc: 0.000000]\n",
      "233: [D loss: 0.603624, acc: 0.714844]  [A loss: 0.743405, acc: 0.390625]\n",
      "234: [D loss: 0.677343, acc: 0.505859]  [A loss: 1.115224, acc: 0.007812]\n",
      "235: [D loss: 0.592784, acc: 0.722656]  [A loss: 0.882867, acc: 0.156250]\n",
      "236: [D loss: 0.653936, acc: 0.572266]  [A loss: 1.438178, acc: 0.000000]\n",
      "237: [D loss: 0.604088, acc: 0.732422]  [A loss: 0.660598, acc: 0.582031]\n",
      "238: [D loss: 0.680648, acc: 0.507812]  [A loss: 1.164652, acc: 0.003906]\n",
      "239: [D loss: 0.589412, acc: 0.759766]  [A loss: 0.821310, acc: 0.261719]\n",
      "240: [D loss: 0.656669, acc: 0.570312]  [A loss: 1.285873, acc: 0.000000]\n",
      "241: [D loss: 0.603100, acc: 0.750000]  [A loss: 0.761028, acc: 0.359375]\n",
      "242: [D loss: 0.661058, acc: 0.515625]  [A loss: 1.317564, acc: 0.000000]\n",
      "243: [D loss: 0.604798, acc: 0.740234]  [A loss: 0.738567, acc: 0.410156]\n",
      "244: [D loss: 0.681786, acc: 0.513672]  [A loss: 1.414098, acc: 0.000000]\n",
      "245: [D loss: 0.596264, acc: 0.757812]  [A loss: 0.709428, acc: 0.476562]\n",
      "246: [D loss: 0.705191, acc: 0.515625]  [A loss: 1.426064, acc: 0.000000]\n",
      "247: [D loss: 0.619359, acc: 0.667969]  [A loss: 0.714514, acc: 0.437500]\n",
      "248: [D loss: 0.649347, acc: 0.515625]  [A loss: 1.083426, acc: 0.035156]\n",
      "249: [D loss: 0.591674, acc: 0.689453]  [A loss: 1.166640, acc: 0.007812]\n",
      "250: [D loss: 0.581280, acc: 0.740234]  [A loss: 1.038043, acc: 0.039062]\n",
      "251: [D loss: 0.625738, acc: 0.619141]  [A loss: 1.348626, acc: 0.003906]\n",
      "252: [D loss: 0.570666, acc: 0.798828]  [A loss: 0.794434, acc: 0.324219]\n",
      "253: [D loss: 0.724625, acc: 0.519531]  [A loss: 1.978742, acc: 0.000000]\n",
      "254: [D loss: 0.667813, acc: 0.531250]  [A loss: 0.610706, acc: 0.710938]\n",
      "255: [D loss: 0.749812, acc: 0.494141]  [A loss: 1.000938, acc: 0.031250]\n",
      "256: [D loss: 0.610613, acc: 0.667969]  [A loss: 0.897582, acc: 0.140625]\n",
      "257: [D loss: 0.624030, acc: 0.587891]  [A loss: 1.152157, acc: 0.007812]\n",
      "258: [D loss: 0.600479, acc: 0.728516]  [A loss: 0.886624, acc: 0.183594]\n",
      "259: [D loss: 0.676050, acc: 0.560547]  [A loss: 1.461205, acc: 0.000000]\n",
      "260: [D loss: 0.629613, acc: 0.658203]  [A loss: 0.695465, acc: 0.527344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261: [D loss: 0.692850, acc: 0.507812]  [A loss: 1.191884, acc: 0.015625]\n",
      "262: [D loss: 0.620307, acc: 0.699219]  [A loss: 0.854562, acc: 0.210938]\n",
      "263: [D loss: 0.656287, acc: 0.546875]  [A loss: 1.286908, acc: 0.000000]\n",
      "264: [D loss: 0.614107, acc: 0.736328]  [A loss: 0.768664, acc: 0.316406]\n",
      "265: [D loss: 0.693441, acc: 0.533203]  [A loss: 1.424506, acc: 0.000000]\n",
      "266: [D loss: 0.622045, acc: 0.697266]  [A loss: 0.707710, acc: 0.507812]\n",
      "267: [D loss: 0.705726, acc: 0.503906]  [A loss: 1.194848, acc: 0.000000]\n",
      "268: [D loss: 0.608676, acc: 0.726562]  [A loss: 0.821577, acc: 0.230469]\n",
      "269: [D loss: 0.676198, acc: 0.533203]  [A loss: 1.352546, acc: 0.000000]\n",
      "270: [D loss: 0.611809, acc: 0.722656]  [A loss: 0.735264, acc: 0.445312]\n",
      "271: [D loss: 0.704884, acc: 0.519531]  [A loss: 1.407935, acc: 0.000000]\n",
      "272: [D loss: 0.647092, acc: 0.652344]  [A loss: 0.737859, acc: 0.359375]\n",
      "273: [D loss: 0.699704, acc: 0.498047]  [A loss: 1.134573, acc: 0.007812]\n",
      "274: [D loss: 0.611895, acc: 0.712891]  [A loss: 0.916038, acc: 0.136719]\n",
      "275: [D loss: 0.658828, acc: 0.560547]  [A loss: 1.277361, acc: 0.000000]\n",
      "276: [D loss: 0.623635, acc: 0.710938]  [A loss: 0.798303, acc: 0.253906]\n",
      "277: [D loss: 0.707815, acc: 0.525391]  [A loss: 1.463934, acc: 0.000000]\n",
      "278: [D loss: 0.649957, acc: 0.609375]  [A loss: 0.678079, acc: 0.570312]\n",
      "279: [D loss: 0.701573, acc: 0.500000]  [A loss: 1.112872, acc: 0.003906]\n",
      "280: [D loss: 0.627038, acc: 0.656250]  [A loss: 0.870551, acc: 0.167969]\n",
      "281: [D loss: 0.661822, acc: 0.572266]  [A loss: 1.234633, acc: 0.000000]\n",
      "282: [D loss: 0.629416, acc: 0.707031]  [A loss: 0.736279, acc: 0.402344]\n",
      "283: [D loss: 0.697545, acc: 0.517578]  [A loss: 1.309827, acc: 0.000000]\n",
      "284: [D loss: 0.640904, acc: 0.652344]  [A loss: 0.724764, acc: 0.417969]\n",
      "285: [D loss: 0.685035, acc: 0.509766]  [A loss: 1.108168, acc: 0.003906]\n",
      "286: [D loss: 0.620369, acc: 0.714844]  [A loss: 0.787294, acc: 0.273438]\n",
      "287: [D loss: 0.669076, acc: 0.539062]  [A loss: 1.204560, acc: 0.007812]\n",
      "288: [D loss: 0.637906, acc: 0.656250]  [A loss: 0.803052, acc: 0.273438]\n",
      "289: [D loss: 0.683420, acc: 0.525391]  [A loss: 1.341304, acc: 0.000000]\n",
      "290: [D loss: 0.648591, acc: 0.640625]  [A loss: 0.709579, acc: 0.460938]\n",
      "291: [D loss: 0.698393, acc: 0.505859]  [A loss: 1.114146, acc: 0.007812]\n",
      "292: [D loss: 0.629101, acc: 0.695312]  [A loss: 0.807402, acc: 0.242188]\n",
      "293: [D loss: 0.673373, acc: 0.560547]  [A loss: 1.311615, acc: 0.000000]\n",
      "294: [D loss: 0.633881, acc: 0.675781]  [A loss: 0.703936, acc: 0.480469]\n",
      "295: [D loss: 0.676561, acc: 0.525391]  [A loss: 1.207749, acc: 0.000000]\n",
      "296: [D loss: 0.626683, acc: 0.705078]  [A loss: 0.765255, acc: 0.351562]\n",
      "297: [D loss: 0.679695, acc: 0.527344]  [A loss: 1.204065, acc: 0.003906]\n",
      "298: [D loss: 0.625209, acc: 0.714844]  [A loss: 0.738677, acc: 0.375000]\n",
      "299: [D loss: 0.693348, acc: 0.521484]  [A loss: 1.359513, acc: 0.000000]\n",
      "300: [D loss: 0.647352, acc: 0.652344]  [A loss: 0.684800, acc: 0.542969]\n",
      "301: [D loss: 0.737053, acc: 0.498047]  [A loss: 1.127427, acc: 0.007812]\n",
      "302: [D loss: 0.636175, acc: 0.675781]  [A loss: 0.815278, acc: 0.195312]\n",
      "303: [D loss: 0.670355, acc: 0.537109]  [A loss: 1.204022, acc: 0.000000]\n",
      "304: [D loss: 0.634670, acc: 0.687500]  [A loss: 0.707162, acc: 0.460938]\n",
      "305: [D loss: 0.680162, acc: 0.529297]  [A loss: 1.202164, acc: 0.003906]\n",
      "306: [D loss: 0.638641, acc: 0.673828]  [A loss: 0.715516, acc: 0.457031]\n",
      "307: [D loss: 0.701979, acc: 0.525391]  [A loss: 1.176849, acc: 0.000000]\n",
      "308: [D loss: 0.634365, acc: 0.697266]  [A loss: 0.738688, acc: 0.394531]\n",
      "309: [D loss: 0.669536, acc: 0.531250]  [A loss: 1.219766, acc: 0.011719]\n",
      "310: [D loss: 0.636012, acc: 0.695312]  [A loss: 0.733250, acc: 0.414062]\n",
      "311: [D loss: 0.675455, acc: 0.531250]  [A loss: 1.264141, acc: 0.000000]\n",
      "312: [D loss: 0.631175, acc: 0.683594]  [A loss: 0.737447, acc: 0.398438]\n",
      "313: [D loss: 0.677935, acc: 0.501953]  [A loss: 1.258199, acc: 0.003906]\n",
      "314: [D loss: 0.629432, acc: 0.687500]  [A loss: 0.714973, acc: 0.441406]\n",
      "315: [D loss: 0.710647, acc: 0.511719]  [A loss: 1.358248, acc: 0.000000]\n",
      "316: [D loss: 0.637345, acc: 0.683594]  [A loss: 0.683182, acc: 0.515625]\n",
      "317: [D loss: 0.723912, acc: 0.507812]  [A loss: 1.237155, acc: 0.000000]\n",
      "318: [D loss: 0.633363, acc: 0.681641]  [A loss: 0.727575, acc: 0.421875]\n",
      "319: [D loss: 0.694969, acc: 0.521484]  [A loss: 1.204282, acc: 0.000000]\n",
      "320: [D loss: 0.639075, acc: 0.652344]  [A loss: 0.754422, acc: 0.320312]\n",
      "321: [D loss: 0.696320, acc: 0.525391]  [A loss: 1.097077, acc: 0.007812]\n",
      "322: [D loss: 0.633564, acc: 0.677734]  [A loss: 0.856151, acc: 0.199219]\n",
      "323: [D loss: 0.653233, acc: 0.566406]  [A loss: 1.089725, acc: 0.007812]\n",
      "324: [D loss: 0.639125, acc: 0.652344]  [A loss: 0.893287, acc: 0.148438]\n",
      "325: [D loss: 0.655772, acc: 0.582031]  [A loss: 1.266747, acc: 0.003906]\n",
      "326: [D loss: 0.639962, acc: 0.662109]  [A loss: 0.809241, acc: 0.273438]\n",
      "327: [D loss: 0.698616, acc: 0.531250]  [A loss: 1.613518, acc: 0.000000]\n",
      "328: [D loss: 0.654738, acc: 0.593750]  [A loss: 0.588452, acc: 0.796875]\n",
      "329: [D loss: 0.749329, acc: 0.498047]  [A loss: 1.231918, acc: 0.000000]\n",
      "330: [D loss: 0.635832, acc: 0.697266]  [A loss: 0.720914, acc: 0.445312]\n",
      "331: [D loss: 0.684755, acc: 0.535156]  [A loss: 1.140241, acc: 0.003906]\n",
      "332: [D loss: 0.636180, acc: 0.669922]  [A loss: 0.822327, acc: 0.230469]\n",
      "333: [D loss: 0.678764, acc: 0.560547]  [A loss: 1.163975, acc: 0.003906]\n",
      "334: [D loss: 0.633598, acc: 0.687500]  [A loss: 0.793031, acc: 0.308594]\n",
      "335: [D loss: 0.672540, acc: 0.552734]  [A loss: 1.244160, acc: 0.000000]\n",
      "336: [D loss: 0.633710, acc: 0.695312]  [A loss: 0.707614, acc: 0.449219]\n",
      "337: [D loss: 0.693031, acc: 0.517578]  [A loss: 1.327957, acc: 0.000000]\n",
      "338: [D loss: 0.644347, acc: 0.648438]  [A loss: 0.651150, acc: 0.628906]\n",
      "339: [D loss: 0.738209, acc: 0.507812]  [A loss: 1.210626, acc: 0.003906]\n",
      "340: [D loss: 0.632477, acc: 0.673828]  [A loss: 0.758408, acc: 0.351562]\n",
      "341: [D loss: 0.707766, acc: 0.529297]  [A loss: 1.332702, acc: 0.000000]\n",
      "342: [D loss: 0.646242, acc: 0.626953]  [A loss: 0.660196, acc: 0.582031]\n",
      "343: [D loss: 0.758157, acc: 0.511719]  [A loss: 1.067696, acc: 0.007812]\n",
      "344: [D loss: 0.643904, acc: 0.644531]  [A loss: 0.804772, acc: 0.246094]\n",
      "345: [D loss: 0.670536, acc: 0.531250]  [A loss: 1.094759, acc: 0.003906]\n",
      "346: [D loss: 0.636491, acc: 0.669922]  [A loss: 0.798678, acc: 0.292969]\n",
      "347: [D loss: 0.676087, acc: 0.541016]  [A loss: 1.046746, acc: 0.027344]\n",
      "348: [D loss: 0.628157, acc: 0.689453]  [A loss: 0.858466, acc: 0.191406]\n",
      "349: [D loss: 0.665243, acc: 0.572266]  [A loss: 1.117132, acc: 0.019531]\n",
      "350: [D loss: 0.645865, acc: 0.671875]  [A loss: 0.884445, acc: 0.148438]\n",
      "351: [D loss: 0.663186, acc: 0.562500]  [A loss: 1.400113, acc: 0.000000]\n",
      "352: [D loss: 0.642133, acc: 0.642578]  [A loss: 0.687090, acc: 0.527344]\n",
      "353: [D loss: 0.758097, acc: 0.507812]  [A loss: 1.595797, acc: 0.000000]\n",
      "354: [D loss: 0.670066, acc: 0.548828]  [A loss: 0.600172, acc: 0.742188]\n",
      "355: [D loss: 0.792747, acc: 0.507812]  [A loss: 1.147128, acc: 0.003906]\n",
      "356: [D loss: 0.649871, acc: 0.634766]  [A loss: 0.758578, acc: 0.363281]\n",
      "357: [D loss: 0.686814, acc: 0.562500]  [A loss: 1.051787, acc: 0.007812]\n",
      "358: [D loss: 0.647020, acc: 0.677734]  [A loss: 0.779193, acc: 0.269531]\n",
      "359: [D loss: 0.680889, acc: 0.539062]  [A loss: 1.013384, acc: 0.035156]\n",
      "360: [D loss: 0.647499, acc: 0.654297]  [A loss: 0.886975, acc: 0.140625]\n",
      "361: [D loss: 0.652587, acc: 0.603516]  [A loss: 1.017853, acc: 0.046875]\n",
      "362: [D loss: 0.656738, acc: 0.646484]  [A loss: 0.937291, acc: 0.089844]\n",
      "363: [D loss: 0.657798, acc: 0.595703]  [A loss: 1.069875, acc: 0.007812]\n",
      "364: [D loss: 0.652503, acc: 0.617188]  [A loss: 0.925346, acc: 0.097656]\n",
      "365: [D loss: 0.678506, acc: 0.548828]  [A loss: 1.191501, acc: 0.000000]\n",
      "366: [D loss: 0.640907, acc: 0.666016]  [A loss: 0.773316, acc: 0.324219]\n",
      "367: [D loss: 0.689684, acc: 0.519531]  [A loss: 1.409445, acc: 0.000000]\n",
      "368: [D loss: 0.667979, acc: 0.576172]  [A loss: 0.617956, acc: 0.679688]\n",
      "369: [D loss: 0.749588, acc: 0.507812]  [A loss: 1.299657, acc: 0.000000]\n",
      "370: [D loss: 0.668244, acc: 0.607422]  [A loss: 0.687665, acc: 0.503906]\n",
      "371: [D loss: 0.681507, acc: 0.527344]  [A loss: 1.114313, acc: 0.011719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372: [D loss: 0.657446, acc: 0.593750]  [A loss: 0.781349, acc: 0.281250]\n",
      "373: [D loss: 0.682991, acc: 0.552734]  [A loss: 1.110205, acc: 0.011719]\n",
      "374: [D loss: 0.647153, acc: 0.650391]  [A loss: 0.811033, acc: 0.285156]\n",
      "375: [D loss: 0.687446, acc: 0.539062]  [A loss: 1.199192, acc: 0.000000]\n",
      "376: [D loss: 0.640300, acc: 0.652344]  [A loss: 0.696153, acc: 0.519531]\n",
      "377: [D loss: 0.709128, acc: 0.523438]  [A loss: 1.177961, acc: 0.011719]\n",
      "378: [D loss: 0.653262, acc: 0.634766]  [A loss: 0.739845, acc: 0.375000]\n",
      "379: [D loss: 0.688048, acc: 0.533203]  [A loss: 1.204066, acc: 0.000000]\n",
      "380: [D loss: 0.654775, acc: 0.638672]  [A loss: 0.755580, acc: 0.347656]\n",
      "381: [D loss: 0.694125, acc: 0.531250]  [A loss: 1.149275, acc: 0.003906]\n",
      "382: [D loss: 0.636388, acc: 0.679688]  [A loss: 0.719300, acc: 0.449219]\n",
      "383: [D loss: 0.704969, acc: 0.519531]  [A loss: 1.237861, acc: 0.003906]\n",
      "384: [D loss: 0.668895, acc: 0.585938]  [A loss: 0.708650, acc: 0.472656]\n",
      "385: [D loss: 0.712284, acc: 0.521484]  [A loss: 1.097255, acc: 0.011719]\n",
      "386: [D loss: 0.651173, acc: 0.648438]  [A loss: 0.832518, acc: 0.207031]\n",
      "387: [D loss: 0.680610, acc: 0.552734]  [A loss: 1.088751, acc: 0.019531]\n",
      "388: [D loss: 0.644420, acc: 0.654297]  [A loss: 0.772692, acc: 0.308594]\n",
      "389: [D loss: 0.683633, acc: 0.541016]  [A loss: 1.108257, acc: 0.042969]\n",
      "390: [D loss: 0.643859, acc: 0.660156]  [A loss: 0.848603, acc: 0.222656]\n",
      "391: [D loss: 0.693401, acc: 0.542969]  [A loss: 1.233037, acc: 0.000000]\n",
      "392: [D loss: 0.659779, acc: 0.603516]  [A loss: 0.695826, acc: 0.472656]\n",
      "393: [D loss: 0.715957, acc: 0.515625]  [A loss: 1.162159, acc: 0.007812]\n",
      "394: [D loss: 0.650846, acc: 0.626953]  [A loss: 0.716437, acc: 0.453125]\n",
      "395: [D loss: 0.699881, acc: 0.521484]  [A loss: 1.181764, acc: 0.003906]\n",
      "396: [D loss: 0.659369, acc: 0.607422]  [A loss: 0.760569, acc: 0.339844]\n",
      "397: [D loss: 0.690616, acc: 0.533203]  [A loss: 1.080514, acc: 0.023438]\n",
      "398: [D loss: 0.660314, acc: 0.607422]  [A loss: 0.793244, acc: 0.300781]\n",
      "399: [D loss: 0.681677, acc: 0.566406]  [A loss: 1.097241, acc: 0.019531]\n",
      "400: [D loss: 0.665631, acc: 0.593750]  [A loss: 0.744860, acc: 0.414062]\n",
      "401: [D loss: 0.695878, acc: 0.542969]  [A loss: 1.162745, acc: 0.003906]\n",
      "402: [D loss: 0.649128, acc: 0.652344]  [A loss: 0.707874, acc: 0.457031]\n",
      "403: [D loss: 0.713828, acc: 0.523438]  [A loss: 1.249859, acc: 0.000000]\n",
      "404: [D loss: 0.666061, acc: 0.617188]  [A loss: 0.655610, acc: 0.628906]\n",
      "405: [D loss: 0.709255, acc: 0.513672]  [A loss: 1.143056, acc: 0.007812]\n",
      "406: [D loss: 0.656946, acc: 0.613281]  [A loss: 0.716746, acc: 0.425781]\n",
      "407: [D loss: 0.693857, acc: 0.533203]  [A loss: 1.126611, acc: 0.011719]\n",
      "408: [D loss: 0.654088, acc: 0.642578]  [A loss: 0.759248, acc: 0.351562]\n",
      "409: [D loss: 0.695318, acc: 0.521484]  [A loss: 1.075679, acc: 0.023438]\n",
      "410: [D loss: 0.665020, acc: 0.593750]  [A loss: 0.805121, acc: 0.292969]\n",
      "411: [D loss: 0.682789, acc: 0.546875]  [A loss: 1.104690, acc: 0.015625]\n",
      "412: [D loss: 0.653533, acc: 0.660156]  [A loss: 0.774832, acc: 0.269531]\n",
      "413: [D loss: 0.703150, acc: 0.525391]  [A loss: 1.190979, acc: 0.003906]\n",
      "414: [D loss: 0.656175, acc: 0.625000]  [A loss: 0.731705, acc: 0.421875]\n",
      "415: [D loss: 0.704958, acc: 0.548828]  [A loss: 1.203186, acc: 0.000000]\n",
      "416: [D loss: 0.671479, acc: 0.556641]  [A loss: 0.661311, acc: 0.597656]\n",
      "417: [D loss: 0.728197, acc: 0.523438]  [A loss: 1.166029, acc: 0.007812]\n",
      "418: [D loss: 0.668405, acc: 0.599609]  [A loss: 0.747330, acc: 0.394531]\n",
      "419: [D loss: 0.699406, acc: 0.513672]  [A loss: 1.061252, acc: 0.015625]\n",
      "420: [D loss: 0.665630, acc: 0.605469]  [A loss: 0.808697, acc: 0.242188]\n",
      "421: [D loss: 0.707700, acc: 0.537109]  [A loss: 1.092282, acc: 0.015625]\n",
      "422: [D loss: 0.675153, acc: 0.613281]  [A loss: 0.833964, acc: 0.179688]\n",
      "423: [D loss: 0.694823, acc: 0.550781]  [A loss: 0.980726, acc: 0.050781]\n",
      "424: [D loss: 0.661812, acc: 0.601562]  [A loss: 0.873450, acc: 0.140625]\n",
      "425: [D loss: 0.665890, acc: 0.587891]  [A loss: 1.025023, acc: 0.027344]\n",
      "426: [D loss: 0.663599, acc: 0.595703]  [A loss: 0.872983, acc: 0.171875]\n",
      "427: [D loss: 0.687727, acc: 0.531250]  [A loss: 1.072909, acc: 0.035156]\n",
      "428: [D loss: 0.668961, acc: 0.585938]  [A loss: 0.775674, acc: 0.324219]\n",
      "429: [D loss: 0.688469, acc: 0.548828]  [A loss: 1.154819, acc: 0.003906]\n",
      "430: [D loss: 0.662978, acc: 0.591797]  [A loss: 0.716976, acc: 0.476562]\n",
      "431: [D loss: 0.705946, acc: 0.515625]  [A loss: 1.241933, acc: 0.000000]\n",
      "432: [D loss: 0.670422, acc: 0.578125]  [A loss: 0.653779, acc: 0.636719]\n",
      "433: [D loss: 0.706290, acc: 0.513672]  [A loss: 1.172462, acc: 0.003906]\n",
      "434: [D loss: 0.660911, acc: 0.615234]  [A loss: 0.710486, acc: 0.472656]\n",
      "435: [D loss: 0.715023, acc: 0.505859]  [A loss: 1.126871, acc: 0.023438]\n",
      "436: [D loss: 0.668486, acc: 0.572266]  [A loss: 0.734921, acc: 0.371094]\n",
      "437: [D loss: 0.685466, acc: 0.542969]  [A loss: 1.038370, acc: 0.019531]\n",
      "438: [D loss: 0.652925, acc: 0.623047]  [A loss: 0.799595, acc: 0.253906]\n",
      "439: [D loss: 0.690874, acc: 0.541016]  [A loss: 1.016710, acc: 0.039062]\n",
      "440: [D loss: 0.659516, acc: 0.605469]  [A loss: 0.789221, acc: 0.257812]\n",
      "441: [D loss: 0.683103, acc: 0.544922]  [A loss: 1.063907, acc: 0.019531]\n",
      "442: [D loss: 0.654107, acc: 0.644531]  [A loss: 0.740833, acc: 0.402344]\n",
      "443: [D loss: 0.689552, acc: 0.544922]  [A loss: 1.119189, acc: 0.007812]\n",
      "444: [D loss: 0.657710, acc: 0.642578]  [A loss: 0.678917, acc: 0.566406]\n",
      "445: [D loss: 0.690288, acc: 0.537109]  [A loss: 1.103427, acc: 0.007812]\n",
      "446: [D loss: 0.658642, acc: 0.603516]  [A loss: 0.750261, acc: 0.359375]\n",
      "447: [D loss: 0.693274, acc: 0.521484]  [A loss: 1.076827, acc: 0.027344]\n",
      "448: [D loss: 0.662955, acc: 0.597656]  [A loss: 0.773555, acc: 0.320312]\n",
      "449: [D loss: 0.681925, acc: 0.550781]  [A loss: 1.127098, acc: 0.019531]\n",
      "450: [D loss: 0.658962, acc: 0.605469]  [A loss: 0.773598, acc: 0.312500]\n",
      "451: [D loss: 0.703070, acc: 0.523438]  [A loss: 1.183615, acc: 0.003906]\n",
      "452: [D loss: 0.667815, acc: 0.597656]  [A loss: 0.668738, acc: 0.593750]\n",
      "453: [D loss: 0.732493, acc: 0.513672]  [A loss: 1.073091, acc: 0.019531]\n",
      "454: [D loss: 0.653712, acc: 0.625000]  [A loss: 0.823801, acc: 0.238281]\n",
      "455: [D loss: 0.676202, acc: 0.562500]  [A loss: 0.979410, acc: 0.054688]\n",
      "456: [D loss: 0.657423, acc: 0.634766]  [A loss: 0.795864, acc: 0.269531]\n",
      "457: [D loss: 0.681772, acc: 0.556641]  [A loss: 1.004439, acc: 0.031250]\n",
      "458: [D loss: 0.646143, acc: 0.650391]  [A loss: 0.826304, acc: 0.226562]\n",
      "459: [D loss: 0.693961, acc: 0.535156]  [A loss: 1.093731, acc: 0.023438]\n",
      "460: [D loss: 0.656313, acc: 0.632812]  [A loss: 0.754479, acc: 0.339844]\n",
      "461: [D loss: 0.691476, acc: 0.542969]  [A loss: 1.119311, acc: 0.007812]\n",
      "462: [D loss: 0.669255, acc: 0.603516]  [A loss: 0.752611, acc: 0.335938]\n",
      "463: [D loss: 0.692659, acc: 0.542969]  [A loss: 1.109102, acc: 0.019531]\n",
      "464: [D loss: 0.666710, acc: 0.617188]  [A loss: 0.743987, acc: 0.394531]\n",
      "465: [D loss: 0.685460, acc: 0.539062]  [A loss: 1.049058, acc: 0.039062]\n",
      "466: [D loss: 0.673209, acc: 0.597656]  [A loss: 0.789418, acc: 0.292969]\n",
      "467: [D loss: 0.680390, acc: 0.533203]  [A loss: 1.064662, acc: 0.007812]\n",
      "468: [D loss: 0.657580, acc: 0.638672]  [A loss: 0.814735, acc: 0.242188]\n",
      "469: [D loss: 0.691779, acc: 0.511719]  [A loss: 1.098738, acc: 0.011719]\n",
      "470: [D loss: 0.669341, acc: 0.607422]  [A loss: 0.744358, acc: 0.394531]\n",
      "471: [D loss: 0.682099, acc: 0.564453]  [A loss: 1.097087, acc: 0.019531]\n",
      "472: [D loss: 0.665106, acc: 0.595703]  [A loss: 0.750055, acc: 0.398438]\n",
      "473: [D loss: 0.684622, acc: 0.542969]  [A loss: 1.125282, acc: 0.011719]\n",
      "474: [D loss: 0.663514, acc: 0.619141]  [A loss: 0.727802, acc: 0.453125]\n",
      "475: [D loss: 0.700480, acc: 0.533203]  [A loss: 1.068935, acc: 0.027344]\n",
      "476: [D loss: 0.668090, acc: 0.601562]  [A loss: 0.787455, acc: 0.285156]\n",
      "477: [D loss: 0.698665, acc: 0.535156]  [A loss: 1.076451, acc: 0.035156]\n",
      "478: [D loss: 0.663398, acc: 0.585938]  [A loss: 0.785163, acc: 0.277344]\n",
      "479: [D loss: 0.705257, acc: 0.542969]  [A loss: 1.050799, acc: 0.007812]\n",
      "480: [D loss: 0.675319, acc: 0.562500]  [A loss: 0.745025, acc: 0.382812]\n",
      "481: [D loss: 0.688325, acc: 0.529297]  [A loss: 1.003971, acc: 0.058594]\n",
      "482: [D loss: 0.659082, acc: 0.593750]  [A loss: 0.777363, acc: 0.300781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483: [D loss: 0.676468, acc: 0.558594]  [A loss: 1.075071, acc: 0.027344]\n",
      "484: [D loss: 0.656010, acc: 0.615234]  [A loss: 0.766729, acc: 0.328125]\n",
      "485: [D loss: 0.683945, acc: 0.548828]  [A loss: 1.075685, acc: 0.019531]\n",
      "486: [D loss: 0.661069, acc: 0.591797]  [A loss: 0.742467, acc: 0.421875]\n",
      "487: [D loss: 0.685089, acc: 0.566406]  [A loss: 1.104217, acc: 0.015625]\n",
      "488: [D loss: 0.663803, acc: 0.607422]  [A loss: 0.746136, acc: 0.414062]\n",
      "489: [D loss: 0.677715, acc: 0.554688]  [A loss: 1.126237, acc: 0.019531]\n",
      "490: [D loss: 0.666715, acc: 0.611328]  [A loss: 0.734780, acc: 0.402344]\n",
      "491: [D loss: 0.709540, acc: 0.527344]  [A loss: 1.166318, acc: 0.003906]\n",
      "492: [D loss: 0.660489, acc: 0.626953]  [A loss: 0.732096, acc: 0.445312]\n",
      "493: [D loss: 0.678066, acc: 0.550781]  [A loss: 1.015891, acc: 0.035156]\n",
      "494: [D loss: 0.667350, acc: 0.597656]  [A loss: 0.777312, acc: 0.296875]\n",
      "495: [D loss: 0.684891, acc: 0.554688]  [A loss: 1.069522, acc: 0.011719]\n",
      "496: [D loss: 0.660987, acc: 0.619141]  [A loss: 0.783906, acc: 0.300781]\n",
      "497: [D loss: 0.700295, acc: 0.525391]  [A loss: 1.011522, acc: 0.070312]\n",
      "498: [D loss: 0.668591, acc: 0.560547]  [A loss: 0.811187, acc: 0.261719]\n",
      "499: [D loss: 0.676517, acc: 0.556641]  [A loss: 0.975151, acc: 0.042969]\n",
      "500: [D loss: 0.659311, acc: 0.625000]  [A loss: 0.878239, acc: 0.164062]\n",
      "501: [D loss: 0.674354, acc: 0.566406]  [A loss: 0.998984, acc: 0.054688]\n",
      "502: [D loss: 0.672665, acc: 0.578125]  [A loss: 0.882280, acc: 0.132812]\n",
      "503: [D loss: 0.671818, acc: 0.562500]  [A loss: 1.047295, acc: 0.027344]\n",
      "504: [D loss: 0.665970, acc: 0.576172]  [A loss: 0.791671, acc: 0.300781]\n",
      "505: [D loss: 0.682775, acc: 0.550781]  [A loss: 1.087419, acc: 0.007812]\n",
      "506: [D loss: 0.655930, acc: 0.613281]  [A loss: 0.716343, acc: 0.460938]\n",
      "507: [D loss: 0.688373, acc: 0.542969]  [A loss: 1.102525, acc: 0.011719]\n",
      "508: [D loss: 0.663545, acc: 0.589844]  [A loss: 0.761368, acc: 0.363281]\n",
      "509: [D loss: 0.663742, acc: 0.570312]  [A loss: 1.090737, acc: 0.019531]\n",
      "510: [D loss: 0.646424, acc: 0.656250]  [A loss: 0.804463, acc: 0.292969]\n",
      "511: [D loss: 0.694103, acc: 0.544922]  [A loss: 1.173040, acc: 0.011719]\n",
      "512: [D loss: 0.660385, acc: 0.601562]  [A loss: 0.690401, acc: 0.531250]\n",
      "513: [D loss: 0.720126, acc: 0.519531]  [A loss: 1.149969, acc: 0.027344]\n",
      "514: [D loss: 0.673278, acc: 0.560547]  [A loss: 0.722116, acc: 0.457031]\n",
      "515: [D loss: 0.707115, acc: 0.541016]  [A loss: 1.162343, acc: 0.011719]\n",
      "516: [D loss: 0.672306, acc: 0.589844]  [A loss: 0.746331, acc: 0.417969]\n",
      "517: [D loss: 0.697782, acc: 0.531250]  [A loss: 1.003416, acc: 0.019531]\n",
      "518: [D loss: 0.660735, acc: 0.576172]  [A loss: 0.837992, acc: 0.222656]\n",
      "519: [D loss: 0.657664, acc: 0.605469]  [A loss: 0.951673, acc: 0.105469]\n",
      "520: [D loss: 0.672491, acc: 0.562500]  [A loss: 0.896690, acc: 0.132812]\n",
      "521: [D loss: 0.666213, acc: 0.593750]  [A loss: 0.907453, acc: 0.140625]\n",
      "522: [D loss: 0.663431, acc: 0.603516]  [A loss: 0.887475, acc: 0.164062]\n",
      "523: [D loss: 0.663227, acc: 0.597656]  [A loss: 0.984857, acc: 0.058594]\n",
      "524: [D loss: 0.653903, acc: 0.615234]  [A loss: 0.913266, acc: 0.109375]\n",
      "525: [D loss: 0.666797, acc: 0.613281]  [A loss: 0.975281, acc: 0.085938]\n",
      "526: [D loss: 0.655790, acc: 0.632812]  [A loss: 0.904446, acc: 0.113281]\n",
      "527: [D loss: 0.677558, acc: 0.562500]  [A loss: 1.074354, acc: 0.035156]\n",
      "528: [D loss: 0.658689, acc: 0.597656]  [A loss: 0.870307, acc: 0.179688]\n",
      "529: [D loss: 0.671206, acc: 0.605469]  [A loss: 1.003662, acc: 0.066406]\n",
      "530: [D loss: 0.664652, acc: 0.587891]  [A loss: 0.910729, acc: 0.144531]\n",
      "531: [D loss: 0.659296, acc: 0.601562]  [A loss: 1.033084, acc: 0.042969]\n",
      "532: [D loss: 0.684380, acc: 0.566406]  [A loss: 1.006594, acc: 0.066406]\n",
      "533: [D loss: 0.660137, acc: 0.621094]  [A loss: 0.927372, acc: 0.175781]\n",
      "534: [D loss: 0.666718, acc: 0.562500]  [A loss: 1.006095, acc: 0.078125]\n",
      "535: [D loss: 0.650622, acc: 0.615234]  [A loss: 0.930883, acc: 0.117188]\n",
      "536: [D loss: 0.680331, acc: 0.574219]  [A loss: 1.189995, acc: 0.007812]\n",
      "537: [D loss: 0.660031, acc: 0.601562]  [A loss: 0.721765, acc: 0.464844]\n",
      "538: [D loss: 0.722793, acc: 0.533203]  [A loss: 1.344033, acc: 0.000000]\n",
      "539: [D loss: 0.671020, acc: 0.560547]  [A loss: 0.554269, acc: 0.835938]\n",
      "540: [D loss: 0.791505, acc: 0.507812]  [A loss: 1.157349, acc: 0.019531]\n",
      "541: [D loss: 0.668995, acc: 0.585938]  [A loss: 0.719529, acc: 0.429688]\n",
      "542: [D loss: 0.703112, acc: 0.523438]  [A loss: 1.071517, acc: 0.023438]\n",
      "543: [D loss: 0.667951, acc: 0.583984]  [A loss: 0.795537, acc: 0.273438]\n",
      "544: [D loss: 0.680466, acc: 0.562500]  [A loss: 0.979315, acc: 0.066406]\n",
      "545: [D loss: 0.654104, acc: 0.623047]  [A loss: 0.808263, acc: 0.269531]\n",
      "546: [D loss: 0.683678, acc: 0.564453]  [A loss: 0.974904, acc: 0.062500]\n",
      "547: [D loss: 0.657427, acc: 0.570312]  [A loss: 0.837917, acc: 0.230469]\n",
      "548: [D loss: 0.670413, acc: 0.595703]  [A loss: 0.953093, acc: 0.085938]\n",
      "549: [D loss: 0.665579, acc: 0.638672]  [A loss: 0.849848, acc: 0.195312]\n",
      "550: [D loss: 0.663870, acc: 0.582031]  [A loss: 0.937419, acc: 0.105469]\n",
      "551: [D loss: 0.659874, acc: 0.589844]  [A loss: 0.877553, acc: 0.191406]\n",
      "552: [D loss: 0.665291, acc: 0.583984]  [A loss: 1.004499, acc: 0.085938]\n",
      "553: [D loss: 0.667088, acc: 0.621094]  [A loss: 0.853509, acc: 0.203125]\n",
      "554: [D loss: 0.694413, acc: 0.542969]  [A loss: 1.091086, acc: 0.035156]\n",
      "555: [D loss: 0.662255, acc: 0.605469]  [A loss: 0.755956, acc: 0.398438]\n",
      "556: [D loss: 0.687667, acc: 0.546875]  [A loss: 1.110974, acc: 0.015625]\n",
      "557: [D loss: 0.658903, acc: 0.601562]  [A loss: 0.738456, acc: 0.394531]\n",
      "558: [D loss: 0.684911, acc: 0.546875]  [A loss: 1.089445, acc: 0.039062]\n",
      "559: [D loss: 0.660927, acc: 0.601562]  [A loss: 0.823083, acc: 0.285156]\n",
      "560: [D loss: 0.701011, acc: 0.531250]  [A loss: 1.117678, acc: 0.007812]\n",
      "561: [D loss: 0.662887, acc: 0.582031]  [A loss: 0.855369, acc: 0.187500]\n",
      "562: [D loss: 0.686787, acc: 0.564453]  [A loss: 0.987646, acc: 0.062500]\n",
      "563: [D loss: 0.654760, acc: 0.625000]  [A loss: 0.864707, acc: 0.218750]\n",
      "564: [D loss: 0.674810, acc: 0.560547]  [A loss: 1.102799, acc: 0.035156]\n",
      "565: [D loss: 0.670907, acc: 0.595703]  [A loss: 0.831305, acc: 0.242188]\n",
      "566: [D loss: 0.670498, acc: 0.566406]  [A loss: 1.052598, acc: 0.054688]\n",
      "567: [D loss: 0.668613, acc: 0.568359]  [A loss: 0.745254, acc: 0.382812]\n",
      "568: [D loss: 0.711775, acc: 0.525391]  [A loss: 1.161609, acc: 0.007812]\n",
      "569: [D loss: 0.668789, acc: 0.587891]  [A loss: 0.679296, acc: 0.539062]\n",
      "570: [D loss: 0.712541, acc: 0.531250]  [A loss: 1.133687, acc: 0.027344]\n",
      "571: [D loss: 0.654378, acc: 0.615234]  [A loss: 0.740050, acc: 0.406250]\n",
      "572: [D loss: 0.689079, acc: 0.539062]  [A loss: 1.085190, acc: 0.015625]\n",
      "573: [D loss: 0.662244, acc: 0.587891]  [A loss: 0.721315, acc: 0.449219]\n",
      "574: [D loss: 0.697751, acc: 0.537109]  [A loss: 0.983653, acc: 0.050781]\n",
      "575: [D loss: 0.661935, acc: 0.613281]  [A loss: 0.789966, acc: 0.324219]\n",
      "576: [D loss: 0.689689, acc: 0.544922]  [A loss: 0.979307, acc: 0.105469]\n",
      "577: [D loss: 0.665209, acc: 0.601562]  [A loss: 0.889783, acc: 0.175781]\n",
      "578: [D loss: 0.687549, acc: 0.578125]  [A loss: 0.996791, acc: 0.062500]\n",
      "579: [D loss: 0.667929, acc: 0.554688]  [A loss: 0.864405, acc: 0.203125]\n",
      "580: [D loss: 0.677628, acc: 0.576172]  [A loss: 0.992432, acc: 0.070312]\n",
      "581: [D loss: 0.672180, acc: 0.585938]  [A loss: 0.827933, acc: 0.320312]\n",
      "582: [D loss: 0.696438, acc: 0.527344]  [A loss: 1.114042, acc: 0.066406]\n",
      "583: [D loss: 0.673980, acc: 0.570312]  [A loss: 0.890445, acc: 0.148438]\n",
      "584: [D loss: 0.673576, acc: 0.582031]  [A loss: 1.007778, acc: 0.066406]\n",
      "585: [D loss: 0.676635, acc: 0.587891]  [A loss: 0.887256, acc: 0.140625]\n",
      "586: [D loss: 0.682565, acc: 0.550781]  [A loss: 1.024344, acc: 0.078125]\n",
      "587: [D loss: 0.668519, acc: 0.591797]  [A loss: 0.875489, acc: 0.183594]\n",
      "588: [D loss: 0.661206, acc: 0.605469]  [A loss: 0.983698, acc: 0.078125]\n",
      "589: [D loss: 0.649527, acc: 0.640625]  [A loss: 0.921033, acc: 0.164062]\n",
      "590: [D loss: 0.670144, acc: 0.599609]  [A loss: 1.062980, acc: 0.035156]\n",
      "591: [D loss: 0.670342, acc: 0.587891]  [A loss: 0.822712, acc: 0.242188]\n",
      "592: [D loss: 0.673265, acc: 0.568359]  [A loss: 1.146448, acc: 0.015625]\n",
      "593: [D loss: 0.665032, acc: 0.597656]  [A loss: 0.726861, acc: 0.445312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594: [D loss: 0.719316, acc: 0.527344]  [A loss: 1.220095, acc: 0.011719]\n",
      "595: [D loss: 0.678765, acc: 0.568359]  [A loss: 0.700695, acc: 0.527344]\n",
      "596: [D loss: 0.719229, acc: 0.509766]  [A loss: 1.103366, acc: 0.035156]\n",
      "597: [D loss: 0.680752, acc: 0.560547]  [A loss: 0.923783, acc: 0.109375]\n",
      "598: [D loss: 0.660474, acc: 0.619141]  [A loss: 1.000682, acc: 0.078125]\n",
      "599: [D loss: 0.652347, acc: 0.632812]  [A loss: 0.883040, acc: 0.164062]\n",
      "600: [D loss: 0.669772, acc: 0.585938]  [A loss: 0.927901, acc: 0.101562]\n",
      "601: [D loss: 0.666589, acc: 0.585938]  [A loss: 0.994254, acc: 0.062500]\n",
      "602: [D loss: 0.662041, acc: 0.574219]  [A loss: 0.897859, acc: 0.140625]\n",
      "603: [D loss: 0.674046, acc: 0.578125]  [A loss: 0.960963, acc: 0.105469]\n",
      "604: [D loss: 0.675938, acc: 0.560547]  [A loss: 0.915551, acc: 0.164062]\n",
      "605: [D loss: 0.674610, acc: 0.585938]  [A loss: 0.986661, acc: 0.078125]\n",
      "606: [D loss: 0.681557, acc: 0.552734]  [A loss: 0.940093, acc: 0.074219]\n",
      "607: [D loss: 0.678012, acc: 0.564453]  [A loss: 1.097508, acc: 0.027344]\n",
      "608: [D loss: 0.678487, acc: 0.583984]  [A loss: 0.901517, acc: 0.171875]\n",
      "609: [D loss: 0.668995, acc: 0.587891]  [A loss: 1.054577, acc: 0.066406]\n",
      "610: [D loss: 0.699062, acc: 0.535156]  [A loss: 1.022409, acc: 0.066406]\n",
      "611: [D loss: 0.674300, acc: 0.564453]  [A loss: 0.924088, acc: 0.171875]\n",
      "612: [D loss: 0.688930, acc: 0.542969]  [A loss: 1.295356, acc: 0.003906]\n",
      "613: [D loss: 0.663978, acc: 0.589844]  [A loss: 0.651058, acc: 0.605469]\n",
      "614: [D loss: 0.744111, acc: 0.513672]  [A loss: 1.344173, acc: 0.000000]\n",
      "615: [D loss: 0.672867, acc: 0.587891]  [A loss: 0.676764, acc: 0.531250]\n",
      "616: [D loss: 0.708362, acc: 0.527344]  [A loss: 1.133443, acc: 0.023438]\n",
      "617: [D loss: 0.657446, acc: 0.589844]  [A loss: 0.786328, acc: 0.332031]\n",
      "618: [D loss: 0.670686, acc: 0.578125]  [A loss: 1.000223, acc: 0.054688]\n",
      "619: [D loss: 0.656824, acc: 0.595703]  [A loss: 0.872960, acc: 0.195312]\n",
      "620: [D loss: 0.666628, acc: 0.591797]  [A loss: 0.969572, acc: 0.078125]\n",
      "621: [D loss: 0.658716, acc: 0.603516]  [A loss: 0.884264, acc: 0.183594]\n",
      "622: [D loss: 0.665977, acc: 0.568359]  [A loss: 1.033990, acc: 0.066406]\n",
      "623: [D loss: 0.670081, acc: 0.587891]  [A loss: 0.957063, acc: 0.128906]\n",
      "624: [D loss: 0.673359, acc: 0.580078]  [A loss: 1.066563, acc: 0.042969]\n",
      "625: [D loss: 0.671097, acc: 0.591797]  [A loss: 0.839811, acc: 0.230469]\n",
      "626: [D loss: 0.686281, acc: 0.560547]  [A loss: 1.040144, acc: 0.035156]\n",
      "627: [D loss: 0.655638, acc: 0.623047]  [A loss: 0.858931, acc: 0.238281]\n",
      "628: [D loss: 0.708071, acc: 0.533203]  [A loss: 1.129633, acc: 0.019531]\n",
      "629: [D loss: 0.673275, acc: 0.583984]  [A loss: 0.850188, acc: 0.210938]\n",
      "630: [D loss: 0.697996, acc: 0.531250]  [A loss: 1.125411, acc: 0.042969]\n",
      "631: [D loss: 0.660004, acc: 0.591797]  [A loss: 0.700724, acc: 0.500000]\n",
      "632: [D loss: 0.702124, acc: 0.550781]  [A loss: 1.145956, acc: 0.015625]\n",
      "633: [D loss: 0.649022, acc: 0.628906]  [A loss: 0.725440, acc: 0.496094]\n",
      "634: [D loss: 0.728704, acc: 0.541016]  [A loss: 1.152155, acc: 0.023438]\n",
      "635: [D loss: 0.661662, acc: 0.597656]  [A loss: 0.742320, acc: 0.414062]\n",
      "636: [D loss: 0.715337, acc: 0.529297]  [A loss: 1.091966, acc: 0.027344]\n",
      "637: [D loss: 0.651389, acc: 0.634766]  [A loss: 0.703014, acc: 0.492188]\n",
      "638: [D loss: 0.722521, acc: 0.515625]  [A loss: 1.130479, acc: 0.019531]\n",
      "639: [D loss: 0.656971, acc: 0.617188]  [A loss: 0.780299, acc: 0.332031]\n",
      "640: [D loss: 0.693008, acc: 0.539062]  [A loss: 1.046317, acc: 0.070312]\n",
      "641: [D loss: 0.660251, acc: 0.607422]  [A loss: 0.818669, acc: 0.242188]\n",
      "642: [D loss: 0.684941, acc: 0.558594]  [A loss: 1.003317, acc: 0.078125]\n",
      "643: [D loss: 0.666628, acc: 0.615234]  [A loss: 0.794050, acc: 0.347656]\n",
      "644: [D loss: 0.694464, acc: 0.570312]  [A loss: 1.046671, acc: 0.054688]\n",
      "645: [D loss: 0.671233, acc: 0.595703]  [A loss: 0.790059, acc: 0.292969]\n",
      "646: [D loss: 0.683491, acc: 0.564453]  [A loss: 1.105497, acc: 0.039062]\n",
      "647: [D loss: 0.652053, acc: 0.623047]  [A loss: 0.808590, acc: 0.250000]\n",
      "648: [D loss: 0.683706, acc: 0.556641]  [A loss: 1.083750, acc: 0.042969]\n",
      "649: [D loss: 0.649735, acc: 0.638672]  [A loss: 0.802515, acc: 0.273438]\n",
      "650: [D loss: 0.676181, acc: 0.570312]  [A loss: 1.115396, acc: 0.046875]\n",
      "651: [D loss: 0.645452, acc: 0.638672]  [A loss: 0.751683, acc: 0.445312]\n",
      "652: [D loss: 0.702816, acc: 0.527344]  [A loss: 1.163298, acc: 0.023438]\n",
      "653: [D loss: 0.661753, acc: 0.628906]  [A loss: 0.719778, acc: 0.472656]\n",
      "654: [D loss: 0.705719, acc: 0.552734]  [A loss: 1.081681, acc: 0.031250]\n",
      "655: [D loss: 0.660257, acc: 0.609375]  [A loss: 0.762516, acc: 0.375000]\n",
      "656: [D loss: 0.681243, acc: 0.546875]  [A loss: 1.064314, acc: 0.046875]\n",
      "657: [D loss: 0.674840, acc: 0.572266]  [A loss: 0.917194, acc: 0.113281]\n",
      "658: [D loss: 0.677024, acc: 0.595703]  [A loss: 0.972865, acc: 0.093750]\n",
      "659: [D loss: 0.648322, acc: 0.619141]  [A loss: 0.833218, acc: 0.218750]\n",
      "660: [D loss: 0.678845, acc: 0.554688]  [A loss: 1.000086, acc: 0.070312]\n",
      "661: [D loss: 0.658510, acc: 0.628906]  [A loss: 0.900283, acc: 0.164062]\n",
      "662: [D loss: 0.674165, acc: 0.574219]  [A loss: 1.011990, acc: 0.046875]\n",
      "663: [D loss: 0.668013, acc: 0.578125]  [A loss: 0.935218, acc: 0.140625]\n",
      "664: [D loss: 0.687601, acc: 0.556641]  [A loss: 1.135503, acc: 0.046875]\n",
      "665: [D loss: 0.667097, acc: 0.603516]  [A loss: 0.894970, acc: 0.164062]\n",
      "666: [D loss: 0.671933, acc: 0.574219]  [A loss: 1.000923, acc: 0.066406]\n",
      "667: [D loss: 0.653021, acc: 0.634766]  [A loss: 0.892214, acc: 0.164062]\n",
      "668: [D loss: 0.704593, acc: 0.525391]  [A loss: 1.078840, acc: 0.062500]\n",
      "669: [D loss: 0.666894, acc: 0.605469]  [A loss: 0.785892, acc: 0.320312]\n",
      "670: [D loss: 0.688615, acc: 0.544922]  [A loss: 1.136333, acc: 0.031250]\n",
      "671: [D loss: 0.655994, acc: 0.603516]  [A loss: 0.788816, acc: 0.359375]\n",
      "672: [D loss: 0.694675, acc: 0.546875]  [A loss: 1.206383, acc: 0.011719]\n",
      "673: [D loss: 0.667791, acc: 0.593750]  [A loss: 0.667106, acc: 0.574219]\n",
      "674: [D loss: 0.710329, acc: 0.515625]  [A loss: 1.162014, acc: 0.023438]\n",
      "675: [D loss: 0.662246, acc: 0.617188]  [A loss: 0.796549, acc: 0.332031]\n",
      "676: [D loss: 0.695351, acc: 0.533203]  [A loss: 1.106404, acc: 0.031250]\n",
      "677: [D loss: 0.653388, acc: 0.626953]  [A loss: 0.775799, acc: 0.371094]\n",
      "678: [D loss: 0.686136, acc: 0.558594]  [A loss: 1.062194, acc: 0.054688]\n",
      "679: [D loss: 0.658345, acc: 0.591797]  [A loss: 0.814440, acc: 0.285156]\n",
      "680: [D loss: 0.678722, acc: 0.574219]  [A loss: 1.050179, acc: 0.050781]\n",
      "681: [D loss: 0.653319, acc: 0.621094]  [A loss: 0.823771, acc: 0.277344]\n",
      "682: [D loss: 0.684669, acc: 0.554688]  [A loss: 1.059022, acc: 0.046875]\n",
      "683: [D loss: 0.651544, acc: 0.611328]  [A loss: 0.814583, acc: 0.292969]\n",
      "684: [D loss: 0.676049, acc: 0.572266]  [A loss: 1.111605, acc: 0.050781]\n",
      "685: [D loss: 0.648703, acc: 0.636719]  [A loss: 0.829982, acc: 0.277344]\n",
      "686: [D loss: 0.681873, acc: 0.574219]  [A loss: 1.098071, acc: 0.023438]\n",
      "687: [D loss: 0.669351, acc: 0.601562]  [A loss: 0.934517, acc: 0.128906]\n",
      "688: [D loss: 0.668598, acc: 0.568359]  [A loss: 1.012433, acc: 0.082031]\n",
      "689: [D loss: 0.644521, acc: 0.619141]  [A loss: 0.975512, acc: 0.121094]\n",
      "690: [D loss: 0.689352, acc: 0.578125]  [A loss: 1.028823, acc: 0.066406]\n",
      "691: [D loss: 0.662962, acc: 0.611328]  [A loss: 0.937308, acc: 0.144531]\n",
      "692: [D loss: 0.668031, acc: 0.601562]  [A loss: 1.005353, acc: 0.074219]\n",
      "693: [D loss: 0.670169, acc: 0.582031]  [A loss: 1.038529, acc: 0.062500]\n",
      "694: [D loss: 0.657676, acc: 0.597656]  [A loss: 0.857991, acc: 0.218750]\n",
      "695: [D loss: 0.689723, acc: 0.574219]  [A loss: 1.255235, acc: 0.007812]\n",
      "696: [D loss: 0.678259, acc: 0.583984]  [A loss: 0.665351, acc: 0.585938]\n",
      "697: [D loss: 0.710716, acc: 0.554688]  [A loss: 1.240456, acc: 0.007812]\n",
      "698: [D loss: 0.647579, acc: 0.603516]  [A loss: 0.724468, acc: 0.453125]\n",
      "699: [D loss: 0.767919, acc: 0.498047]  [A loss: 1.201382, acc: 0.007812]\n",
      "700: [D loss: 0.680672, acc: 0.572266]  [A loss: 0.797164, acc: 0.316406]\n",
      "701: [D loss: 0.693754, acc: 0.539062]  [A loss: 1.101985, acc: 0.027344]\n",
      "702: [D loss: 0.675097, acc: 0.550781]  [A loss: 0.774143, acc: 0.347656]\n",
      "703: [D loss: 0.683632, acc: 0.544922]  [A loss: 1.033679, acc: 0.039062]\n",
      "704: [D loss: 0.638706, acc: 0.617188]  [A loss: 0.870630, acc: 0.218750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705: [D loss: 0.673358, acc: 0.560547]  [A loss: 1.079425, acc: 0.035156]\n",
      "706: [D loss: 0.655156, acc: 0.599609]  [A loss: 0.842674, acc: 0.250000]\n",
      "707: [D loss: 0.678683, acc: 0.554688]  [A loss: 1.038140, acc: 0.089844]\n",
      "708: [D loss: 0.658771, acc: 0.626953]  [A loss: 0.819772, acc: 0.269531]\n",
      "709: [D loss: 0.681580, acc: 0.541016]  [A loss: 1.104068, acc: 0.023438]\n",
      "710: [D loss: 0.644845, acc: 0.630859]  [A loss: 0.854271, acc: 0.226562]\n",
      "711: [D loss: 0.666263, acc: 0.589844]  [A loss: 1.109000, acc: 0.035156]\n",
      "712: [D loss: 0.660093, acc: 0.601562]  [A loss: 0.851461, acc: 0.218750]\n",
      "713: [D loss: 0.668881, acc: 0.574219]  [A loss: 1.050351, acc: 0.054688]\n",
      "714: [D loss: 0.654571, acc: 0.636719]  [A loss: 0.786532, acc: 0.312500]\n",
      "715: [D loss: 0.697232, acc: 0.542969]  [A loss: 1.181427, acc: 0.035156]\n",
      "716: [D loss: 0.644749, acc: 0.634766]  [A loss: 0.732471, acc: 0.406250]\n",
      "717: [D loss: 0.691793, acc: 0.539062]  [A loss: 1.201725, acc: 0.015625]\n",
      "718: [D loss: 0.661312, acc: 0.591797]  [A loss: 0.745853, acc: 0.425781]\n",
      "719: [D loss: 0.716811, acc: 0.544922]  [A loss: 1.202749, acc: 0.015625]\n",
      "720: [D loss: 0.663704, acc: 0.603516]  [A loss: 0.851425, acc: 0.269531]\n",
      "721: [D loss: 0.703398, acc: 0.539062]  [A loss: 1.047596, acc: 0.085938]\n",
      "722: [D loss: 0.661246, acc: 0.609375]  [A loss: 0.901847, acc: 0.144531]\n",
      "723: [D loss: 0.679060, acc: 0.556641]  [A loss: 0.978301, acc: 0.082031]\n",
      "724: [D loss: 0.659747, acc: 0.617188]  [A loss: 0.878269, acc: 0.199219]\n",
      "725: [D loss: 0.662721, acc: 0.578125]  [A loss: 0.991086, acc: 0.125000]\n",
      "726: [D loss: 0.672460, acc: 0.578125]  [A loss: 0.975729, acc: 0.132812]\n",
      "727: [D loss: 0.684430, acc: 0.556641]  [A loss: 1.024733, acc: 0.093750]\n",
      "728: [D loss: 0.661851, acc: 0.609375]  [A loss: 0.869539, acc: 0.242188]\n",
      "729: [D loss: 0.676750, acc: 0.552734]  [A loss: 1.071622, acc: 0.046875]\n",
      "730: [D loss: 0.665121, acc: 0.607422]  [A loss: 0.800361, acc: 0.335938]\n",
      "731: [D loss: 0.702281, acc: 0.568359]  [A loss: 1.191461, acc: 0.027344]\n",
      "732: [D loss: 0.679084, acc: 0.595703]  [A loss: 0.720764, acc: 0.468750]\n",
      "733: [D loss: 0.725834, acc: 0.537109]  [A loss: 1.245561, acc: 0.015625]\n",
      "734: [D loss: 0.675978, acc: 0.585938]  [A loss: 0.685108, acc: 0.539062]\n",
      "735: [D loss: 0.730049, acc: 0.525391]  [A loss: 1.239895, acc: 0.042969]\n",
      "736: [D loss: 0.667649, acc: 0.593750]  [A loss: 0.742629, acc: 0.437500]\n",
      "737: [D loss: 0.734084, acc: 0.503906]  [A loss: 1.076230, acc: 0.039062]\n",
      "738: [D loss: 0.654750, acc: 0.613281]  [A loss: 0.778574, acc: 0.367188]\n",
      "739: [D loss: 0.684138, acc: 0.572266]  [A loss: 1.005049, acc: 0.089844]\n",
      "740: [D loss: 0.669636, acc: 0.568359]  [A loss: 0.842377, acc: 0.250000]\n",
      "741: [D loss: 0.665745, acc: 0.564453]  [A loss: 0.967087, acc: 0.101562]\n",
      "742: [D loss: 0.664916, acc: 0.544922]  [A loss: 0.917081, acc: 0.164062]\n",
      "743: [D loss: 0.660649, acc: 0.607422]  [A loss: 0.979921, acc: 0.101562]\n",
      "744: [D loss: 0.660088, acc: 0.626953]  [A loss: 0.962813, acc: 0.128906]\n",
      "745: [D loss: 0.681532, acc: 0.578125]  [A loss: 1.024538, acc: 0.062500]\n",
      "746: [D loss: 0.647239, acc: 0.632812]  [A loss: 0.926762, acc: 0.171875]\n",
      "747: [D loss: 0.658042, acc: 0.609375]  [A loss: 1.072726, acc: 0.085938]\n",
      "748: [D loss: 0.675722, acc: 0.552734]  [A loss: 0.838850, acc: 0.273438]\n",
      "749: [D loss: 0.677601, acc: 0.564453]  [A loss: 1.101888, acc: 0.042969]\n",
      "750: [D loss: 0.651402, acc: 0.625000]  [A loss: 0.836747, acc: 0.261719]\n",
      "751: [D loss: 0.713281, acc: 0.529297]  [A loss: 1.232845, acc: 0.023438]\n",
      "752: [D loss: 0.669916, acc: 0.548828]  [A loss: 0.740840, acc: 0.410156]\n",
      "753: [D loss: 0.728341, acc: 0.537109]  [A loss: 1.226985, acc: 0.011719]\n",
      "754: [D loss: 0.683235, acc: 0.556641]  [A loss: 0.727281, acc: 0.429688]\n",
      "755: [D loss: 0.704525, acc: 0.531250]  [A loss: 1.142889, acc: 0.015625]\n",
      "756: [D loss: 0.674228, acc: 0.580078]  [A loss: 0.823048, acc: 0.269531]\n",
      "757: [D loss: 0.699600, acc: 0.531250]  [A loss: 1.051153, acc: 0.054688]\n",
      "758: [D loss: 0.675190, acc: 0.550781]  [A loss: 0.809903, acc: 0.257812]\n",
      "759: [D loss: 0.683375, acc: 0.562500]  [A loss: 1.019926, acc: 0.074219]\n",
      "760: [D loss: 0.669457, acc: 0.603516]  [A loss: 0.871733, acc: 0.218750]\n",
      "761: [D loss: 0.687449, acc: 0.560547]  [A loss: 1.044820, acc: 0.039062]\n",
      "762: [D loss: 0.658141, acc: 0.625000]  [A loss: 0.880896, acc: 0.175781]\n",
      "763: [D loss: 0.658304, acc: 0.609375]  [A loss: 0.944748, acc: 0.128906]\n",
      "764: [D loss: 0.658389, acc: 0.607422]  [A loss: 1.007932, acc: 0.121094]\n",
      "765: [D loss: 0.674185, acc: 0.566406]  [A loss: 0.915149, acc: 0.167969]\n",
      "766: [D loss: 0.656425, acc: 0.605469]  [A loss: 0.941523, acc: 0.136719]\n",
      "767: [D loss: 0.671029, acc: 0.587891]  [A loss: 1.019826, acc: 0.074219]\n",
      "768: [D loss: 0.672276, acc: 0.570312]  [A loss: 0.887855, acc: 0.171875]\n",
      "769: [D loss: 0.676223, acc: 0.601562]  [A loss: 1.137166, acc: 0.027344]\n",
      "770: [D loss: 0.665714, acc: 0.580078]  [A loss: 0.760988, acc: 0.394531]\n",
      "771: [D loss: 0.722854, acc: 0.527344]  [A loss: 1.225703, acc: 0.035156]\n",
      "772: [D loss: 0.672374, acc: 0.595703]  [A loss: 0.656130, acc: 0.593750]\n",
      "773: [D loss: 0.727121, acc: 0.517578]  [A loss: 1.240771, acc: 0.035156]\n",
      "774: [D loss: 0.663870, acc: 0.589844]  [A loss: 0.896139, acc: 0.175781]\n",
      "775: [D loss: 0.700111, acc: 0.554688]  [A loss: 1.012759, acc: 0.054688]\n",
      "776: [D loss: 0.673581, acc: 0.570312]  [A loss: 0.865565, acc: 0.230469]\n",
      "777: [D loss: 0.657061, acc: 0.589844]  [A loss: 0.973426, acc: 0.117188]\n",
      "778: [D loss: 0.679219, acc: 0.533203]  [A loss: 0.876222, acc: 0.226562]\n",
      "779: [D loss: 0.666211, acc: 0.595703]  [A loss: 1.023041, acc: 0.101562]\n",
      "780: [D loss: 0.693705, acc: 0.546875]  [A loss: 0.868330, acc: 0.210938]\n",
      "781: [D loss: 0.673300, acc: 0.589844]  [A loss: 1.062870, acc: 0.046875]\n",
      "782: [D loss: 0.654418, acc: 0.605469]  [A loss: 0.792881, acc: 0.316406]\n",
      "783: [D loss: 0.695139, acc: 0.541016]  [A loss: 1.218938, acc: 0.019531]\n",
      "784: [D loss: 0.654844, acc: 0.599609]  [A loss: 0.762573, acc: 0.394531]\n",
      "785: [D loss: 0.684928, acc: 0.560547]  [A loss: 1.203256, acc: 0.031250]\n",
      "786: [D loss: 0.659086, acc: 0.597656]  [A loss: 0.702288, acc: 0.507812]\n",
      "787: [D loss: 0.706130, acc: 0.529297]  [A loss: 1.243487, acc: 0.015625]\n",
      "788: [D loss: 0.670420, acc: 0.597656]  [A loss: 0.684909, acc: 0.542969]\n",
      "789: [D loss: 0.771589, acc: 0.511719]  [A loss: 1.164283, acc: 0.023438]\n",
      "790: [D loss: 0.664596, acc: 0.591797]  [A loss: 0.747750, acc: 0.390625]\n",
      "791: [D loss: 0.700642, acc: 0.556641]  [A loss: 1.084279, acc: 0.042969]\n",
      "792: [D loss: 0.655385, acc: 0.623047]  [A loss: 0.810258, acc: 0.316406]\n",
      "793: [D loss: 0.713841, acc: 0.507812]  [A loss: 1.031482, acc: 0.066406]\n",
      "794: [D loss: 0.663195, acc: 0.583984]  [A loss: 0.846713, acc: 0.257812]\n",
      "795: [D loss: 0.695558, acc: 0.552734]  [A loss: 0.969597, acc: 0.109375]\n",
      "796: [D loss: 0.668430, acc: 0.576172]  [A loss: 0.979402, acc: 0.105469]\n",
      "797: [D loss: 0.670088, acc: 0.572266]  [A loss: 0.972063, acc: 0.121094]\n",
      "798: [D loss: 0.663247, acc: 0.587891]  [A loss: 0.933692, acc: 0.121094]\n",
      "799: [D loss: 0.671805, acc: 0.593750]  [A loss: 0.993802, acc: 0.082031]\n",
      "800: [D loss: 0.679462, acc: 0.580078]  [A loss: 0.995368, acc: 0.097656]\n",
      "801: [D loss: 0.663688, acc: 0.578125]  [A loss: 0.961452, acc: 0.125000]\n",
      "802: [D loss: 0.668138, acc: 0.591797]  [A loss: 0.995190, acc: 0.085938]\n",
      "803: [D loss: 0.641623, acc: 0.626953]  [A loss: 0.913429, acc: 0.195312]\n",
      "804: [D loss: 0.715342, acc: 0.546875]  [A loss: 1.190382, acc: 0.019531]\n",
      "805: [D loss: 0.680825, acc: 0.535156]  [A loss: 0.721757, acc: 0.464844]\n",
      "806: [D loss: 0.683713, acc: 0.560547]  [A loss: 1.262364, acc: 0.011719]\n",
      "807: [D loss: 0.660382, acc: 0.611328]  [A loss: 0.739867, acc: 0.441406]\n",
      "808: [D loss: 0.715361, acc: 0.521484]  [A loss: 1.222979, acc: 0.015625]\n",
      "809: [D loss: 0.651563, acc: 0.621094]  [A loss: 0.722563, acc: 0.503906]\n",
      "810: [D loss: 0.692687, acc: 0.541016]  [A loss: 1.069403, acc: 0.066406]\n",
      "811: [D loss: 0.677351, acc: 0.574219]  [A loss: 0.809391, acc: 0.277344]\n",
      "812: [D loss: 0.676298, acc: 0.583984]  [A loss: 1.010051, acc: 0.070312]\n",
      "813: [D loss: 0.664203, acc: 0.591797]  [A loss: 0.818053, acc: 0.289062]\n",
      "814: [D loss: 0.701345, acc: 0.554688]  [A loss: 1.124387, acc: 0.027344]\n",
      "815: [D loss: 0.653716, acc: 0.599609]  [A loss: 0.752084, acc: 0.371094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816: [D loss: 0.717768, acc: 0.533203]  [A loss: 1.059140, acc: 0.074219]\n",
      "817: [D loss: 0.646630, acc: 0.626953]  [A loss: 0.793701, acc: 0.289062]\n",
      "818: [D loss: 0.686559, acc: 0.572266]  [A loss: 1.052375, acc: 0.074219]\n",
      "819: [D loss: 0.669589, acc: 0.578125]  [A loss: 0.815159, acc: 0.292969]\n",
      "820: [D loss: 0.676701, acc: 0.576172]  [A loss: 1.090621, acc: 0.035156]\n",
      "821: [D loss: 0.648333, acc: 0.621094]  [A loss: 0.756872, acc: 0.382812]\n",
      "822: [D loss: 0.695841, acc: 0.570312]  [A loss: 1.121686, acc: 0.039062]\n",
      "823: [D loss: 0.662874, acc: 0.603516]  [A loss: 0.781709, acc: 0.363281]\n",
      "824: [D loss: 0.705596, acc: 0.548828]  [A loss: 1.187575, acc: 0.039062]\n",
      "825: [D loss: 0.677282, acc: 0.587891]  [A loss: 0.789486, acc: 0.320312]\n",
      "826: [D loss: 0.700935, acc: 0.548828]  [A loss: 1.041829, acc: 0.039062]\n",
      "827: [D loss: 0.667779, acc: 0.585938]  [A loss: 0.822821, acc: 0.332031]\n",
      "828: [D loss: 0.684322, acc: 0.593750]  [A loss: 1.022444, acc: 0.066406]\n",
      "829: [D loss: 0.652548, acc: 0.603516]  [A loss: 0.829930, acc: 0.261719]\n",
      "830: [D loss: 0.663901, acc: 0.605469]  [A loss: 1.082397, acc: 0.035156]\n",
      "831: [D loss: 0.667384, acc: 0.603516]  [A loss: 0.797471, acc: 0.320312]\n",
      "832: [D loss: 0.685093, acc: 0.554688]  [A loss: 1.071005, acc: 0.035156]\n",
      "833: [D loss: 0.652849, acc: 0.617188]  [A loss: 0.848361, acc: 0.265625]\n",
      "834: [D loss: 0.715988, acc: 0.535156]  [A loss: 1.096344, acc: 0.042969]\n",
      "835: [D loss: 0.673140, acc: 0.574219]  [A loss: 0.809929, acc: 0.289062]\n",
      "836: [D loss: 0.688664, acc: 0.556641]  [A loss: 1.150866, acc: 0.046875]\n",
      "837: [D loss: 0.676005, acc: 0.570312]  [A loss: 0.839567, acc: 0.269531]\n",
      "838: [D loss: 0.704058, acc: 0.542969]  [A loss: 1.085993, acc: 0.050781]\n",
      "839: [D loss: 0.648510, acc: 0.638672]  [A loss: 0.808690, acc: 0.289062]\n",
      "840: [D loss: 0.679984, acc: 0.578125]  [A loss: 1.123736, acc: 0.050781]\n",
      "841: [D loss: 0.658439, acc: 0.611328]  [A loss: 0.772642, acc: 0.363281]\n",
      "842: [D loss: 0.672370, acc: 0.566406]  [A loss: 1.195450, acc: 0.015625]\n",
      "843: [D loss: 0.646258, acc: 0.623047]  [A loss: 0.768763, acc: 0.394531]\n",
      "844: [D loss: 0.735567, acc: 0.529297]  [A loss: 1.282771, acc: 0.011719]\n",
      "845: [D loss: 0.680349, acc: 0.558594]  [A loss: 0.692109, acc: 0.558594]\n",
      "846: [D loss: 0.714772, acc: 0.556641]  [A loss: 1.138495, acc: 0.015625]\n",
      "847: [D loss: 0.666108, acc: 0.599609]  [A loss: 0.818171, acc: 0.238281]\n",
      "848: [D loss: 0.684647, acc: 0.570312]  [A loss: 1.062473, acc: 0.070312]\n",
      "849: [D loss: 0.667810, acc: 0.599609]  [A loss: 0.876465, acc: 0.175781]\n",
      "850: [D loss: 0.684217, acc: 0.556641]  [A loss: 1.011517, acc: 0.066406]\n",
      "851: [D loss: 0.679270, acc: 0.568359]  [A loss: 0.922323, acc: 0.136719]\n",
      "852: [D loss: 0.661623, acc: 0.572266]  [A loss: 0.997234, acc: 0.117188]\n",
      "853: [D loss: 0.679225, acc: 0.539062]  [A loss: 0.920459, acc: 0.160156]\n",
      "854: [D loss: 0.660564, acc: 0.593750]  [A loss: 1.072836, acc: 0.066406]\n",
      "855: [D loss: 0.638106, acc: 0.630859]  [A loss: 0.845283, acc: 0.250000]\n",
      "856: [D loss: 0.705707, acc: 0.550781]  [A loss: 1.195585, acc: 0.042969]\n",
      "857: [D loss: 0.675156, acc: 0.576172]  [A loss: 0.667588, acc: 0.625000]\n",
      "858: [D loss: 0.740803, acc: 0.517578]  [A loss: 1.272285, acc: 0.031250]\n",
      "859: [D loss: 0.675900, acc: 0.582031]  [A loss: 0.703450, acc: 0.531250]\n",
      "860: [D loss: 0.733527, acc: 0.525391]  [A loss: 1.153050, acc: 0.015625]\n",
      "861: [D loss: 0.678808, acc: 0.564453]  [A loss: 0.781221, acc: 0.359375]\n",
      "862: [D loss: 0.688130, acc: 0.527344]  [A loss: 1.019507, acc: 0.093750]\n",
      "863: [D loss: 0.672090, acc: 0.595703]  [A loss: 0.774919, acc: 0.355469]\n",
      "864: [D loss: 0.673377, acc: 0.558594]  [A loss: 0.975460, acc: 0.125000]\n",
      "865: [D loss: 0.685129, acc: 0.562500]  [A loss: 0.883364, acc: 0.195312]\n",
      "866: [D loss: 0.671364, acc: 0.582031]  [A loss: 1.032925, acc: 0.097656]\n",
      "867: [D loss: 0.675726, acc: 0.564453]  [A loss: 0.952044, acc: 0.148438]\n",
      "868: [D loss: 0.675219, acc: 0.580078]  [A loss: 0.916504, acc: 0.148438]\n",
      "869: [D loss: 0.692170, acc: 0.570312]  [A loss: 0.986255, acc: 0.105469]\n",
      "870: [D loss: 0.673382, acc: 0.576172]  [A loss: 0.887030, acc: 0.222656]\n",
      "871: [D loss: 0.656098, acc: 0.623047]  [A loss: 0.984896, acc: 0.105469]\n",
      "872: [D loss: 0.669657, acc: 0.589844]  [A loss: 0.926613, acc: 0.164062]\n",
      "873: [D loss: 0.673577, acc: 0.578125]  [A loss: 1.024631, acc: 0.097656]\n",
      "874: [D loss: 0.671723, acc: 0.585938]  [A loss: 0.884908, acc: 0.183594]\n",
      "875: [D loss: 0.677613, acc: 0.570312]  [A loss: 1.050954, acc: 0.058594]\n",
      "876: [D loss: 0.678590, acc: 0.572266]  [A loss: 0.967964, acc: 0.140625]\n",
      "877: [D loss: 0.677953, acc: 0.554688]  [A loss: 1.050717, acc: 0.062500]\n",
      "878: [D loss: 0.673051, acc: 0.589844]  [A loss: 0.896964, acc: 0.179688]\n",
      "879: [D loss: 0.704275, acc: 0.537109]  [A loss: 1.191735, acc: 0.039062]\n",
      "880: [D loss: 0.675146, acc: 0.566406]  [A loss: 0.717884, acc: 0.457031]\n",
      "881: [D loss: 0.731264, acc: 0.519531]  [A loss: 1.323152, acc: 0.003906]\n",
      "882: [D loss: 0.688843, acc: 0.550781]  [A loss: 0.563239, acc: 0.792969]\n",
      "883: [D loss: 0.756836, acc: 0.525391]  [A loss: 1.282012, acc: 0.011719]\n",
      "884: [D loss: 0.683642, acc: 0.558594]  [A loss: 0.673242, acc: 0.597656]\n",
      "885: [D loss: 0.734967, acc: 0.515625]  [A loss: 1.073348, acc: 0.050781]\n",
      "886: [D loss: 0.670948, acc: 0.566406]  [A loss: 0.806040, acc: 0.312500]\n",
      "887: [D loss: 0.691921, acc: 0.552734]  [A loss: 1.051414, acc: 0.089844]\n",
      "888: [D loss: 0.672851, acc: 0.589844]  [A loss: 0.783724, acc: 0.382812]\n",
      "889: [D loss: 0.695436, acc: 0.564453]  [A loss: 1.107335, acc: 0.066406]\n",
      "890: [D loss: 0.675658, acc: 0.570312]  [A loss: 0.764743, acc: 0.378906]\n",
      "891: [D loss: 0.716469, acc: 0.521484]  [A loss: 1.104082, acc: 0.050781]\n",
      "892: [D loss: 0.672413, acc: 0.570312]  [A loss: 0.791099, acc: 0.335938]\n",
      "893: [D loss: 0.686937, acc: 0.578125]  [A loss: 1.055658, acc: 0.066406]\n",
      "894: [D loss: 0.666776, acc: 0.609375]  [A loss: 0.841581, acc: 0.234375]\n",
      "895: [D loss: 0.712129, acc: 0.542969]  [A loss: 1.132815, acc: 0.027344]\n",
      "896: [D loss: 0.677938, acc: 0.578125]  [A loss: 0.806746, acc: 0.292969]\n",
      "897: [D loss: 0.679630, acc: 0.554688]  [A loss: 1.053875, acc: 0.066406]\n",
      "898: [D loss: 0.677501, acc: 0.568359]  [A loss: 0.813078, acc: 0.277344]\n",
      "899: [D loss: 0.699131, acc: 0.535156]  [A loss: 1.072887, acc: 0.062500]\n",
      "900: [D loss: 0.659726, acc: 0.601562]  [A loss: 0.791785, acc: 0.320312]\n",
      "901: [D loss: 0.711782, acc: 0.535156]  [A loss: 1.118469, acc: 0.011719]\n",
      "902: [D loss: 0.667461, acc: 0.607422]  [A loss: 0.733842, acc: 0.457031]\n",
      "903: [D loss: 0.683196, acc: 0.568359]  [A loss: 1.115877, acc: 0.039062]\n",
      "904: [D loss: 0.670760, acc: 0.578125]  [A loss: 0.841867, acc: 0.246094]\n",
      "905: [D loss: 0.693590, acc: 0.550781]  [A loss: 1.121969, acc: 0.039062]\n",
      "906: [D loss: 0.672892, acc: 0.597656]  [A loss: 0.765401, acc: 0.410156]\n",
      "907: [D loss: 0.716346, acc: 0.503906]  [A loss: 1.155377, acc: 0.035156]\n",
      "908: [D loss: 0.668904, acc: 0.572266]  [A loss: 0.765832, acc: 0.378906]\n",
      "909: [D loss: 0.704516, acc: 0.537109]  [A loss: 1.109737, acc: 0.058594]\n",
      "910: [D loss: 0.669593, acc: 0.597656]  [A loss: 0.770952, acc: 0.367188]\n",
      "911: [D loss: 0.677112, acc: 0.578125]  [A loss: 1.093157, acc: 0.058594]\n",
      "912: [D loss: 0.677830, acc: 0.564453]  [A loss: 0.849199, acc: 0.257812]\n",
      "913: [D loss: 0.694794, acc: 0.541016]  [A loss: 1.073291, acc: 0.054688]\n",
      "914: [D loss: 0.667885, acc: 0.578125]  [A loss: 0.736635, acc: 0.449219]\n",
      "915: [D loss: 0.716101, acc: 0.523438]  [A loss: 1.193683, acc: 0.027344]\n",
      "916: [D loss: 0.676525, acc: 0.574219]  [A loss: 0.714216, acc: 0.488281]\n",
      "917: [D loss: 0.710650, acc: 0.533203]  [A loss: 1.133838, acc: 0.023438]\n",
      "918: [D loss: 0.661421, acc: 0.582031]  [A loss: 0.754281, acc: 0.410156]\n",
      "919: [D loss: 0.744598, acc: 0.521484]  [A loss: 1.054801, acc: 0.078125]\n",
      "920: [D loss: 0.669450, acc: 0.603516]  [A loss: 0.809976, acc: 0.328125]\n",
      "921: [D loss: 0.705917, acc: 0.529297]  [A loss: 1.056150, acc: 0.066406]\n",
      "922: [D loss: 0.664779, acc: 0.585938]  [A loss: 0.815428, acc: 0.289062]\n",
      "923: [D loss: 0.713638, acc: 0.533203]  [A loss: 1.084556, acc: 0.042969]\n",
      "924: [D loss: 0.677149, acc: 0.572266]  [A loss: 0.801968, acc: 0.285156]\n",
      "925: [D loss: 0.687769, acc: 0.550781]  [A loss: 1.003342, acc: 0.074219]\n",
      "926: [D loss: 0.667264, acc: 0.583984]  [A loss: 0.830572, acc: 0.289062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927: [D loss: 0.682719, acc: 0.585938]  [A loss: 1.005780, acc: 0.074219]\n",
      "928: [D loss: 0.667974, acc: 0.597656]  [A loss: 0.839367, acc: 0.261719]\n",
      "929: [D loss: 0.677374, acc: 0.572266]  [A loss: 1.006034, acc: 0.070312]\n",
      "930: [D loss: 0.682726, acc: 0.556641]  [A loss: 0.811679, acc: 0.300781]\n",
      "931: [D loss: 0.710610, acc: 0.533203]  [A loss: 1.180547, acc: 0.039062]\n",
      "932: [D loss: 0.666616, acc: 0.591797]  [A loss: 0.741732, acc: 0.398438]\n",
      "933: [D loss: 0.712641, acc: 0.544922]  [A loss: 1.131222, acc: 0.023438]\n",
      "934: [D loss: 0.655786, acc: 0.638672]  [A loss: 0.741831, acc: 0.445312]\n",
      "935: [D loss: 0.704555, acc: 0.544922]  [A loss: 1.118421, acc: 0.039062]\n",
      "936: [D loss: 0.660906, acc: 0.607422]  [A loss: 0.766376, acc: 0.406250]\n",
      "937: [D loss: 0.691878, acc: 0.564453]  [A loss: 1.131383, acc: 0.031250]\n",
      "938: [D loss: 0.678383, acc: 0.582031]  [A loss: 0.737599, acc: 0.417969]\n",
      "939: [D loss: 0.709977, acc: 0.560547]  [A loss: 1.125433, acc: 0.023438]\n",
      "940: [D loss: 0.660642, acc: 0.605469]  [A loss: 0.738464, acc: 0.402344]\n",
      "941: [D loss: 0.696217, acc: 0.537109]  [A loss: 1.092065, acc: 0.039062]\n",
      "942: [D loss: 0.685023, acc: 0.554688]  [A loss: 0.785943, acc: 0.375000]\n",
      "943: [D loss: 0.678622, acc: 0.572266]  [A loss: 1.004813, acc: 0.085938]\n",
      "944: [D loss: 0.692437, acc: 0.531250]  [A loss: 0.842236, acc: 0.238281]\n",
      "945: [D loss: 0.684836, acc: 0.525391]  [A loss: 1.039913, acc: 0.062500]\n",
      "946: [D loss: 0.665398, acc: 0.568359]  [A loss: 0.840043, acc: 0.277344]\n",
      "947: [D loss: 0.714681, acc: 0.511719]  [A loss: 1.060394, acc: 0.058594]\n",
      "948: [D loss: 0.674274, acc: 0.570312]  [A loss: 0.823180, acc: 0.261719]\n",
      "949: [D loss: 0.706848, acc: 0.541016]  [A loss: 1.095939, acc: 0.054688]\n",
      "950: [D loss: 0.665964, acc: 0.576172]  [A loss: 0.815038, acc: 0.320312]\n",
      "951: [D loss: 0.691247, acc: 0.570312]  [A loss: 1.081233, acc: 0.054688]\n",
      "952: [D loss: 0.671637, acc: 0.570312]  [A loss: 0.784709, acc: 0.339844]\n",
      "953: [D loss: 0.691208, acc: 0.562500]  [A loss: 1.097085, acc: 0.039062]\n",
      "954: [D loss: 0.667257, acc: 0.597656]  [A loss: 0.757203, acc: 0.398438]\n",
      "955: [D loss: 0.730973, acc: 0.513672]  [A loss: 1.172370, acc: 0.042969]\n",
      "956: [D loss: 0.666178, acc: 0.593750]  [A loss: 0.680125, acc: 0.570312]\n",
      "957: [D loss: 0.708444, acc: 0.541016]  [A loss: 1.141747, acc: 0.058594]\n",
      "958: [D loss: 0.654983, acc: 0.613281]  [A loss: 0.725835, acc: 0.425781]\n",
      "959: [D loss: 0.715923, acc: 0.546875]  [A loss: 1.087418, acc: 0.039062]\n",
      "960: [D loss: 0.668189, acc: 0.591797]  [A loss: 0.778086, acc: 0.390625]\n",
      "961: [D loss: 0.688371, acc: 0.568359]  [A loss: 1.026118, acc: 0.078125]\n",
      "962: [D loss: 0.663684, acc: 0.609375]  [A loss: 0.782751, acc: 0.355469]\n",
      "963: [D loss: 0.690510, acc: 0.568359]  [A loss: 1.003436, acc: 0.085938]\n",
      "964: [D loss: 0.676664, acc: 0.558594]  [A loss: 0.826901, acc: 0.312500]\n",
      "965: [D loss: 0.683087, acc: 0.552734]  [A loss: 1.062295, acc: 0.046875]\n",
      "966: [D loss: 0.660310, acc: 0.583984]  [A loss: 0.803274, acc: 0.304688]\n",
      "967: [D loss: 0.692933, acc: 0.539062]  [A loss: 1.085197, acc: 0.054688]\n",
      "968: [D loss: 0.709929, acc: 0.523438]  [A loss: 0.808352, acc: 0.296875]\n",
      "969: [D loss: 0.684426, acc: 0.548828]  [A loss: 1.152405, acc: 0.031250]\n",
      "970: [D loss: 0.687917, acc: 0.542969]  [A loss: 0.802811, acc: 0.324219]\n",
      "971: [D loss: 0.674730, acc: 0.591797]  [A loss: 1.051631, acc: 0.062500]\n",
      "972: [D loss: 0.675050, acc: 0.583984]  [A loss: 0.809355, acc: 0.308594]\n",
      "973: [D loss: 0.688164, acc: 0.554688]  [A loss: 1.110346, acc: 0.054688]\n",
      "974: [D loss: 0.675752, acc: 0.572266]  [A loss: 0.762746, acc: 0.394531]\n",
      "975: [D loss: 0.691180, acc: 0.550781]  [A loss: 1.195740, acc: 0.027344]\n",
      "976: [D loss: 0.662428, acc: 0.599609]  [A loss: 0.716705, acc: 0.511719]\n",
      "977: [D loss: 0.724998, acc: 0.539062]  [A loss: 1.164791, acc: 0.023438]\n",
      "978: [D loss: 0.678826, acc: 0.599609]  [A loss: 0.719684, acc: 0.468750]\n",
      "979: [D loss: 0.717465, acc: 0.533203]  [A loss: 1.082736, acc: 0.046875]\n",
      "980: [D loss: 0.664531, acc: 0.597656]  [A loss: 0.792822, acc: 0.335938]\n",
      "981: [D loss: 0.691664, acc: 0.574219]  [A loss: 1.084705, acc: 0.027344]\n",
      "982: [D loss: 0.673312, acc: 0.589844]  [A loss: 0.812898, acc: 0.292969]\n",
      "983: [D loss: 0.696831, acc: 0.541016]  [A loss: 1.114274, acc: 0.050781]\n",
      "984: [D loss: 0.668162, acc: 0.587891]  [A loss: 0.749260, acc: 0.398438]\n",
      "985: [D loss: 0.710685, acc: 0.535156]  [A loss: 1.018936, acc: 0.070312]\n",
      "986: [D loss: 0.684240, acc: 0.550781]  [A loss: 0.835064, acc: 0.246094]\n",
      "987: [D loss: 0.674264, acc: 0.580078]  [A loss: 1.006503, acc: 0.066406]\n",
      "988: [D loss: 0.663933, acc: 0.585938]  [A loss: 0.842530, acc: 0.234375]\n",
      "989: [D loss: 0.680173, acc: 0.583984]  [A loss: 0.972706, acc: 0.128906]\n",
      "990: [D loss: 0.664883, acc: 0.587891]  [A loss: 0.913328, acc: 0.148438]\n",
      "991: [D loss: 0.666041, acc: 0.591797]  [A loss: 0.963067, acc: 0.128906]\n",
      "992: [D loss: 0.676022, acc: 0.580078]  [A loss: 0.904757, acc: 0.148438]\n",
      "993: [D loss: 0.662235, acc: 0.587891]  [A loss: 0.976228, acc: 0.105469]\n",
      "994: [D loss: 0.674330, acc: 0.589844]  [A loss: 0.964881, acc: 0.128906]\n",
      "995: [D loss: 0.683864, acc: 0.552734]  [A loss: 0.945752, acc: 0.144531]\n",
      "996: [D loss: 0.663282, acc: 0.607422]  [A loss: 1.026075, acc: 0.070312]\n",
      "997: [D loss: 0.681902, acc: 0.550781]  [A loss: 0.920010, acc: 0.148438]\n",
      "998: [D loss: 0.683451, acc: 0.552734]  [A loss: 1.191405, acc: 0.007812]\n",
      "999: [D loss: 0.665163, acc: 0.613281]  [A loss: 0.720475, acc: 0.507812]\n",
      "1000: [D loss: 0.726828, acc: 0.513672]  [A loss: 1.396895, acc: 0.000000]\n",
      "1001: [D loss: 0.671158, acc: 0.601562]  [A loss: 0.625076, acc: 0.644531]\n",
      "1002: [D loss: 0.775565, acc: 0.500000]  [A loss: 1.095117, acc: 0.085938]\n",
      "1003: [D loss: 0.699173, acc: 0.552734]  [A loss: 0.794032, acc: 0.312500]\n",
      "1004: [D loss: 0.700484, acc: 0.537109]  [A loss: 1.096906, acc: 0.031250]\n",
      "1005: [D loss: 0.664456, acc: 0.603516]  [A loss: 0.824072, acc: 0.300781]\n",
      "1006: [D loss: 0.689502, acc: 0.570312]  [A loss: 1.066415, acc: 0.054688]\n",
      "1007: [D loss: 0.662131, acc: 0.623047]  [A loss: 0.782417, acc: 0.332031]\n",
      "1008: [D loss: 0.696536, acc: 0.537109]  [A loss: 1.059569, acc: 0.078125]\n",
      "1009: [D loss: 0.663249, acc: 0.621094]  [A loss: 0.834855, acc: 0.285156]\n",
      "1010: [D loss: 0.687896, acc: 0.556641]  [A loss: 1.059824, acc: 0.046875]\n",
      "1011: [D loss: 0.674305, acc: 0.576172]  [A loss: 0.798767, acc: 0.355469]\n",
      "1012: [D loss: 0.670486, acc: 0.591797]  [A loss: 1.117137, acc: 0.050781]\n",
      "1013: [D loss: 0.667088, acc: 0.580078]  [A loss: 0.841579, acc: 0.289062]\n",
      "1014: [D loss: 0.681005, acc: 0.582031]  [A loss: 1.063909, acc: 0.058594]\n",
      "1015: [D loss: 0.666261, acc: 0.609375]  [A loss: 0.854849, acc: 0.234375]\n",
      "1016: [D loss: 0.706808, acc: 0.517578]  [A loss: 1.144931, acc: 0.054688]\n",
      "1017: [D loss: 0.662685, acc: 0.607422]  [A loss: 0.751225, acc: 0.425781]\n",
      "1018: [D loss: 0.707155, acc: 0.533203]  [A loss: 1.105649, acc: 0.058594]\n",
      "1019: [D loss: 0.684974, acc: 0.558594]  [A loss: 0.752878, acc: 0.425781]\n",
      "1020: [D loss: 0.709742, acc: 0.550781]  [A loss: 1.129193, acc: 0.031250]\n",
      "1021: [D loss: 0.672472, acc: 0.556641]  [A loss: 0.685005, acc: 0.542969]\n",
      "1022: [D loss: 0.733276, acc: 0.519531]  [A loss: 1.059441, acc: 0.066406]\n",
      "1023: [D loss: 0.685843, acc: 0.558594]  [A loss: 0.839506, acc: 0.265625]\n",
      "1024: [D loss: 0.692605, acc: 0.560547]  [A loss: 1.100906, acc: 0.046875]\n",
      "1025: [D loss: 0.695445, acc: 0.548828]  [A loss: 0.883335, acc: 0.222656]\n",
      "1026: [D loss: 0.675018, acc: 0.583984]  [A loss: 0.959976, acc: 0.125000]\n",
      "1027: [D loss: 0.664727, acc: 0.605469]  [A loss: 0.917822, acc: 0.175781]\n",
      "1028: [D loss: 0.673618, acc: 0.572266]  [A loss: 0.951482, acc: 0.152344]\n",
      "1029: [D loss: 0.689306, acc: 0.554688]  [A loss: 0.933772, acc: 0.171875]\n",
      "1030: [D loss: 0.662491, acc: 0.603516]  [A loss: 0.997105, acc: 0.093750]\n",
      "1031: [D loss: 0.663538, acc: 0.580078]  [A loss: 0.969697, acc: 0.121094]\n",
      "1032: [D loss: 0.669618, acc: 0.589844]  [A loss: 0.895651, acc: 0.207031]\n",
      "1033: [D loss: 0.689744, acc: 0.572266]  [A loss: 1.056241, acc: 0.082031]\n",
      "1034: [D loss: 0.665948, acc: 0.582031]  [A loss: 0.789417, acc: 0.355469]\n",
      "1035: [D loss: 0.714443, acc: 0.554688]  [A loss: 1.249596, acc: 0.019531]\n",
      "1036: [D loss: 0.669146, acc: 0.603516]  [A loss: 0.635583, acc: 0.671875]\n",
      "1037: [D loss: 0.724980, acc: 0.523438]  [A loss: 1.349557, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038: [D loss: 0.705352, acc: 0.529297]  [A loss: 0.703965, acc: 0.515625]\n",
      "1039: [D loss: 0.700589, acc: 0.541016]  [A loss: 1.082454, acc: 0.066406]\n",
      "1040: [D loss: 0.658833, acc: 0.589844]  [A loss: 0.777722, acc: 0.382812]\n",
      "1041: [D loss: 0.692165, acc: 0.542969]  [A loss: 1.048900, acc: 0.082031]\n",
      "1042: [D loss: 0.648914, acc: 0.634766]  [A loss: 0.785390, acc: 0.371094]\n",
      "1043: [D loss: 0.696465, acc: 0.572266]  [A loss: 1.074899, acc: 0.054688]\n",
      "1044: [D loss: 0.678534, acc: 0.566406]  [A loss: 0.795355, acc: 0.316406]\n",
      "1045: [D loss: 0.675666, acc: 0.580078]  [A loss: 1.053611, acc: 0.062500]\n",
      "1046: [D loss: 0.661405, acc: 0.609375]  [A loss: 0.851308, acc: 0.289062]\n",
      "1047: [D loss: 0.676702, acc: 0.554688]  [A loss: 0.981563, acc: 0.132812]\n",
      "1048: [D loss: 0.667078, acc: 0.617188]  [A loss: 0.804330, acc: 0.328125]\n",
      "1049: [D loss: 0.693476, acc: 0.539062]  [A loss: 1.082303, acc: 0.054688]\n",
      "1050: [D loss: 0.670379, acc: 0.587891]  [A loss: 0.736650, acc: 0.480469]\n",
      "1051: [D loss: 0.705332, acc: 0.552734]  [A loss: 1.121371, acc: 0.042969]\n",
      "1052: [D loss: 0.670231, acc: 0.589844]  [A loss: 0.733067, acc: 0.449219]\n",
      "1053: [D loss: 0.676166, acc: 0.564453]  [A loss: 1.119771, acc: 0.031250]\n",
      "1054: [D loss: 0.673996, acc: 0.582031]  [A loss: 0.789789, acc: 0.359375]\n",
      "1055: [D loss: 0.723088, acc: 0.531250]  [A loss: 1.137392, acc: 0.039062]\n",
      "1056: [D loss: 0.679696, acc: 0.591797]  [A loss: 0.780896, acc: 0.324219]\n",
      "1057: [D loss: 0.687917, acc: 0.544922]  [A loss: 1.119770, acc: 0.042969]\n",
      "1058: [D loss: 0.668929, acc: 0.605469]  [A loss: 0.812544, acc: 0.328125]\n",
      "1059: [D loss: 0.699853, acc: 0.552734]  [A loss: 1.080282, acc: 0.042969]\n",
      "1060: [D loss: 0.659570, acc: 0.599609]  [A loss: 0.758745, acc: 0.417969]\n",
      "1061: [D loss: 0.703812, acc: 0.562500]  [A loss: 1.150177, acc: 0.042969]\n",
      "1062: [D loss: 0.669462, acc: 0.587891]  [A loss: 0.756488, acc: 0.382812]\n",
      "1063: [D loss: 0.697557, acc: 0.554688]  [A loss: 1.110533, acc: 0.054688]\n",
      "1064: [D loss: 0.670804, acc: 0.572266]  [A loss: 0.791033, acc: 0.335938]\n",
      "1065: [D loss: 0.687413, acc: 0.554688]  [A loss: 1.036518, acc: 0.074219]\n",
      "1066: [D loss: 0.669944, acc: 0.593750]  [A loss: 0.835435, acc: 0.281250]\n",
      "1067: [D loss: 0.687662, acc: 0.558594]  [A loss: 1.040013, acc: 0.113281]\n",
      "1068: [D loss: 0.659742, acc: 0.628906]  [A loss: 0.838941, acc: 0.250000]\n",
      "1069: [D loss: 0.694630, acc: 0.548828]  [A loss: 0.964229, acc: 0.144531]\n",
      "1070: [D loss: 0.654705, acc: 0.642578]  [A loss: 0.924201, acc: 0.183594]\n",
      "1071: [D loss: 0.711656, acc: 0.519531]  [A loss: 1.082236, acc: 0.070312]\n",
      "1072: [D loss: 0.672527, acc: 0.593750]  [A loss: 0.800214, acc: 0.332031]\n",
      "1073: [D loss: 0.682897, acc: 0.541016]  [A loss: 1.098690, acc: 0.039062]\n",
      "1074: [D loss: 0.658137, acc: 0.619141]  [A loss: 0.843363, acc: 0.261719]\n",
      "1075: [D loss: 0.680682, acc: 0.582031]  [A loss: 1.103673, acc: 0.085938]\n",
      "1076: [D loss: 0.655540, acc: 0.601562]  [A loss: 0.738656, acc: 0.421875]\n",
      "1077: [D loss: 0.703614, acc: 0.558594]  [A loss: 1.248847, acc: 0.011719]\n",
      "1078: [D loss: 0.686715, acc: 0.599609]  [A loss: 0.729521, acc: 0.453125]\n",
      "1079: [D loss: 0.717339, acc: 0.527344]  [A loss: 1.189783, acc: 0.039062]\n",
      "1080: [D loss: 0.675663, acc: 0.582031]  [A loss: 0.676075, acc: 0.558594]\n",
      "1081: [D loss: 0.718253, acc: 0.535156]  [A loss: 1.161260, acc: 0.050781]\n",
      "1082: [D loss: 0.669748, acc: 0.578125]  [A loss: 0.719993, acc: 0.480469]\n",
      "1083: [D loss: 0.701190, acc: 0.566406]  [A loss: 1.103617, acc: 0.062500]\n",
      "1084: [D loss: 0.658851, acc: 0.613281]  [A loss: 0.806592, acc: 0.324219]\n",
      "1085: [D loss: 0.671218, acc: 0.572266]  [A loss: 0.957677, acc: 0.144531]\n",
      "1086: [D loss: 0.674517, acc: 0.583984]  [A loss: 0.808789, acc: 0.316406]\n",
      "1087: [D loss: 0.681920, acc: 0.582031]  [A loss: 1.042399, acc: 0.085938]\n",
      "1088: [D loss: 0.676385, acc: 0.546875]  [A loss: 0.834977, acc: 0.285156]\n",
      "1089: [D loss: 0.673246, acc: 0.591797]  [A loss: 0.970435, acc: 0.121094]\n",
      "1090: [D loss: 0.666134, acc: 0.587891]  [A loss: 0.909312, acc: 0.199219]\n",
      "1091: [D loss: 0.669157, acc: 0.560547]  [A loss: 1.028243, acc: 0.054688]\n",
      "1092: [D loss: 0.674195, acc: 0.558594]  [A loss: 0.948851, acc: 0.164062]\n",
      "1093: [D loss: 0.685772, acc: 0.560547]  [A loss: 0.954175, acc: 0.195312]\n",
      "1094: [D loss: 0.662957, acc: 0.593750]  [A loss: 1.004084, acc: 0.085938]\n",
      "1095: [D loss: 0.668431, acc: 0.580078]  [A loss: 0.941178, acc: 0.148438]\n",
      "1096: [D loss: 0.663279, acc: 0.593750]  [A loss: 1.030352, acc: 0.105469]\n",
      "1097: [D loss: 0.672327, acc: 0.576172]  [A loss: 0.966512, acc: 0.156250]\n",
      "1098: [D loss: 0.692438, acc: 0.556641]  [A loss: 1.082002, acc: 0.050781]\n",
      "1099: [D loss: 0.681035, acc: 0.568359]  [A loss: 0.967404, acc: 0.156250]\n",
      "1100: [D loss: 0.691654, acc: 0.535156]  [A loss: 0.941404, acc: 0.148438]\n",
      "1101: [D loss: 0.684214, acc: 0.566406]  [A loss: 1.006122, acc: 0.117188]\n",
      "1102: [D loss: 0.679639, acc: 0.562500]  [A loss: 1.100030, acc: 0.050781]\n",
      "1103: [D loss: 0.664336, acc: 0.578125]  [A loss: 0.774178, acc: 0.382812]\n",
      "1104: [D loss: 0.696088, acc: 0.570312]  [A loss: 1.367710, acc: 0.011719]\n",
      "1105: [D loss: 0.702732, acc: 0.562500]  [A loss: 0.595347, acc: 0.714844]\n",
      "1106: [D loss: 0.771030, acc: 0.517578]  [A loss: 1.268566, acc: 0.015625]\n",
      "1107: [D loss: 0.679689, acc: 0.562500]  [A loss: 0.663977, acc: 0.542969]\n",
      "1108: [D loss: 0.694484, acc: 0.535156]  [A loss: 1.019681, acc: 0.093750]\n",
      "1109: [D loss: 0.669678, acc: 0.599609]  [A loss: 0.779349, acc: 0.351562]\n",
      "1110: [D loss: 0.679510, acc: 0.583984]  [A loss: 1.039224, acc: 0.058594]\n",
      "1111: [D loss: 0.650526, acc: 0.599609]  [A loss: 0.763155, acc: 0.402344]\n",
      "1112: [D loss: 0.694621, acc: 0.560547]  [A loss: 0.957025, acc: 0.171875]\n",
      "1113: [D loss: 0.669641, acc: 0.566406]  [A loss: 0.938988, acc: 0.183594]\n",
      "1114: [D loss: 0.683061, acc: 0.554688]  [A loss: 1.033960, acc: 0.085938]\n",
      "1115: [D loss: 0.665668, acc: 0.603516]  [A loss: 0.937129, acc: 0.144531]\n",
      "1116: [D loss: 0.700221, acc: 0.546875]  [A loss: 1.023991, acc: 0.097656]\n",
      "1117: [D loss: 0.651750, acc: 0.625000]  [A loss: 0.831261, acc: 0.296875]\n",
      "1118: [D loss: 0.712709, acc: 0.554688]  [A loss: 1.150631, acc: 0.070312]\n",
      "1119: [D loss: 0.666817, acc: 0.580078]  [A loss: 0.805328, acc: 0.316406]\n",
      "1120: [D loss: 0.690064, acc: 0.558594]  [A loss: 1.067084, acc: 0.089844]\n",
      "1121: [D loss: 0.684599, acc: 0.546875]  [A loss: 0.816169, acc: 0.355469]\n",
      "1122: [D loss: 0.684223, acc: 0.580078]  [A loss: 1.108344, acc: 0.066406]\n",
      "1123: [D loss: 0.667556, acc: 0.572266]  [A loss: 0.733504, acc: 0.468750]\n",
      "1124: [D loss: 0.720869, acc: 0.529297]  [A loss: 1.192724, acc: 0.023438]\n",
      "1125: [D loss: 0.678518, acc: 0.591797]  [A loss: 0.773028, acc: 0.378906]\n",
      "1126: [D loss: 0.701205, acc: 0.548828]  [A loss: 1.122164, acc: 0.039062]\n",
      "1127: [D loss: 0.680861, acc: 0.562500]  [A loss: 0.711555, acc: 0.507812]\n",
      "1128: [D loss: 0.718227, acc: 0.523438]  [A loss: 1.132972, acc: 0.046875]\n",
      "1129: [D loss: 0.661043, acc: 0.585938]  [A loss: 0.681870, acc: 0.546875]\n",
      "1130: [D loss: 0.724992, acc: 0.535156]  [A loss: 1.144768, acc: 0.035156]\n",
      "1131: [D loss: 0.671917, acc: 0.589844]  [A loss: 0.763969, acc: 0.425781]\n",
      "1132: [D loss: 0.734798, acc: 0.542969]  [A loss: 1.075389, acc: 0.101562]\n",
      "1133: [D loss: 0.670195, acc: 0.603516]  [A loss: 0.858835, acc: 0.269531]\n",
      "1134: [D loss: 0.683288, acc: 0.576172]  [A loss: 1.024233, acc: 0.062500]\n",
      "1135: [D loss: 0.661724, acc: 0.607422]  [A loss: 0.888722, acc: 0.191406]\n",
      "1136: [D loss: 0.679512, acc: 0.558594]  [A loss: 0.992984, acc: 0.144531]\n",
      "1137: [D loss: 0.688239, acc: 0.542969]  [A loss: 0.929005, acc: 0.191406]\n",
      "1138: [D loss: 0.676769, acc: 0.582031]  [A loss: 0.973468, acc: 0.125000]\n",
      "1139: [D loss: 0.660475, acc: 0.593750]  [A loss: 0.891634, acc: 0.187500]\n",
      "1140: [D loss: 0.669041, acc: 0.597656]  [A loss: 1.063330, acc: 0.062500]\n",
      "1141: [D loss: 0.680393, acc: 0.574219]  [A loss: 0.842567, acc: 0.292969]\n",
      "1142: [D loss: 0.699952, acc: 0.533203]  [A loss: 1.123450, acc: 0.074219]\n",
      "1143: [D loss: 0.695191, acc: 0.570312]  [A loss: 0.791925, acc: 0.343750]\n",
      "1144: [D loss: 0.697792, acc: 0.548828]  [A loss: 1.186402, acc: 0.042969]\n",
      "1145: [D loss: 0.668315, acc: 0.566406]  [A loss: 0.708470, acc: 0.515625]\n",
      "1146: [D loss: 0.733267, acc: 0.527344]  [A loss: 1.242821, acc: 0.023438]\n",
      "1147: [D loss: 0.681149, acc: 0.583984]  [A loss: 0.622130, acc: 0.707031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148: [D loss: 0.721295, acc: 0.550781]  [A loss: 1.230466, acc: 0.019531]\n",
      "1149: [D loss: 0.687593, acc: 0.548828]  [A loss: 0.745037, acc: 0.433594]\n",
      "1150: [D loss: 0.702569, acc: 0.533203]  [A loss: 1.014793, acc: 0.113281]\n",
      "1151: [D loss: 0.650167, acc: 0.630859]  [A loss: 0.788392, acc: 0.328125]\n",
      "1152: [D loss: 0.699521, acc: 0.552734]  [A loss: 1.029452, acc: 0.117188]\n",
      "1153: [D loss: 0.675070, acc: 0.572266]  [A loss: 0.801347, acc: 0.339844]\n",
      "1154: [D loss: 0.710638, acc: 0.544922]  [A loss: 0.997281, acc: 0.125000]\n",
      "1155: [D loss: 0.670342, acc: 0.615234]  [A loss: 0.789415, acc: 0.343750]\n",
      "1156: [D loss: 0.691328, acc: 0.570312]  [A loss: 1.131619, acc: 0.042969]\n",
      "1157: [D loss: 0.656655, acc: 0.613281]  [A loss: 0.757146, acc: 0.433594]\n",
      "1158: [D loss: 0.695505, acc: 0.537109]  [A loss: 1.087121, acc: 0.046875]\n",
      "1159: [D loss: 0.671193, acc: 0.583984]  [A loss: 0.825219, acc: 0.296875]\n",
      "1160: [D loss: 0.695546, acc: 0.541016]  [A loss: 0.978978, acc: 0.121094]\n",
      "1161: [D loss: 0.679126, acc: 0.585938]  [A loss: 0.856440, acc: 0.222656]\n",
      "1162: [D loss: 0.693714, acc: 0.554688]  [A loss: 1.046486, acc: 0.070312]\n",
      "1163: [D loss: 0.671603, acc: 0.568359]  [A loss: 0.844869, acc: 0.269531]\n",
      "1164: [D loss: 0.693577, acc: 0.541016]  [A loss: 0.992658, acc: 0.144531]\n",
      "1165: [D loss: 0.675780, acc: 0.578125]  [A loss: 0.803840, acc: 0.312500]\n",
      "1166: [D loss: 0.700950, acc: 0.541016]  [A loss: 1.159002, acc: 0.078125]\n",
      "1167: [D loss: 0.687754, acc: 0.576172]  [A loss: 0.816042, acc: 0.308594]\n",
      "1168: [D loss: 0.690915, acc: 0.562500]  [A loss: 1.193740, acc: 0.023438]\n",
      "1169: [D loss: 0.661548, acc: 0.595703]  [A loss: 0.728279, acc: 0.417969]\n",
      "1170: [D loss: 0.719938, acc: 0.527344]  [A loss: 1.212162, acc: 0.023438]\n",
      "1171: [D loss: 0.688686, acc: 0.548828]  [A loss: 0.687994, acc: 0.546875]\n",
      "1172: [D loss: 0.725484, acc: 0.546875]  [A loss: 1.171583, acc: 0.035156]\n",
      "1173: [D loss: 0.657828, acc: 0.634766]  [A loss: 0.712340, acc: 0.488281]\n",
      "1174: [D loss: 0.698604, acc: 0.539062]  [A loss: 1.110637, acc: 0.042969]\n",
      "1175: [D loss: 0.665973, acc: 0.599609]  [A loss: 0.750584, acc: 0.441406]\n",
      "1176: [D loss: 0.690712, acc: 0.580078]  [A loss: 1.013583, acc: 0.089844]\n",
      "1177: [D loss: 0.666699, acc: 0.589844]  [A loss: 0.855635, acc: 0.214844]\n",
      "1178: [D loss: 0.669702, acc: 0.605469]  [A loss: 1.036489, acc: 0.078125]\n",
      "1179: [D loss: 0.681720, acc: 0.560547]  [A loss: 0.848043, acc: 0.292969]\n",
      "1180: [D loss: 0.675041, acc: 0.570312]  [A loss: 1.025043, acc: 0.066406]\n",
      "1181: [D loss: 0.648856, acc: 0.642578]  [A loss: 0.873975, acc: 0.253906]\n",
      "1182: [D loss: 0.687730, acc: 0.548828]  [A loss: 1.091155, acc: 0.054688]\n",
      "1183: [D loss: 0.663586, acc: 0.576172]  [A loss: 0.775794, acc: 0.359375]\n",
      "1184: [D loss: 0.697024, acc: 0.544922]  [A loss: 1.220877, acc: 0.019531]\n",
      "1185: [D loss: 0.669877, acc: 0.597656]  [A loss: 0.669876, acc: 0.558594]\n",
      "1186: [D loss: 0.805701, acc: 0.509766]  [A loss: 1.139204, acc: 0.082031]\n",
      "1187: [D loss: 0.696989, acc: 0.560547]  [A loss: 0.923905, acc: 0.199219]\n",
      "1188: [D loss: 0.730483, acc: 0.500000]  [A loss: 1.138826, acc: 0.046875]\n",
      "1189: [D loss: 0.662558, acc: 0.580078]  [A loss: 0.808881, acc: 0.328125]\n",
      "1190: [D loss: 0.704885, acc: 0.542969]  [A loss: 1.135601, acc: 0.054688]\n",
      "1191: [D loss: 0.674585, acc: 0.570312]  [A loss: 0.891689, acc: 0.230469]\n",
      "1192: [D loss: 0.687570, acc: 0.570312]  [A loss: 1.019129, acc: 0.121094]\n",
      "1193: [D loss: 0.684541, acc: 0.564453]  [A loss: 0.920954, acc: 0.183594]\n",
      "1194: [D loss: 0.690458, acc: 0.572266]  [A loss: 0.966924, acc: 0.148438]\n",
      "1195: [D loss: 0.675917, acc: 0.554688]  [A loss: 0.968702, acc: 0.171875]\n",
      "1196: [D loss: 0.685150, acc: 0.570312]  [A loss: 0.947537, acc: 0.152344]\n",
      "1197: [D loss: 0.664460, acc: 0.603516]  [A loss: 1.035268, acc: 0.113281]\n",
      "1198: [D loss: 0.681665, acc: 0.564453]  [A loss: 1.026304, acc: 0.109375]\n",
      "1199: [D loss: 0.669906, acc: 0.597656]  [A loss: 0.869904, acc: 0.230469]\n",
      "1200: [D loss: 0.703544, acc: 0.552734]  [A loss: 1.037765, acc: 0.097656]\n",
      "1201: [D loss: 0.663051, acc: 0.609375]  [A loss: 0.870036, acc: 0.265625]\n",
      "1202: [D loss: 0.671049, acc: 0.609375]  [A loss: 1.218922, acc: 0.015625]\n",
      "1203: [D loss: 0.662994, acc: 0.576172]  [A loss: 0.708822, acc: 0.542969]\n",
      "1204: [D loss: 0.723474, acc: 0.529297]  [A loss: 1.297675, acc: 0.023438]\n",
      "1205: [D loss: 0.678650, acc: 0.572266]  [A loss: 0.597231, acc: 0.707031]\n",
      "1206: [D loss: 0.733185, acc: 0.509766]  [A loss: 1.273579, acc: 0.015625]\n",
      "1207: [D loss: 0.687650, acc: 0.585938]  [A loss: 0.652087, acc: 0.621094]\n",
      "1208: [D loss: 0.729465, acc: 0.523438]  [A loss: 1.029115, acc: 0.097656]\n",
      "1209: [D loss: 0.692764, acc: 0.554688]  [A loss: 0.808966, acc: 0.312500]\n",
      "1210: [D loss: 0.670778, acc: 0.580078]  [A loss: 0.993534, acc: 0.125000]\n",
      "1211: [D loss: 0.677286, acc: 0.568359]  [A loss: 0.888887, acc: 0.234375]\n",
      "1212: [D loss: 0.708426, acc: 0.556641]  [A loss: 0.989929, acc: 0.171875]\n",
      "1213: [D loss: 0.694634, acc: 0.542969]  [A loss: 0.810265, acc: 0.355469]\n",
      "1214: [D loss: 0.689443, acc: 0.562500]  [A loss: 1.065423, acc: 0.093750]\n",
      "1215: [D loss: 0.682253, acc: 0.587891]  [A loss: 0.824510, acc: 0.304688]\n",
      "1216: [D loss: 0.690211, acc: 0.535156]  [A loss: 1.012420, acc: 0.093750]\n",
      "1217: [D loss: 0.685199, acc: 0.523438]  [A loss: 0.894689, acc: 0.203125]\n",
      "1218: [D loss: 0.677765, acc: 0.570312]  [A loss: 0.978831, acc: 0.164062]\n",
      "1219: [D loss: 0.673358, acc: 0.585938]  [A loss: 0.952963, acc: 0.136719]\n",
      "1220: [D loss: 0.674742, acc: 0.585938]  [A loss: 0.980217, acc: 0.128906]\n",
      "1221: [D loss: 0.674748, acc: 0.570312]  [A loss: 0.920115, acc: 0.160156]\n",
      "1222: [D loss: 0.684182, acc: 0.593750]  [A loss: 1.036767, acc: 0.097656]\n",
      "1223: [D loss: 0.660931, acc: 0.603516]  [A loss: 0.841300, acc: 0.292969]\n",
      "1224: [D loss: 0.700083, acc: 0.552734]  [A loss: 1.211823, acc: 0.035156]\n",
      "1225: [D loss: 0.681970, acc: 0.570312]  [A loss: 0.651893, acc: 0.621094]\n",
      "1226: [D loss: 0.734216, acc: 0.509766]  [A loss: 1.301311, acc: 0.015625]\n",
      "1227: [D loss: 0.688331, acc: 0.560547]  [A loss: 0.681459, acc: 0.546875]\n",
      "1228: [D loss: 0.747608, acc: 0.521484]  [A loss: 1.118524, acc: 0.078125]\n",
      "1229: [D loss: 0.704340, acc: 0.566406]  [A loss: 0.932530, acc: 0.207031]\n",
      "1230: [D loss: 0.691588, acc: 0.539062]  [A loss: 1.049907, acc: 0.062500]\n",
      "1231: [D loss: 0.660418, acc: 0.605469]  [A loss: 0.899708, acc: 0.210938]\n",
      "1232: [D loss: 0.687595, acc: 0.562500]  [A loss: 1.114336, acc: 0.046875]\n",
      "1233: [D loss: 0.677028, acc: 0.597656]  [A loss: 0.844412, acc: 0.289062]\n",
      "1234: [D loss: 0.705611, acc: 0.562500]  [A loss: 1.102259, acc: 0.105469]\n",
      "1235: [D loss: 0.650938, acc: 0.628906]  [A loss: 0.834991, acc: 0.328125]\n",
      "1236: [D loss: 0.684929, acc: 0.593750]  [A loss: 1.095186, acc: 0.074219]\n",
      "1237: [D loss: 0.654205, acc: 0.634766]  [A loss: 0.790937, acc: 0.386719]\n",
      "1238: [D loss: 0.708044, acc: 0.548828]  [A loss: 1.213668, acc: 0.023438]\n",
      "1239: [D loss: 0.678533, acc: 0.585938]  [A loss: 0.693876, acc: 0.527344]\n",
      "1240: [D loss: 0.726540, acc: 0.541016]  [A loss: 1.176497, acc: 0.039062]\n",
      "1241: [D loss: 0.688417, acc: 0.554688]  [A loss: 0.701234, acc: 0.511719]\n",
      "1242: [D loss: 0.724525, acc: 0.515625]  [A loss: 1.148746, acc: 0.050781]\n",
      "1243: [D loss: 0.675439, acc: 0.578125]  [A loss: 0.759351, acc: 0.406250]\n",
      "1244: [D loss: 0.699806, acc: 0.546875]  [A loss: 1.034180, acc: 0.085938]\n",
      "1245: [D loss: 0.678922, acc: 0.576172]  [A loss: 0.858086, acc: 0.257812]\n",
      "1246: [D loss: 0.673504, acc: 0.570312]  [A loss: 0.979762, acc: 0.140625]\n",
      "1247: [D loss: 0.661817, acc: 0.595703]  [A loss: 0.871771, acc: 0.238281]\n",
      "1248: [D loss: 0.689731, acc: 0.562500]  [A loss: 1.013103, acc: 0.128906]\n",
      "1249: [D loss: 0.688498, acc: 0.548828]  [A loss: 0.983042, acc: 0.121094]\n",
      "1250: [D loss: 0.665340, acc: 0.587891]  [A loss: 0.924796, acc: 0.203125]\n",
      "1251: [D loss: 0.663950, acc: 0.582031]  [A loss: 1.026427, acc: 0.117188]\n",
      "1252: [D loss: 0.678581, acc: 0.570312]  [A loss: 0.896485, acc: 0.230469]\n",
      "1253: [D loss: 0.685907, acc: 0.556641]  [A loss: 1.071049, acc: 0.093750]\n",
      "1254: [D loss: 0.682046, acc: 0.548828]  [A loss: 0.808194, acc: 0.343750]\n",
      "1255: [D loss: 0.699150, acc: 0.558594]  [A loss: 1.255787, acc: 0.031250]\n",
      "1256: [D loss: 0.679168, acc: 0.568359]  [A loss: 0.648884, acc: 0.613281]\n",
      "1257: [D loss: 0.747622, acc: 0.527344]  [A loss: 1.355362, acc: 0.011719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258: [D loss: 0.674025, acc: 0.589844]  [A loss: 0.619479, acc: 0.652344]\n",
      "1259: [D loss: 0.730928, acc: 0.548828]  [A loss: 1.052036, acc: 0.132812]\n",
      "1260: [D loss: 0.703927, acc: 0.531250]  [A loss: 0.841682, acc: 0.281250]\n",
      "1261: [D loss: 0.705265, acc: 0.527344]  [A loss: 1.095982, acc: 0.042969]\n",
      "1262: [D loss: 0.687081, acc: 0.548828]  [A loss: 0.852933, acc: 0.238281]\n",
      "1263: [D loss: 0.701168, acc: 0.550781]  [A loss: 1.008919, acc: 0.121094]\n",
      "1264: [D loss: 0.676764, acc: 0.583984]  [A loss: 0.880149, acc: 0.187500]\n",
      "1265: [D loss: 0.690227, acc: 0.558594]  [A loss: 0.951726, acc: 0.144531]\n",
      "1266: [D loss: 0.674156, acc: 0.605469]  [A loss: 0.936300, acc: 0.164062]\n",
      "1267: [D loss: 0.650478, acc: 0.615234]  [A loss: 0.914021, acc: 0.171875]\n",
      "1268: [D loss: 0.686245, acc: 0.564453]  [A loss: 0.952067, acc: 0.140625]\n",
      "1269: [D loss: 0.686998, acc: 0.562500]  [A loss: 1.029845, acc: 0.117188]\n",
      "1270: [D loss: 0.689643, acc: 0.529297]  [A loss: 0.863986, acc: 0.253906]\n",
      "1271: [D loss: 0.712917, acc: 0.539062]  [A loss: 1.016705, acc: 0.105469]\n",
      "1272: [D loss: 0.692972, acc: 0.546875]  [A loss: 0.941734, acc: 0.171875]\n",
      "1273: [D loss: 0.659681, acc: 0.603516]  [A loss: 1.029522, acc: 0.132812]\n",
      "1274: [D loss: 0.700872, acc: 0.539062]  [A loss: 0.927931, acc: 0.156250]\n",
      "1275: [D loss: 0.675671, acc: 0.617188]  [A loss: 0.910815, acc: 0.195312]\n",
      "1276: [D loss: 0.693606, acc: 0.570312]  [A loss: 0.980169, acc: 0.136719]\n",
      "1277: [D loss: 0.669074, acc: 0.580078]  [A loss: 0.978193, acc: 0.152344]\n",
      "1278: [D loss: 0.689929, acc: 0.541016]  [A loss: 1.134043, acc: 0.066406]\n",
      "1279: [D loss: 0.674321, acc: 0.576172]  [A loss: 0.687488, acc: 0.550781]\n",
      "1280: [D loss: 0.721571, acc: 0.541016]  [A loss: 1.347764, acc: 0.027344]\n",
      "1281: [D loss: 0.683074, acc: 0.556641]  [A loss: 0.570089, acc: 0.757812]\n",
      "1282: [D loss: 0.771981, acc: 0.519531]  [A loss: 1.197607, acc: 0.054688]\n",
      "1283: [D loss: 0.717848, acc: 0.519531]  [A loss: 0.822496, acc: 0.289062]\n",
      "1284: [D loss: 0.714987, acc: 0.523438]  [A loss: 1.080767, acc: 0.062500]\n",
      "1285: [D loss: 0.664475, acc: 0.625000]  [A loss: 0.777279, acc: 0.363281]\n",
      "1286: [D loss: 0.722509, acc: 0.517578]  [A loss: 1.107657, acc: 0.054688]\n",
      "1287: [D loss: 0.661847, acc: 0.583984]  [A loss: 0.723818, acc: 0.492188]\n",
      "1288: [D loss: 0.715368, acc: 0.546875]  [A loss: 1.125931, acc: 0.046875]\n",
      "1289: [D loss: 0.682616, acc: 0.570312]  [A loss: 0.708391, acc: 0.515625]\n",
      "1290: [D loss: 0.738234, acc: 0.515625]  [A loss: 1.140455, acc: 0.050781]\n",
      "1291: [D loss: 0.676175, acc: 0.542969]  [A loss: 0.749855, acc: 0.402344]\n",
      "1292: [D loss: 0.690496, acc: 0.550781]  [A loss: 0.956249, acc: 0.128906]\n",
      "1293: [D loss: 0.677095, acc: 0.593750]  [A loss: 0.868731, acc: 0.250000]\n",
      "1294: [D loss: 0.666885, acc: 0.601562]  [A loss: 0.985847, acc: 0.128906]\n",
      "1295: [D loss: 0.682214, acc: 0.580078]  [A loss: 0.905825, acc: 0.183594]\n",
      "1296: [D loss: 0.688158, acc: 0.556641]  [A loss: 1.031811, acc: 0.085938]\n",
      "1297: [D loss: 0.668787, acc: 0.595703]  [A loss: 0.949058, acc: 0.160156]\n",
      "1298: [D loss: 0.690383, acc: 0.546875]  [A loss: 1.061237, acc: 0.136719]\n",
      "1299: [D loss: 0.658841, acc: 0.597656]  [A loss: 0.792689, acc: 0.363281]\n",
      "1300: [D loss: 0.710095, acc: 0.541016]  [A loss: 1.198706, acc: 0.027344]\n",
      "1301: [D loss: 0.679139, acc: 0.578125]  [A loss: 0.702764, acc: 0.523438]\n",
      "1302: [D loss: 0.715483, acc: 0.533203]  [A loss: 1.108005, acc: 0.046875]\n",
      "1303: [D loss: 0.676756, acc: 0.587891]  [A loss: 0.759772, acc: 0.414062]\n",
      "1304: [D loss: 0.697953, acc: 0.552734]  [A loss: 1.181541, acc: 0.023438]\n",
      "1305: [D loss: 0.670936, acc: 0.589844]  [A loss: 0.714206, acc: 0.511719]\n",
      "1306: [D loss: 0.713344, acc: 0.527344]  [A loss: 1.189958, acc: 0.019531]\n",
      "1307: [D loss: 0.677686, acc: 0.570312]  [A loss: 0.751213, acc: 0.429688]\n",
      "1308: [D loss: 0.686198, acc: 0.550781]  [A loss: 1.072710, acc: 0.062500]\n",
      "1309: [D loss: 0.672813, acc: 0.580078]  [A loss: 0.760196, acc: 0.417969]\n",
      "1310: [D loss: 0.710872, acc: 0.529297]  [A loss: 1.112919, acc: 0.046875]\n",
      "1311: [D loss: 0.669378, acc: 0.587891]  [A loss: 0.744348, acc: 0.472656]\n",
      "1312: [D loss: 0.713890, acc: 0.541016]  [A loss: 1.068802, acc: 0.050781]\n",
      "1313: [D loss: 0.675144, acc: 0.570312]  [A loss: 0.766426, acc: 0.359375]\n",
      "1314: [D loss: 0.677029, acc: 0.564453]  [A loss: 1.016864, acc: 0.093750]\n",
      "1315: [D loss: 0.668331, acc: 0.583984]  [A loss: 0.856566, acc: 0.269531]\n",
      "1316: [D loss: 0.668007, acc: 0.582031]  [A loss: 0.985663, acc: 0.121094]\n",
      "1317: [D loss: 0.669252, acc: 0.591797]  [A loss: 0.898076, acc: 0.187500]\n",
      "1318: [D loss: 0.679249, acc: 0.564453]  [A loss: 0.970284, acc: 0.132812]\n",
      "1319: [D loss: 0.669552, acc: 0.611328]  [A loss: 0.976658, acc: 0.117188]\n",
      "1320: [D loss: 0.709565, acc: 0.554688]  [A loss: 0.880023, acc: 0.210938]\n",
      "1321: [D loss: 0.680098, acc: 0.582031]  [A loss: 0.945091, acc: 0.160156]\n",
      "1322: [D loss: 0.677287, acc: 0.578125]  [A loss: 1.039317, acc: 0.082031]\n",
      "1323: [D loss: 0.691774, acc: 0.564453]  [A loss: 0.952236, acc: 0.148438]\n",
      "1324: [D loss: 0.673423, acc: 0.583984]  [A loss: 1.010367, acc: 0.089844]\n",
      "1325: [D loss: 0.676533, acc: 0.568359]  [A loss: 0.888988, acc: 0.222656]\n",
      "1326: [D loss: 0.694397, acc: 0.558594]  [A loss: 1.014371, acc: 0.132812]\n",
      "1327: [D loss: 0.673690, acc: 0.580078]  [A loss: 0.957140, acc: 0.125000]\n",
      "1328: [D loss: 0.683101, acc: 0.580078]  [A loss: 0.915837, acc: 0.175781]\n",
      "1329: [D loss: 0.700895, acc: 0.554688]  [A loss: 1.151150, acc: 0.070312]\n",
      "1330: [D loss: 0.697118, acc: 0.527344]  [A loss: 0.658896, acc: 0.593750]\n",
      "1331: [D loss: 0.747948, acc: 0.525391]  [A loss: 1.413333, acc: 0.007812]\n",
      "1332: [D loss: 0.690122, acc: 0.558594]  [A loss: 0.556519, acc: 0.781250]\n",
      "1333: [D loss: 0.781707, acc: 0.509766]  [A loss: 1.137853, acc: 0.082031]\n",
      "1334: [D loss: 0.693392, acc: 0.544922]  [A loss: 0.699630, acc: 0.519531]\n",
      "1335: [D loss: 0.719848, acc: 0.529297]  [A loss: 1.031944, acc: 0.085938]\n",
      "1336: [D loss: 0.673458, acc: 0.562500]  [A loss: 0.826890, acc: 0.285156]\n",
      "1337: [D loss: 0.692866, acc: 0.521484]  [A loss: 0.996437, acc: 0.113281]\n",
      "1338: [D loss: 0.664825, acc: 0.591797]  [A loss: 0.797160, acc: 0.351562]\n",
      "1339: [D loss: 0.682959, acc: 0.580078]  [A loss: 0.980664, acc: 0.148438]\n",
      "1340: [D loss: 0.663375, acc: 0.615234]  [A loss: 0.810932, acc: 0.343750]\n",
      "1341: [D loss: 0.681817, acc: 0.562500]  [A loss: 0.963163, acc: 0.156250]\n",
      "1342: [D loss: 0.673626, acc: 0.585938]  [A loss: 0.871585, acc: 0.238281]\n",
      "1343: [D loss: 0.699676, acc: 0.552734]  [A loss: 1.013480, acc: 0.121094]\n",
      "1344: [D loss: 0.688218, acc: 0.566406]  [A loss: 0.883873, acc: 0.238281]\n",
      "1345: [D loss: 0.696714, acc: 0.554688]  [A loss: 1.031888, acc: 0.101562]\n",
      "1346: [D loss: 0.683217, acc: 0.541016]  [A loss: 0.868540, acc: 0.246094]\n",
      "1347: [D loss: 0.688878, acc: 0.546875]  [A loss: 1.020127, acc: 0.101562]\n",
      "1348: [D loss: 0.679869, acc: 0.562500]  [A loss: 0.854485, acc: 0.246094]\n",
      "1349: [D loss: 0.673139, acc: 0.580078]  [A loss: 1.110448, acc: 0.050781]\n",
      "1350: [D loss: 0.675466, acc: 0.578125]  [A loss: 0.796007, acc: 0.343750]\n",
      "1351: [D loss: 0.689238, acc: 0.548828]  [A loss: 1.091652, acc: 0.066406]\n",
      "1352: [D loss: 0.674849, acc: 0.583984]  [A loss: 0.784994, acc: 0.386719]\n",
      "1353: [D loss: 0.718245, acc: 0.525391]  [A loss: 1.251956, acc: 0.015625]\n",
      "1354: [D loss: 0.693352, acc: 0.537109]  [A loss: 0.626189, acc: 0.679688]\n",
      "1355: [D loss: 0.718938, acc: 0.531250]  [A loss: 1.190590, acc: 0.019531]\n",
      "1356: [D loss: 0.675390, acc: 0.576172]  [A loss: 0.720984, acc: 0.484375]\n",
      "1357: [D loss: 0.716768, acc: 0.515625]  [A loss: 1.118608, acc: 0.066406]\n",
      "1358: [D loss: 0.674715, acc: 0.556641]  [A loss: 0.743194, acc: 0.429688]\n",
      "1359: [D loss: 0.698682, acc: 0.529297]  [A loss: 1.052529, acc: 0.074219]\n",
      "1360: [D loss: 0.673672, acc: 0.568359]  [A loss: 0.789803, acc: 0.394531]\n",
      "1361: [D loss: 0.694167, acc: 0.552734]  [A loss: 1.004726, acc: 0.117188]\n",
      "1362: [D loss: 0.671664, acc: 0.560547]  [A loss: 0.864700, acc: 0.230469]\n",
      "1363: [D loss: 0.689010, acc: 0.550781]  [A loss: 1.013247, acc: 0.101562]\n",
      "1364: [D loss: 0.670307, acc: 0.572266]  [A loss: 0.998285, acc: 0.109375]\n",
      "1365: [D loss: 0.690271, acc: 0.556641]  [A loss: 0.984728, acc: 0.136719]\n",
      "1366: [D loss: 0.679542, acc: 0.558594]  [A loss: 0.869612, acc: 0.238281]\n",
      "1367: [D loss: 0.691883, acc: 0.562500]  [A loss: 1.088856, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368: [D loss: 0.668824, acc: 0.605469]  [A loss: 0.767911, acc: 0.402344]\n",
      "1369: [D loss: 0.712144, acc: 0.546875]  [A loss: 1.204985, acc: 0.027344]\n",
      "1370: [D loss: 0.690940, acc: 0.556641]  [A loss: 0.704847, acc: 0.507812]\n",
      "1371: [D loss: 0.689593, acc: 0.542969]  [A loss: 1.132075, acc: 0.050781]\n",
      "1372: [D loss: 0.679819, acc: 0.539062]  [A loss: 0.694588, acc: 0.535156]\n",
      "1373: [D loss: 0.731728, acc: 0.511719]  [A loss: 1.165631, acc: 0.035156]\n",
      "1374: [D loss: 0.687840, acc: 0.537109]  [A loss: 0.689533, acc: 0.562500]\n",
      "1375: [D loss: 0.709202, acc: 0.542969]  [A loss: 1.136617, acc: 0.074219]\n",
      "1376: [D loss: 0.657347, acc: 0.607422]  [A loss: 0.728181, acc: 0.449219]\n",
      "1377: [D loss: 0.728924, acc: 0.544922]  [A loss: 1.006736, acc: 0.164062]\n",
      "1378: [D loss: 0.697909, acc: 0.548828]  [A loss: 0.985296, acc: 0.121094]\n",
      "1379: [D loss: 0.658127, acc: 0.597656]  [A loss: 0.893134, acc: 0.207031]\n",
      "1380: [D loss: 0.703266, acc: 0.525391]  [A loss: 0.985348, acc: 0.128906]\n",
      "1381: [D loss: 0.678150, acc: 0.576172]  [A loss: 0.762293, acc: 0.394531]\n",
      "1382: [D loss: 0.696352, acc: 0.548828]  [A loss: 1.131302, acc: 0.039062]\n",
      "1383: [D loss: 0.668187, acc: 0.599609]  [A loss: 0.739866, acc: 0.457031]\n",
      "1384: [D loss: 0.716326, acc: 0.531250]  [A loss: 1.134630, acc: 0.050781]\n",
      "1385: [D loss: 0.681011, acc: 0.548828]  [A loss: 0.810022, acc: 0.316406]\n",
      "1386: [D loss: 0.687272, acc: 0.566406]  [A loss: 1.076291, acc: 0.105469]\n",
      "1387: [D loss: 0.678409, acc: 0.601562]  [A loss: 0.776761, acc: 0.347656]\n",
      "1388: [D loss: 0.705948, acc: 0.554688]  [A loss: 1.117471, acc: 0.046875]\n",
      "1389: [D loss: 0.680919, acc: 0.582031]  [A loss: 0.700603, acc: 0.503906]\n",
      "1390: [D loss: 0.730572, acc: 0.529297]  [A loss: 1.173372, acc: 0.039062]\n",
      "1391: [D loss: 0.665083, acc: 0.603516]  [A loss: 0.699004, acc: 0.550781]\n",
      "1392: [D loss: 0.717932, acc: 0.531250]  [A loss: 1.084084, acc: 0.089844]\n",
      "1393: [D loss: 0.676002, acc: 0.562500]  [A loss: 0.807976, acc: 0.328125]\n",
      "1394: [D loss: 0.712184, acc: 0.546875]  [A loss: 1.055113, acc: 0.117188]\n",
      "1395: [D loss: 0.687978, acc: 0.539062]  [A loss: 0.801972, acc: 0.351562]\n",
      "1396: [D loss: 0.704655, acc: 0.531250]  [A loss: 1.084195, acc: 0.070312]\n",
      "1397: [D loss: 0.675700, acc: 0.566406]  [A loss: 0.815108, acc: 0.320312]\n",
      "1398: [D loss: 0.688011, acc: 0.560547]  [A loss: 1.097296, acc: 0.062500]\n",
      "1399: [D loss: 0.668134, acc: 0.591797]  [A loss: 0.760347, acc: 0.398438]\n",
      "1400: [D loss: 0.701369, acc: 0.558594]  [A loss: 1.114807, acc: 0.070312]\n",
      "1401: [D loss: 0.684937, acc: 0.564453]  [A loss: 0.838923, acc: 0.300781]\n",
      "1402: [D loss: 0.687176, acc: 0.564453]  [A loss: 0.938875, acc: 0.203125]\n",
      "1403: [D loss: 0.681231, acc: 0.587891]  [A loss: 0.992797, acc: 0.113281]\n",
      "1404: [D loss: 0.665826, acc: 0.603516]  [A loss: 0.950436, acc: 0.171875]\n",
      "1405: [D loss: 0.662246, acc: 0.583984]  [A loss: 1.036819, acc: 0.136719]\n",
      "1406: [D loss: 0.688900, acc: 0.576172]  [A loss: 0.898394, acc: 0.234375]\n",
      "1407: [D loss: 0.693635, acc: 0.550781]  [A loss: 0.992913, acc: 0.144531]\n",
      "1408: [D loss: 0.674330, acc: 0.597656]  [A loss: 0.896991, acc: 0.203125]\n",
      "1409: [D loss: 0.690563, acc: 0.585938]  [A loss: 1.126023, acc: 0.078125]\n",
      "1410: [D loss: 0.678453, acc: 0.566406]  [A loss: 0.747214, acc: 0.425781]\n",
      "1411: [D loss: 0.716781, acc: 0.519531]  [A loss: 1.237923, acc: 0.039062]\n",
      "1412: [D loss: 0.683912, acc: 0.564453]  [A loss: 0.862769, acc: 0.285156]\n",
      "1413: [D loss: 0.707988, acc: 0.531250]  [A loss: 1.129924, acc: 0.074219]\n",
      "1414: [D loss: 0.670808, acc: 0.591797]  [A loss: 0.736312, acc: 0.445312]\n",
      "1415: [D loss: 0.719980, acc: 0.542969]  [A loss: 1.197621, acc: 0.031250]\n",
      "1416: [D loss: 0.680443, acc: 0.582031]  [A loss: 0.653485, acc: 0.636719]\n",
      "1417: [D loss: 0.742620, acc: 0.515625]  [A loss: 1.227795, acc: 0.046875]\n",
      "1418: [D loss: 0.672919, acc: 0.578125]  [A loss: 0.719620, acc: 0.503906]\n",
      "1419: [D loss: 0.717422, acc: 0.531250]  [A loss: 1.094574, acc: 0.082031]\n",
      "1420: [D loss: 0.682199, acc: 0.560547]  [A loss: 0.759956, acc: 0.425781]\n",
      "1421: [D loss: 0.699294, acc: 0.548828]  [A loss: 1.032071, acc: 0.093750]\n",
      "1422: [D loss: 0.677586, acc: 0.578125]  [A loss: 0.830636, acc: 0.261719]\n",
      "1423: [D loss: 0.699059, acc: 0.542969]  [A loss: 0.978713, acc: 0.167969]\n",
      "1424: [D loss: 0.693657, acc: 0.537109]  [A loss: 0.955368, acc: 0.171875]\n",
      "1425: [D loss: 0.684714, acc: 0.582031]  [A loss: 1.059827, acc: 0.074219]\n",
      "1426: [D loss: 0.682462, acc: 0.576172]  [A loss: 0.883060, acc: 0.222656]\n",
      "1427: [D loss: 0.678630, acc: 0.582031]  [A loss: 1.097655, acc: 0.062500]\n",
      "1428: [D loss: 0.675616, acc: 0.556641]  [A loss: 0.820873, acc: 0.328125]\n",
      "1429: [D loss: 0.707470, acc: 0.541016]  [A loss: 1.111907, acc: 0.062500]\n",
      "1430: [D loss: 0.664720, acc: 0.607422]  [A loss: 0.712239, acc: 0.519531]\n",
      "1431: [D loss: 0.711158, acc: 0.544922]  [A loss: 1.187244, acc: 0.019531]\n",
      "1432: [D loss: 0.687596, acc: 0.585938]  [A loss: 0.679130, acc: 0.570312]\n",
      "1433: [D loss: 0.730746, acc: 0.517578]  [A loss: 1.247966, acc: 0.054688]\n",
      "1434: [D loss: 0.691990, acc: 0.560547]  [A loss: 0.649305, acc: 0.613281]\n",
      "1435: [D loss: 0.737738, acc: 0.515625]  [A loss: 1.165013, acc: 0.039062]\n",
      "1436: [D loss: 0.689045, acc: 0.544922]  [A loss: 0.702018, acc: 0.507812]\n",
      "1437: [D loss: 0.701001, acc: 0.541016]  [A loss: 0.975940, acc: 0.183594]\n",
      "1438: [D loss: 0.688836, acc: 0.554688]  [A loss: 0.909268, acc: 0.183594]\n",
      "1439: [D loss: 0.686804, acc: 0.558594]  [A loss: 0.919382, acc: 0.183594]\n",
      "1440: [D loss: 0.678393, acc: 0.570312]  [A loss: 0.947388, acc: 0.132812]\n",
      "1441: [D loss: 0.679069, acc: 0.554688]  [A loss: 0.908970, acc: 0.199219]\n",
      "1442: [D loss: 0.679908, acc: 0.564453]  [A loss: 0.949811, acc: 0.179688]\n",
      "1443: [D loss: 0.681193, acc: 0.570312]  [A loss: 0.921913, acc: 0.207031]\n",
      "1444: [D loss: 0.703527, acc: 0.539062]  [A loss: 0.909647, acc: 0.195312]\n",
      "1445: [D loss: 0.693022, acc: 0.527344]  [A loss: 0.960271, acc: 0.148438]\n",
      "1446: [D loss: 0.682140, acc: 0.568359]  [A loss: 0.939898, acc: 0.160156]\n",
      "1447: [D loss: 0.676867, acc: 0.572266]  [A loss: 0.970660, acc: 0.136719]\n",
      "1448: [D loss: 0.665712, acc: 0.605469]  [A loss: 0.950236, acc: 0.148438]\n",
      "1449: [D loss: 0.674168, acc: 0.605469]  [A loss: 0.960121, acc: 0.140625]\n",
      "1450: [D loss: 0.703446, acc: 0.523438]  [A loss: 1.019412, acc: 0.136719]\n",
      "1451: [D loss: 0.690513, acc: 0.570312]  [A loss: 0.992382, acc: 0.128906]\n",
      "1452: [D loss: 0.680632, acc: 0.556641]  [A loss: 0.889464, acc: 0.250000]\n",
      "1453: [D loss: 0.685710, acc: 0.578125]  [A loss: 1.126487, acc: 0.074219]\n",
      "1454: [D loss: 0.697813, acc: 0.544922]  [A loss: 0.942121, acc: 0.128906]\n",
      "1455: [D loss: 0.685961, acc: 0.560547]  [A loss: 0.990646, acc: 0.132812]\n",
      "1456: [D loss: 0.694263, acc: 0.542969]  [A loss: 1.159493, acc: 0.035156]\n",
      "1457: [D loss: 0.674948, acc: 0.578125]  [A loss: 0.678509, acc: 0.570312]\n",
      "1458: [D loss: 0.722286, acc: 0.537109]  [A loss: 1.471184, acc: 0.000000]\n",
      "1459: [D loss: 0.694805, acc: 0.546875]  [A loss: 0.534987, acc: 0.757812]\n",
      "1460: [D loss: 0.859141, acc: 0.515625]  [A loss: 1.143786, acc: 0.054688]\n",
      "1461: [D loss: 0.670008, acc: 0.585938]  [A loss: 0.767330, acc: 0.414062]\n",
      "1462: [D loss: 0.695881, acc: 0.535156]  [A loss: 1.027907, acc: 0.093750]\n",
      "1463: [D loss: 0.676739, acc: 0.572266]  [A loss: 0.810707, acc: 0.300781]\n",
      "1464: [D loss: 0.698893, acc: 0.535156]  [A loss: 0.998807, acc: 0.093750]\n",
      "1465: [D loss: 0.678238, acc: 0.589844]  [A loss: 0.834693, acc: 0.324219]\n",
      "1466: [D loss: 0.681757, acc: 0.572266]  [A loss: 0.895594, acc: 0.257812]\n",
      "1467: [D loss: 0.673714, acc: 0.576172]  [A loss: 0.871346, acc: 0.253906]\n",
      "1468: [D loss: 0.684894, acc: 0.541016]  [A loss: 0.981205, acc: 0.132812]\n",
      "1469: [D loss: 0.680442, acc: 0.589844]  [A loss: 0.880971, acc: 0.230469]\n",
      "1470: [D loss: 0.684567, acc: 0.554688]  [A loss: 0.991157, acc: 0.109375]\n",
      "1471: [D loss: 0.667779, acc: 0.593750]  [A loss: 0.893384, acc: 0.222656]\n",
      "1472: [D loss: 0.702138, acc: 0.535156]  [A loss: 1.073482, acc: 0.082031]\n",
      "1473: [D loss: 0.670755, acc: 0.589844]  [A loss: 0.804065, acc: 0.347656]\n",
      "1474: [D loss: 0.723100, acc: 0.558594]  [A loss: 1.144841, acc: 0.062500]\n",
      "1475: [D loss: 0.691501, acc: 0.560547]  [A loss: 0.766689, acc: 0.402344]\n",
      "1476: [D loss: 0.716323, acc: 0.544922]  [A loss: 1.212116, acc: 0.039062]\n",
      "1477: [D loss: 0.697664, acc: 0.554688]  [A loss: 0.678030, acc: 0.578125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1478: [D loss: 0.728968, acc: 0.535156]  [A loss: 1.256597, acc: 0.031250]\n",
      "1479: [D loss: 0.677301, acc: 0.580078]  [A loss: 0.710480, acc: 0.500000]\n",
      "1480: [D loss: 0.722253, acc: 0.513672]  [A loss: 1.025384, acc: 0.128906]\n",
      "1481: [D loss: 0.699899, acc: 0.537109]  [A loss: 0.894873, acc: 0.246094]\n",
      "1482: [D loss: 0.695581, acc: 0.556641]  [A loss: 0.961398, acc: 0.136719]\n",
      "1483: [D loss: 0.678649, acc: 0.568359]  [A loss: 0.875329, acc: 0.222656]\n",
      "1484: [D loss: 0.680943, acc: 0.583984]  [A loss: 0.891959, acc: 0.222656]\n",
      "1485: [D loss: 0.690126, acc: 0.550781]  [A loss: 0.960587, acc: 0.160156]\n",
      "1486: [D loss: 0.691567, acc: 0.578125]  [A loss: 0.971796, acc: 0.132812]\n",
      "1487: [D loss: 0.684363, acc: 0.541016]  [A loss: 0.859356, acc: 0.285156]\n",
      "1488: [D loss: 0.678606, acc: 0.568359]  [A loss: 0.970441, acc: 0.156250]\n",
      "1489: [D loss: 0.677829, acc: 0.578125]  [A loss: 0.925386, acc: 0.191406]\n",
      "1490: [D loss: 0.681537, acc: 0.580078]  [A loss: 0.946078, acc: 0.175781]\n",
      "1491: [D loss: 0.693119, acc: 0.578125]  [A loss: 0.996263, acc: 0.125000]\n",
      "1492: [D loss: 0.686330, acc: 0.541016]  [A loss: 0.972260, acc: 0.191406]\n",
      "1493: [D loss: 0.707571, acc: 0.533203]  [A loss: 0.908544, acc: 0.246094]\n",
      "1494: [D loss: 0.688781, acc: 0.560547]  [A loss: 1.177450, acc: 0.050781]\n",
      "1495: [D loss: 0.693585, acc: 0.541016]  [A loss: 0.796225, acc: 0.320312]\n",
      "1496: [D loss: 0.713840, acc: 0.525391]  [A loss: 1.262932, acc: 0.031250]\n",
      "1497: [D loss: 0.700105, acc: 0.548828]  [A loss: 0.607327, acc: 0.695312]\n",
      "1498: [D loss: 0.720795, acc: 0.537109]  [A loss: 1.288788, acc: 0.015625]\n",
      "1499: [D loss: 0.675663, acc: 0.585938]  [A loss: 0.661049, acc: 0.582031]\n",
      "1500: [D loss: 0.791997, acc: 0.515625]  [A loss: 1.062816, acc: 0.101562]\n",
      "1501: [D loss: 0.677889, acc: 0.544922]  [A loss: 0.850316, acc: 0.265625]\n",
      "1502: [D loss: 0.686378, acc: 0.548828]  [A loss: 1.065722, acc: 0.097656]\n",
      "1503: [D loss: 0.674337, acc: 0.583984]  [A loss: 0.830186, acc: 0.292969]\n",
      "1504: [D loss: 0.683979, acc: 0.570312]  [A loss: 1.030467, acc: 0.109375]\n",
      "1505: [D loss: 0.674118, acc: 0.562500]  [A loss: 0.819044, acc: 0.347656]\n",
      "1506: [D loss: 0.683232, acc: 0.572266]  [A loss: 1.098838, acc: 0.042969]\n",
      "1507: [D loss: 0.680561, acc: 0.566406]  [A loss: 0.801577, acc: 0.371094]\n",
      "1508: [D loss: 0.721840, acc: 0.533203]  [A loss: 1.242849, acc: 0.027344]\n",
      "1509: [D loss: 0.693960, acc: 0.521484]  [A loss: 0.714863, acc: 0.500000]\n",
      "1510: [D loss: 0.734233, acc: 0.513672]  [A loss: 1.134999, acc: 0.042969]\n",
      "1511: [D loss: 0.676882, acc: 0.580078]  [A loss: 0.716088, acc: 0.472656]\n",
      "1512: [D loss: 0.689671, acc: 0.552734]  [A loss: 1.080185, acc: 0.078125]\n",
      "1513: [D loss: 0.682972, acc: 0.554688]  [A loss: 0.808009, acc: 0.351562]\n",
      "1514: [D loss: 0.674395, acc: 0.597656]  [A loss: 0.985778, acc: 0.132812]\n",
      "1515: [D loss: 0.679851, acc: 0.587891]  [A loss: 0.917761, acc: 0.203125]\n",
      "1516: [D loss: 0.674947, acc: 0.601562]  [A loss: 0.987380, acc: 0.117188]\n",
      "1517: [D loss: 0.683622, acc: 0.556641]  [A loss: 0.946696, acc: 0.148438]\n",
      "1518: [D loss: 0.684845, acc: 0.566406]  [A loss: 1.006031, acc: 0.132812]\n",
      "1519: [D loss: 0.694494, acc: 0.544922]  [A loss: 0.855381, acc: 0.246094]\n",
      "1520: [D loss: 0.703701, acc: 0.544922]  [A loss: 1.145236, acc: 0.062500]\n",
      "1521: [D loss: 0.678011, acc: 0.576172]  [A loss: 0.732460, acc: 0.457031]\n",
      "1522: [D loss: 0.729899, acc: 0.525391]  [A loss: 1.218360, acc: 0.035156]\n",
      "1523: [D loss: 0.692807, acc: 0.546875]  [A loss: 0.619610, acc: 0.656250]\n",
      "1524: [D loss: 0.761659, acc: 0.486328]  [A loss: 1.297177, acc: 0.019531]\n",
      "1525: [D loss: 0.671313, acc: 0.566406]  [A loss: 0.668620, acc: 0.578125]\n",
      "1526: [D loss: 0.737407, acc: 0.537109]  [A loss: 1.032339, acc: 0.105469]\n",
      "1527: [D loss: 0.690025, acc: 0.560547]  [A loss: 0.832043, acc: 0.300781]\n",
      "1528: [D loss: 0.682789, acc: 0.578125]  [A loss: 0.984110, acc: 0.140625]\n",
      "1529: [D loss: 0.672303, acc: 0.597656]  [A loss: 0.880122, acc: 0.246094]\n",
      "1530: [D loss: 0.703645, acc: 0.531250]  [A loss: 0.979146, acc: 0.160156]\n",
      "1531: [D loss: 0.676910, acc: 0.578125]  [A loss: 0.917180, acc: 0.214844]\n",
      "1532: [D loss: 0.692960, acc: 0.542969]  [A loss: 0.852666, acc: 0.285156]\n",
      "1533: [D loss: 0.679889, acc: 0.582031]  [A loss: 0.985519, acc: 0.140625]\n",
      "1534: [D loss: 0.680189, acc: 0.562500]  [A loss: 0.825044, acc: 0.281250]\n",
      "1535: [D loss: 0.709402, acc: 0.548828]  [A loss: 1.076417, acc: 0.097656]\n",
      "1536: [D loss: 0.672850, acc: 0.587891]  [A loss: 0.776647, acc: 0.390625]\n",
      "1537: [D loss: 0.708534, acc: 0.544922]  [A loss: 1.188181, acc: 0.027344]\n",
      "1538: [D loss: 0.674388, acc: 0.576172]  [A loss: 0.689456, acc: 0.539062]\n",
      "1539: [D loss: 0.707626, acc: 0.546875]  [A loss: 1.237280, acc: 0.019531]\n",
      "1540: [D loss: 0.688437, acc: 0.548828]  [A loss: 0.657997, acc: 0.593750]\n",
      "1541: [D loss: 0.724368, acc: 0.542969]  [A loss: 1.086950, acc: 0.085938]\n",
      "1542: [D loss: 0.677529, acc: 0.562500]  [A loss: 0.714212, acc: 0.484375]\n",
      "1543: [D loss: 0.721381, acc: 0.542969]  [A loss: 1.066756, acc: 0.109375]\n",
      "1544: [D loss: 0.697798, acc: 0.525391]  [A loss: 0.801228, acc: 0.316406]\n",
      "1545: [D loss: 0.689067, acc: 0.554688]  [A loss: 1.022431, acc: 0.132812]\n",
      "1546: [D loss: 0.682319, acc: 0.566406]  [A loss: 0.818720, acc: 0.308594]\n",
      "1547: [D loss: 0.696112, acc: 0.562500]  [A loss: 0.991447, acc: 0.128906]\n",
      "1548: [D loss: 0.665187, acc: 0.611328]  [A loss: 0.834982, acc: 0.304688]\n",
      "1549: [D loss: 0.696847, acc: 0.564453]  [A loss: 1.051359, acc: 0.109375]\n",
      "1550: [D loss: 0.692743, acc: 0.541016]  [A loss: 0.851726, acc: 0.253906]\n",
      "1551: [D loss: 0.679783, acc: 0.578125]  [A loss: 1.032654, acc: 0.078125]\n",
      "1552: [D loss: 0.683221, acc: 0.566406]  [A loss: 0.875387, acc: 0.234375]\n",
      "1553: [D loss: 0.681741, acc: 0.578125]  [A loss: 1.096168, acc: 0.085938]\n",
      "1554: [D loss: 0.699012, acc: 0.539062]  [A loss: 0.700598, acc: 0.539062]\n",
      "1555: [D loss: 0.698320, acc: 0.546875]  [A loss: 1.213163, acc: 0.007812]\n",
      "1556: [D loss: 0.679449, acc: 0.589844]  [A loss: 0.737357, acc: 0.476562]\n",
      "1557: [D loss: 0.731365, acc: 0.519531]  [A loss: 1.094711, acc: 0.089844]\n",
      "1558: [D loss: 0.667567, acc: 0.597656]  [A loss: 0.731029, acc: 0.460938]\n",
      "1559: [D loss: 0.698686, acc: 0.542969]  [A loss: 1.153429, acc: 0.058594]\n",
      "1560: [D loss: 0.685266, acc: 0.548828]  [A loss: 0.675730, acc: 0.574219]\n",
      "1561: [D loss: 0.724646, acc: 0.509766]  [A loss: 1.019838, acc: 0.085938]\n",
      "1562: [D loss: 0.665020, acc: 0.613281]  [A loss: 0.767459, acc: 0.414062]\n",
      "1563: [D loss: 0.687777, acc: 0.566406]  [A loss: 0.964450, acc: 0.144531]\n",
      "1564: [D loss: 0.699032, acc: 0.564453]  [A loss: 0.806849, acc: 0.343750]\n",
      "1565: [D loss: 0.689785, acc: 0.576172]  [A loss: 0.994067, acc: 0.132812]\n",
      "1566: [D loss: 0.695289, acc: 0.539062]  [A loss: 0.848093, acc: 0.285156]\n",
      "1567: [D loss: 0.697271, acc: 0.550781]  [A loss: 1.064484, acc: 0.054688]\n",
      "1568: [D loss: 0.677491, acc: 0.578125]  [A loss: 0.822247, acc: 0.304688]\n",
      "1569: [D loss: 0.692623, acc: 0.562500]  [A loss: 1.047838, acc: 0.089844]\n",
      "1570: [D loss: 0.684259, acc: 0.564453]  [A loss: 0.730950, acc: 0.480469]\n",
      "1571: [D loss: 0.691848, acc: 0.554688]  [A loss: 1.047317, acc: 0.082031]\n",
      "1572: [D loss: 0.699907, acc: 0.537109]  [A loss: 0.840934, acc: 0.281250]\n",
      "1573: [D loss: 0.697088, acc: 0.527344]  [A loss: 0.963245, acc: 0.160156]\n",
      "1574: [D loss: 0.717839, acc: 0.533203]  [A loss: 0.894715, acc: 0.171875]\n",
      "1575: [D loss: 0.670981, acc: 0.576172]  [A loss: 1.055220, acc: 0.082031]\n",
      "1576: [D loss: 0.667471, acc: 0.583984]  [A loss: 0.817626, acc: 0.304688]\n",
      "1577: [D loss: 0.686044, acc: 0.568359]  [A loss: 1.181201, acc: 0.035156]\n",
      "1578: [D loss: 0.688286, acc: 0.568359]  [A loss: 0.655983, acc: 0.582031]\n",
      "1579: [D loss: 0.731769, acc: 0.523438]  [A loss: 1.135431, acc: 0.050781]\n",
      "1580: [D loss: 0.677506, acc: 0.580078]  [A loss: 0.694232, acc: 0.531250]\n",
      "1581: [D loss: 0.743942, acc: 0.509766]  [A loss: 1.111568, acc: 0.074219]\n",
      "1582: [D loss: 0.691711, acc: 0.550781]  [A loss: 0.742235, acc: 0.421875]\n",
      "1583: [D loss: 0.701055, acc: 0.542969]  [A loss: 1.109758, acc: 0.066406]\n",
      "1584: [D loss: 0.674974, acc: 0.566406]  [A loss: 0.736052, acc: 0.453125]\n",
      "1585: [D loss: 0.702855, acc: 0.541016]  [A loss: 0.973244, acc: 0.082031]\n",
      "1586: [D loss: 0.675566, acc: 0.607422]  [A loss: 0.758901, acc: 0.425781]\n",
      "1587: [D loss: 0.690682, acc: 0.535156]  [A loss: 0.999399, acc: 0.121094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588: [D loss: 0.678312, acc: 0.550781]  [A loss: 0.805157, acc: 0.339844]\n",
      "1589: [D loss: 0.679834, acc: 0.578125]  [A loss: 0.987141, acc: 0.132812]\n",
      "1590: [D loss: 0.686389, acc: 0.564453]  [A loss: 0.851020, acc: 0.265625]\n",
      "1591: [D loss: 0.701476, acc: 0.541016]  [A loss: 1.073134, acc: 0.085938]\n",
      "1592: [D loss: 0.672655, acc: 0.574219]  [A loss: 0.796816, acc: 0.339844]\n",
      "1593: [D loss: 0.671289, acc: 0.601562]  [A loss: 1.082484, acc: 0.070312]\n",
      "1594: [D loss: 0.694084, acc: 0.562500]  [A loss: 0.821986, acc: 0.324219]\n",
      "1595: [D loss: 0.691837, acc: 0.550781]  [A loss: 1.080733, acc: 0.078125]\n",
      "1596: [D loss: 0.672214, acc: 0.585938]  [A loss: 0.813964, acc: 0.285156]\n",
      "1597: [D loss: 0.700491, acc: 0.544922]  [A loss: 1.096971, acc: 0.097656]\n",
      "1598: [D loss: 0.678794, acc: 0.562500]  [A loss: 0.754651, acc: 0.406250]\n",
      "1599: [D loss: 0.689565, acc: 0.568359]  [A loss: 1.062667, acc: 0.097656]\n",
      "1600: [D loss: 0.678937, acc: 0.546875]  [A loss: 0.826522, acc: 0.324219]\n",
      "1601: [D loss: 0.696489, acc: 0.550781]  [A loss: 1.127923, acc: 0.070312]\n",
      "1602: [D loss: 0.684088, acc: 0.544922]  [A loss: 0.839007, acc: 0.285156]\n",
      "1603: [D loss: 0.685809, acc: 0.566406]  [A loss: 0.959148, acc: 0.171875]\n",
      "1604: [D loss: 0.663964, acc: 0.593750]  [A loss: 0.803206, acc: 0.371094]\n",
      "1605: [D loss: 0.682594, acc: 0.562500]  [A loss: 1.109846, acc: 0.054688]\n",
      "1606: [D loss: 0.679215, acc: 0.583984]  [A loss: 0.739823, acc: 0.460938]\n",
      "1607: [D loss: 0.723624, acc: 0.541016]  [A loss: 1.262736, acc: 0.023438]\n",
      "1608: [D loss: 0.683294, acc: 0.574219]  [A loss: 0.749479, acc: 0.519531]\n",
      "1609: [D loss: 0.716471, acc: 0.541016]  [A loss: 1.113140, acc: 0.035156]\n",
      "1610: [D loss: 0.680829, acc: 0.587891]  [A loss: 0.755030, acc: 0.421875]\n",
      "1611: [D loss: 0.716505, acc: 0.519531]  [A loss: 1.021616, acc: 0.125000]\n",
      "1612: [D loss: 0.684401, acc: 0.568359]  [A loss: 0.704598, acc: 0.562500]\n",
      "1613: [D loss: 0.697589, acc: 0.558594]  [A loss: 1.005370, acc: 0.101562]\n",
      "1614: [D loss: 0.667364, acc: 0.583984]  [A loss: 0.745764, acc: 0.406250]\n",
      "1615: [D loss: 0.699603, acc: 0.574219]  [A loss: 1.054740, acc: 0.113281]\n",
      "1616: [D loss: 0.668954, acc: 0.585938]  [A loss: 0.755370, acc: 0.406250]\n",
      "1617: [D loss: 0.680826, acc: 0.576172]  [A loss: 0.980899, acc: 0.128906]\n",
      "1618: [D loss: 0.690741, acc: 0.552734]  [A loss: 0.898735, acc: 0.214844]\n",
      "1619: [D loss: 0.681115, acc: 0.585938]  [A loss: 0.951762, acc: 0.183594]\n",
      "1620: [D loss: 0.687818, acc: 0.582031]  [A loss: 0.872329, acc: 0.246094]\n",
      "1621: [D loss: 0.689482, acc: 0.542969]  [A loss: 1.088905, acc: 0.050781]\n",
      "1622: [D loss: 0.672871, acc: 0.585938]  [A loss: 0.773121, acc: 0.390625]\n",
      "1623: [D loss: 0.681201, acc: 0.576172]  [A loss: 0.997215, acc: 0.140625]\n",
      "1624: [D loss: 0.677670, acc: 0.582031]  [A loss: 0.937921, acc: 0.167969]\n",
      "1625: [D loss: 0.687080, acc: 0.576172]  [A loss: 0.959039, acc: 0.160156]\n",
      "1626: [D loss: 0.687707, acc: 0.576172]  [A loss: 0.984828, acc: 0.148438]\n",
      "1627: [D loss: 0.689092, acc: 0.544922]  [A loss: 1.156089, acc: 0.054688]\n",
      "1628: [D loss: 0.693131, acc: 0.574219]  [A loss: 0.667483, acc: 0.574219]\n",
      "1629: [D loss: 0.718771, acc: 0.539062]  [A loss: 1.233981, acc: 0.027344]\n",
      "1630: [D loss: 0.690139, acc: 0.550781]  [A loss: 0.611676, acc: 0.695312]\n",
      "1631: [D loss: 0.734329, acc: 0.542969]  [A loss: 1.157783, acc: 0.058594]\n",
      "1632: [D loss: 0.675077, acc: 0.572266]  [A loss: 0.661318, acc: 0.582031]\n",
      "1633: [D loss: 0.712542, acc: 0.548828]  [A loss: 1.137792, acc: 0.042969]\n",
      "1634: [D loss: 0.683425, acc: 0.562500]  [A loss: 0.725380, acc: 0.445312]\n",
      "1635: [D loss: 0.710138, acc: 0.511719]  [A loss: 0.965082, acc: 0.140625]\n",
      "1636: [D loss: 0.680045, acc: 0.560547]  [A loss: 0.861729, acc: 0.277344]\n",
      "1637: [D loss: 0.700423, acc: 0.548828]  [A loss: 0.969341, acc: 0.144531]\n",
      "1638: [D loss: 0.667002, acc: 0.607422]  [A loss: 0.887379, acc: 0.207031]\n",
      "1639: [D loss: 0.668097, acc: 0.582031]  [A loss: 0.935004, acc: 0.164062]\n",
      "1640: [D loss: 0.675791, acc: 0.578125]  [A loss: 0.888584, acc: 0.207031]\n",
      "1641: [D loss: 0.673268, acc: 0.587891]  [A loss: 1.065316, acc: 0.074219]\n",
      "1642: [D loss: 0.675215, acc: 0.583984]  [A loss: 0.847240, acc: 0.277344]\n",
      "1643: [D loss: 0.680874, acc: 0.566406]  [A loss: 0.910646, acc: 0.207031]\n",
      "1644: [D loss: 0.696258, acc: 0.558594]  [A loss: 1.056997, acc: 0.085938]\n",
      "1645: [D loss: 0.680968, acc: 0.568359]  [A loss: 0.776481, acc: 0.382812]\n",
      "1646: [D loss: 0.709169, acc: 0.542969]  [A loss: 1.159582, acc: 0.050781]\n",
      "1647: [D loss: 0.679589, acc: 0.595703]  [A loss: 0.668809, acc: 0.609375]\n",
      "1648: [D loss: 0.706424, acc: 0.541016]  [A loss: 1.216955, acc: 0.023438]\n",
      "1649: [D loss: 0.680865, acc: 0.568359]  [A loss: 0.665730, acc: 0.613281]\n",
      "1650: [D loss: 0.699843, acc: 0.556641]  [A loss: 1.062680, acc: 0.074219]\n",
      "1651: [D loss: 0.670706, acc: 0.554688]  [A loss: 0.888247, acc: 0.230469]\n",
      "1652: [D loss: 0.685019, acc: 0.564453]  [A loss: 1.003784, acc: 0.128906]\n",
      "1653: [D loss: 0.675792, acc: 0.576172]  [A loss: 0.829132, acc: 0.308594]\n",
      "1654: [D loss: 0.684658, acc: 0.585938]  [A loss: 1.022488, acc: 0.113281]\n",
      "1655: [D loss: 0.693707, acc: 0.544922]  [A loss: 0.841186, acc: 0.292969]\n",
      "1656: [D loss: 0.682188, acc: 0.568359]  [A loss: 1.052557, acc: 0.113281]\n",
      "1657: [D loss: 0.673094, acc: 0.583984]  [A loss: 0.755550, acc: 0.433594]\n",
      "1658: [D loss: 0.696764, acc: 0.560547]  [A loss: 1.210094, acc: 0.039062]\n",
      "1659: [D loss: 0.673544, acc: 0.576172]  [A loss: 0.712036, acc: 0.500000]\n",
      "1660: [D loss: 0.726050, acc: 0.529297]  [A loss: 1.215540, acc: 0.050781]\n",
      "1661: [D loss: 0.677945, acc: 0.583984]  [A loss: 0.777879, acc: 0.402344]\n",
      "1662: [D loss: 0.687757, acc: 0.572266]  [A loss: 1.226692, acc: 0.039062]\n",
      "1663: [D loss: 0.661332, acc: 0.593750]  [A loss: 0.688196, acc: 0.554688]\n",
      "1664: [D loss: 0.700694, acc: 0.546875]  [A loss: 1.147348, acc: 0.070312]\n",
      "1665: [D loss: 0.696588, acc: 0.548828]  [A loss: 0.761710, acc: 0.398438]\n",
      "1666: [D loss: 0.702421, acc: 0.533203]  [A loss: 1.059108, acc: 0.105469]\n",
      "1667: [D loss: 0.690715, acc: 0.548828]  [A loss: 0.746113, acc: 0.433594]\n",
      "1668: [D loss: 0.684198, acc: 0.558594]  [A loss: 0.971848, acc: 0.156250]\n",
      "1669: [D loss: 0.685122, acc: 0.546875]  [A loss: 0.885819, acc: 0.218750]\n",
      "1670: [D loss: 0.674109, acc: 0.587891]  [A loss: 0.928049, acc: 0.183594]\n",
      "1671: [D loss: 0.675489, acc: 0.582031]  [A loss: 0.871255, acc: 0.210938]\n",
      "1672: [D loss: 0.688603, acc: 0.585938]  [A loss: 0.975688, acc: 0.152344]\n",
      "1673: [D loss: 0.663826, acc: 0.587891]  [A loss: 0.840828, acc: 0.273438]\n",
      "1674: [D loss: 0.684089, acc: 0.593750]  [A loss: 0.979726, acc: 0.125000]\n",
      "1675: [D loss: 0.679245, acc: 0.562500]  [A loss: 0.839698, acc: 0.277344]\n",
      "1676: [D loss: 0.720465, acc: 0.505859]  [A loss: 1.049007, acc: 0.109375]\n",
      "1677: [D loss: 0.659733, acc: 0.587891]  [A loss: 0.724535, acc: 0.468750]\n",
      "1678: [D loss: 0.715773, acc: 0.552734]  [A loss: 1.047677, acc: 0.089844]\n",
      "1679: [D loss: 0.682032, acc: 0.568359]  [A loss: 0.915339, acc: 0.187500]\n",
      "1680: [D loss: 0.681820, acc: 0.570312]  [A loss: 1.060402, acc: 0.082031]\n",
      "1681: [D loss: 0.681631, acc: 0.578125]  [A loss: 0.817698, acc: 0.316406]\n",
      "1682: [D loss: 0.704589, acc: 0.570312]  [A loss: 1.141683, acc: 0.035156]\n",
      "1683: [D loss: 0.673494, acc: 0.562500]  [A loss: 0.763162, acc: 0.425781]\n",
      "1684: [D loss: 0.691794, acc: 0.572266]  [A loss: 1.129048, acc: 0.097656]\n",
      "1685: [D loss: 0.668195, acc: 0.597656]  [A loss: 0.680742, acc: 0.558594]\n",
      "1686: [D loss: 0.726266, acc: 0.505859]  [A loss: 1.240997, acc: 0.035156]\n",
      "1687: [D loss: 0.703010, acc: 0.541016]  [A loss: 0.710543, acc: 0.515625]\n",
      "1688: [D loss: 0.698900, acc: 0.562500]  [A loss: 1.015166, acc: 0.125000]\n",
      "1689: [D loss: 0.690060, acc: 0.525391]  [A loss: 0.774590, acc: 0.378906]\n",
      "1690: [D loss: 0.684805, acc: 0.566406]  [A loss: 1.028307, acc: 0.093750]\n",
      "1691: [D loss: 0.686289, acc: 0.525391]  [A loss: 0.820656, acc: 0.316406]\n",
      "1692: [D loss: 0.703909, acc: 0.554688]  [A loss: 0.957424, acc: 0.156250]\n",
      "1693: [D loss: 0.684041, acc: 0.558594]  [A loss: 0.913987, acc: 0.191406]\n",
      "1694: [D loss: 0.688197, acc: 0.548828]  [A loss: 0.973892, acc: 0.156250]\n",
      "1695: [D loss: 0.689099, acc: 0.562500]  [A loss: 0.939057, acc: 0.171875]\n",
      "1696: [D loss: 0.689675, acc: 0.544922]  [A loss: 0.808814, acc: 0.312500]\n",
      "1697: [D loss: 0.694001, acc: 0.541016]  [A loss: 1.046274, acc: 0.089844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698: [D loss: 0.673393, acc: 0.578125]  [A loss: 0.804418, acc: 0.351562]\n",
      "1699: [D loss: 0.695709, acc: 0.546875]  [A loss: 1.170756, acc: 0.050781]\n",
      "1700: [D loss: 0.691287, acc: 0.560547]  [A loss: 0.628060, acc: 0.683594]\n",
      "1701: [D loss: 0.721151, acc: 0.525391]  [A loss: 1.078279, acc: 0.027344]\n",
      "1702: [D loss: 0.679145, acc: 0.552734]  [A loss: 0.781902, acc: 0.386719]\n",
      "1703: [D loss: 0.707769, acc: 0.537109]  [A loss: 1.034684, acc: 0.074219]\n",
      "1704: [D loss: 0.673612, acc: 0.560547]  [A loss: 0.772685, acc: 0.414062]\n",
      "1705: [D loss: 0.704621, acc: 0.546875]  [A loss: 1.062793, acc: 0.078125]\n",
      "1706: [D loss: 0.674998, acc: 0.562500]  [A loss: 0.764667, acc: 0.394531]\n",
      "1707: [D loss: 0.686366, acc: 0.560547]  [A loss: 1.041511, acc: 0.089844]\n",
      "1708: [D loss: 0.682081, acc: 0.544922]  [A loss: 0.761513, acc: 0.406250]\n",
      "1709: [D loss: 0.694062, acc: 0.556641]  [A loss: 1.064435, acc: 0.085938]\n",
      "1710: [D loss: 0.699424, acc: 0.548828]  [A loss: 0.790033, acc: 0.402344]\n",
      "1711: [D loss: 0.702593, acc: 0.529297]  [A loss: 1.069930, acc: 0.050781]\n",
      "1712: [D loss: 0.684809, acc: 0.576172]  [A loss: 0.784126, acc: 0.371094]\n",
      "1713: [D loss: 0.702355, acc: 0.560547]  [A loss: 1.021400, acc: 0.089844]\n",
      "1714: [D loss: 0.670598, acc: 0.582031]  [A loss: 0.776033, acc: 0.351562]\n",
      "1715: [D loss: 0.704251, acc: 0.548828]  [A loss: 1.112248, acc: 0.039062]\n",
      "1716: [D loss: 0.681739, acc: 0.574219]  [A loss: 0.750378, acc: 0.410156]\n",
      "1717: [D loss: 0.688288, acc: 0.558594]  [A loss: 1.139804, acc: 0.031250]\n",
      "1718: [D loss: 0.686327, acc: 0.578125]  [A loss: 0.737021, acc: 0.457031]\n",
      "1719: [D loss: 0.717355, acc: 0.523438]  [A loss: 1.170238, acc: 0.031250]\n",
      "1720: [D loss: 0.684187, acc: 0.558594]  [A loss: 0.680942, acc: 0.539062]\n",
      "1721: [D loss: 0.707973, acc: 0.544922]  [A loss: 1.038965, acc: 0.109375]\n",
      "1722: [D loss: 0.682940, acc: 0.544922]  [A loss: 0.816599, acc: 0.281250]\n",
      "1723: [D loss: 0.665416, acc: 0.593750]  [A loss: 0.918378, acc: 0.148438]\n",
      "1724: [D loss: 0.694883, acc: 0.535156]  [A loss: 0.859477, acc: 0.292969]\n",
      "1725: [D loss: 0.681362, acc: 0.558594]  [A loss: 0.936472, acc: 0.136719]\n",
      "1726: [D loss: 0.679276, acc: 0.574219]  [A loss: 0.871230, acc: 0.218750]\n",
      "1727: [D loss: 0.697946, acc: 0.533203]  [A loss: 0.947347, acc: 0.144531]\n",
      "1728: [D loss: 0.669454, acc: 0.595703]  [A loss: 0.971331, acc: 0.125000]\n",
      "1729: [D loss: 0.674153, acc: 0.560547]  [A loss: 0.889582, acc: 0.218750]\n",
      "1730: [D loss: 0.682772, acc: 0.552734]  [A loss: 1.053292, acc: 0.093750]\n",
      "1731: [D loss: 0.694769, acc: 0.537109]  [A loss: 0.716396, acc: 0.511719]\n",
      "1732: [D loss: 0.719347, acc: 0.548828]  [A loss: 1.083072, acc: 0.089844]\n",
      "1733: [D loss: 0.684693, acc: 0.556641]  [A loss: 0.695024, acc: 0.542969]\n",
      "1734: [D loss: 0.728988, acc: 0.523438]  [A loss: 1.117983, acc: 0.042969]\n",
      "1735: [D loss: 0.686540, acc: 0.578125]  [A loss: 0.734577, acc: 0.464844]\n",
      "1736: [D loss: 0.717252, acc: 0.503906]  [A loss: 0.978453, acc: 0.117188]\n",
      "1737: [D loss: 0.709753, acc: 0.498047]  [A loss: 0.815624, acc: 0.281250]\n",
      "1738: [D loss: 0.682518, acc: 0.589844]  [A loss: 0.990288, acc: 0.093750]\n",
      "1739: [D loss: 0.682849, acc: 0.566406]  [A loss: 0.783908, acc: 0.382812]\n",
      "1740: [D loss: 0.714072, acc: 0.537109]  [A loss: 1.026540, acc: 0.074219]\n",
      "1741: [D loss: 0.678330, acc: 0.558594]  [A loss: 0.787866, acc: 0.320312]\n",
      "1742: [D loss: 0.698826, acc: 0.525391]  [A loss: 1.061677, acc: 0.058594]\n",
      "1743: [D loss: 0.678532, acc: 0.570312]  [A loss: 0.748938, acc: 0.425781]\n",
      "1744: [D loss: 0.725223, acc: 0.515625]  [A loss: 1.252567, acc: 0.023438]\n",
      "1745: [D loss: 0.685499, acc: 0.562500]  [A loss: 0.653351, acc: 0.636719]\n",
      "1746: [D loss: 0.725231, acc: 0.507812]  [A loss: 0.999614, acc: 0.078125]\n",
      "1747: [D loss: 0.668071, acc: 0.583984]  [A loss: 0.812919, acc: 0.292969]\n",
      "1748: [D loss: 0.713178, acc: 0.537109]  [A loss: 0.890082, acc: 0.207031]\n",
      "1749: [D loss: 0.682540, acc: 0.558594]  [A loss: 0.818106, acc: 0.300781]\n",
      "1750: [D loss: 0.691654, acc: 0.558594]  [A loss: 0.960933, acc: 0.109375]\n",
      "1751: [D loss: 0.686398, acc: 0.556641]  [A loss: 0.857408, acc: 0.195312]\n",
      "1752: [D loss: 0.674169, acc: 0.564453]  [A loss: 0.941813, acc: 0.160156]\n",
      "1753: [D loss: 0.666525, acc: 0.591797]  [A loss: 0.883773, acc: 0.187500]\n",
      "1754: [D loss: 0.673987, acc: 0.568359]  [A loss: 0.911814, acc: 0.191406]\n",
      "1755: [D loss: 0.706400, acc: 0.527344]  [A loss: 0.903110, acc: 0.199219]\n",
      "1756: [D loss: 0.675009, acc: 0.585938]  [A loss: 0.909472, acc: 0.156250]\n",
      "1757: [D loss: 0.689342, acc: 0.578125]  [A loss: 0.951485, acc: 0.132812]\n",
      "1758: [D loss: 0.673320, acc: 0.587891]  [A loss: 0.890284, acc: 0.199219]\n",
      "1759: [D loss: 0.684964, acc: 0.533203]  [A loss: 1.060659, acc: 0.078125]\n",
      "1760: [D loss: 0.683243, acc: 0.560547]  [A loss: 0.732731, acc: 0.421875]\n",
      "1761: [D loss: 0.684386, acc: 0.570312]  [A loss: 1.099873, acc: 0.035156]\n",
      "1762: [D loss: 0.675118, acc: 0.601562]  [A loss: 0.712727, acc: 0.468750]\n",
      "1763: [D loss: 0.706035, acc: 0.529297]  [A loss: 1.151503, acc: 0.046875]\n",
      "1764: [D loss: 0.681327, acc: 0.572266]  [A loss: 0.724527, acc: 0.460938]\n",
      "1765: [D loss: 0.700826, acc: 0.564453]  [A loss: 1.024622, acc: 0.078125]\n",
      "1766: [D loss: 0.688942, acc: 0.531250]  [A loss: 0.743426, acc: 0.417969]\n",
      "1767: [D loss: 0.684934, acc: 0.558594]  [A loss: 0.940360, acc: 0.156250]\n",
      "1768: [D loss: 0.682060, acc: 0.566406]  [A loss: 0.866556, acc: 0.199219]\n",
      "1769: [D loss: 0.685600, acc: 0.562500]  [A loss: 1.003312, acc: 0.089844]\n",
      "1770: [D loss: 0.681741, acc: 0.578125]  [A loss: 0.854540, acc: 0.234375]\n",
      "1771: [D loss: 0.712893, acc: 0.519531]  [A loss: 1.082354, acc: 0.031250]\n",
      "1772: [D loss: 0.674019, acc: 0.582031]  [A loss: 0.782193, acc: 0.398438]\n",
      "1773: [D loss: 0.709041, acc: 0.515625]  [A loss: 1.133531, acc: 0.035156]\n",
      "1774: [D loss: 0.684042, acc: 0.546875]  [A loss: 0.682040, acc: 0.562500]\n",
      "1775: [D loss: 0.715720, acc: 0.523438]  [A loss: 1.006745, acc: 0.058594]\n",
      "1776: [D loss: 0.688001, acc: 0.544922]  [A loss: 0.737385, acc: 0.429688]\n",
      "1777: [D loss: 0.701482, acc: 0.537109]  [A loss: 0.934240, acc: 0.128906]\n",
      "1778: [D loss: 0.710086, acc: 0.492188]  [A loss: 0.767343, acc: 0.359375]\n",
      "1779: [D loss: 0.692937, acc: 0.525391]  [A loss: 0.991986, acc: 0.078125]\n",
      "1780: [D loss: 0.686636, acc: 0.566406]  [A loss: 0.853411, acc: 0.230469]\n",
      "1781: [D loss: 0.696778, acc: 0.525391]  [A loss: 1.007077, acc: 0.070312]\n",
      "1782: [D loss: 0.679231, acc: 0.541016]  [A loss: 0.756936, acc: 0.367188]\n",
      "1783: [D loss: 0.697130, acc: 0.537109]  [A loss: 0.936441, acc: 0.152344]\n",
      "1784: [D loss: 0.681838, acc: 0.552734]  [A loss: 0.807873, acc: 0.289062]\n",
      "1785: [D loss: 0.684097, acc: 0.550781]  [A loss: 0.996218, acc: 0.097656]\n",
      "1786: [D loss: 0.691558, acc: 0.521484]  [A loss: 0.753541, acc: 0.429688]\n",
      "1787: [D loss: 0.699782, acc: 0.558594]  [A loss: 1.044302, acc: 0.062500]\n",
      "1788: [D loss: 0.666155, acc: 0.576172]  [A loss: 0.787841, acc: 0.347656]\n",
      "1789: [D loss: 0.694999, acc: 0.541016]  [A loss: 1.008940, acc: 0.078125]\n",
      "1790: [D loss: 0.678199, acc: 0.558594]  [A loss: 0.781465, acc: 0.332031]\n",
      "1791: [D loss: 0.682631, acc: 0.582031]  [A loss: 1.047227, acc: 0.054688]\n",
      "1792: [D loss: 0.681929, acc: 0.572266]  [A loss: 0.750764, acc: 0.417969]\n",
      "1793: [D loss: 0.690288, acc: 0.535156]  [A loss: 1.030143, acc: 0.093750]\n",
      "1794: [D loss: 0.672197, acc: 0.605469]  [A loss: 0.737329, acc: 0.414062]\n",
      "1795: [D loss: 0.701760, acc: 0.542969]  [A loss: 0.977786, acc: 0.113281]\n",
      "1796: [D loss: 0.680121, acc: 0.601562]  [A loss: 0.752276, acc: 0.429688]\n",
      "1797: [D loss: 0.711167, acc: 0.517578]  [A loss: 1.082726, acc: 0.042969]\n",
      "1798: [D loss: 0.685538, acc: 0.535156]  [A loss: 0.830055, acc: 0.285156]\n",
      "1799: [D loss: 0.696449, acc: 0.521484]  [A loss: 0.892779, acc: 0.207031]\n",
      "1800: [D loss: 0.681071, acc: 0.578125]  [A loss: 0.858132, acc: 0.234375]\n",
      "1801: [D loss: 0.683427, acc: 0.544922]  [A loss: 0.858254, acc: 0.265625]\n",
      "1802: [D loss: 0.692032, acc: 0.541016]  [A loss: 0.939128, acc: 0.175781]\n",
      "1803: [D loss: 0.694567, acc: 0.550781]  [A loss: 0.895350, acc: 0.164062]\n",
      "1804: [D loss: 0.678355, acc: 0.585938]  [A loss: 0.957222, acc: 0.121094]\n",
      "1805: [D loss: 0.694082, acc: 0.525391]  [A loss: 0.915333, acc: 0.160156]\n",
      "1806: [D loss: 0.672845, acc: 0.597656]  [A loss: 0.857924, acc: 0.214844]\n",
      "1807: [D loss: 0.674685, acc: 0.554688]  [A loss: 1.013051, acc: 0.097656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808: [D loss: 0.660110, acc: 0.625000]  [A loss: 0.798556, acc: 0.335938]\n",
      "1809: [D loss: 0.699607, acc: 0.535156]  [A loss: 1.156249, acc: 0.042969]\n",
      "1810: [D loss: 0.697317, acc: 0.558594]  [A loss: 0.659953, acc: 0.640625]\n",
      "1811: [D loss: 0.696530, acc: 0.541016]  [A loss: 1.056361, acc: 0.070312]\n",
      "1812: [D loss: 0.681814, acc: 0.582031]  [A loss: 0.744861, acc: 0.433594]\n",
      "1813: [D loss: 0.693544, acc: 0.546875]  [A loss: 1.058057, acc: 0.058594]\n",
      "1814: [D loss: 0.683150, acc: 0.550781]  [A loss: 0.700091, acc: 0.515625]\n",
      "1815: [D loss: 0.700429, acc: 0.537109]  [A loss: 1.030534, acc: 0.070312]\n",
      "1816: [D loss: 0.664602, acc: 0.595703]  [A loss: 0.748325, acc: 0.441406]\n",
      "1817: [D loss: 0.704146, acc: 0.533203]  [A loss: 0.971457, acc: 0.117188]\n",
      "1818: [D loss: 0.671672, acc: 0.572266]  [A loss: 0.738330, acc: 0.410156]\n",
      "1819: [D loss: 0.696518, acc: 0.541016]  [A loss: 1.013444, acc: 0.074219]\n",
      "1820: [D loss: 0.673661, acc: 0.583984]  [A loss: 0.757928, acc: 0.371094]\n",
      "1821: [D loss: 0.693959, acc: 0.554688]  [A loss: 0.950127, acc: 0.128906]\n",
      "1822: [D loss: 0.680473, acc: 0.566406]  [A loss: 0.867372, acc: 0.234375]\n",
      "1823: [D loss: 0.677057, acc: 0.568359]  [A loss: 0.938099, acc: 0.125000]\n",
      "1824: [D loss: 0.679270, acc: 0.582031]  [A loss: 0.869561, acc: 0.222656]\n",
      "1825: [D loss: 0.689426, acc: 0.548828]  [A loss: 0.905763, acc: 0.167969]\n",
      "1826: [D loss: 0.675738, acc: 0.589844]  [A loss: 0.894004, acc: 0.187500]\n",
      "1827: [D loss: 0.687261, acc: 0.539062]  [A loss: 0.885751, acc: 0.179688]\n",
      "1828: [D loss: 0.677596, acc: 0.580078]  [A loss: 0.900364, acc: 0.191406]\n",
      "1829: [D loss: 0.684542, acc: 0.541016]  [A loss: 0.824836, acc: 0.281250]\n",
      "1830: [D loss: 0.703517, acc: 0.531250]  [A loss: 1.028338, acc: 0.070312]\n",
      "1831: [D loss: 0.684551, acc: 0.554688]  [A loss: 0.692969, acc: 0.535156]\n",
      "1832: [D loss: 0.720531, acc: 0.525391]  [A loss: 1.137390, acc: 0.039062]\n",
      "1833: [D loss: 0.684176, acc: 0.556641]  [A loss: 0.707811, acc: 0.515625]\n",
      "1834: [D loss: 0.717246, acc: 0.517578]  [A loss: 0.968876, acc: 0.117188]\n",
      "1835: [D loss: 0.683379, acc: 0.564453]  [A loss: 0.849294, acc: 0.218750]\n",
      "1836: [D loss: 0.676838, acc: 0.568359]  [A loss: 0.868239, acc: 0.199219]\n",
      "1837: [D loss: 0.686836, acc: 0.550781]  [A loss: 0.904087, acc: 0.167969]\n",
      "1838: [D loss: 0.694004, acc: 0.552734]  [A loss: 0.935788, acc: 0.117188]\n",
      "1839: [D loss: 0.686411, acc: 0.591797]  [A loss: 0.843279, acc: 0.277344]\n",
      "1840: [D loss: 0.685842, acc: 0.572266]  [A loss: 0.968000, acc: 0.109375]\n",
      "1841: [D loss: 0.675178, acc: 0.582031]  [A loss: 0.835707, acc: 0.269531]\n",
      "1842: [D loss: 0.692216, acc: 0.525391]  [A loss: 0.975443, acc: 0.093750]\n",
      "1843: [D loss: 0.682729, acc: 0.572266]  [A loss: 0.791950, acc: 0.339844]\n",
      "1844: [D loss: 0.689955, acc: 0.552734]  [A loss: 1.014741, acc: 0.117188]\n",
      "1845: [D loss: 0.676417, acc: 0.574219]  [A loss: 0.706603, acc: 0.507812]\n",
      "1846: [D loss: 0.716285, acc: 0.521484]  [A loss: 1.031193, acc: 0.093750]\n",
      "1847: [D loss: 0.684692, acc: 0.554688]  [A loss: 0.697900, acc: 0.519531]\n",
      "1848: [D loss: 0.708285, acc: 0.539062]  [A loss: 0.965980, acc: 0.101562]\n",
      "1849: [D loss: 0.671036, acc: 0.580078]  [A loss: 0.796718, acc: 0.355469]\n",
      "1850: [D loss: 0.700862, acc: 0.537109]  [A loss: 0.912977, acc: 0.164062]\n",
      "1851: [D loss: 0.690271, acc: 0.531250]  [A loss: 0.803390, acc: 0.332031]\n",
      "1852: [D loss: 0.706334, acc: 0.501953]  [A loss: 1.030731, acc: 0.089844]\n",
      "1853: [D loss: 0.675865, acc: 0.585938]  [A loss: 0.779085, acc: 0.363281]\n",
      "1854: [D loss: 0.701270, acc: 0.539062]  [A loss: 0.974382, acc: 0.113281]\n",
      "1855: [D loss: 0.696487, acc: 0.546875]  [A loss: 0.786950, acc: 0.300781]\n",
      "1856: [D loss: 0.704151, acc: 0.509766]  [A loss: 0.986365, acc: 0.097656]\n",
      "1857: [D loss: 0.685872, acc: 0.566406]  [A loss: 0.745257, acc: 0.421875]\n",
      "1858: [D loss: 0.695749, acc: 0.533203]  [A loss: 0.915594, acc: 0.160156]\n",
      "1859: [D loss: 0.680710, acc: 0.550781]  [A loss: 0.811277, acc: 0.257812]\n",
      "1860: [D loss: 0.686824, acc: 0.570312]  [A loss: 0.955862, acc: 0.117188]\n",
      "1861: [D loss: 0.678135, acc: 0.560547]  [A loss: 0.873147, acc: 0.226562]\n",
      "1862: [D loss: 0.707688, acc: 0.539062]  [A loss: 0.925619, acc: 0.144531]\n",
      "1863: [D loss: 0.674900, acc: 0.593750]  [A loss: 0.820125, acc: 0.277344]\n",
      "1864: [D loss: 0.691955, acc: 0.544922]  [A loss: 0.967745, acc: 0.089844]\n",
      "1865: [D loss: 0.682656, acc: 0.535156]  [A loss: 0.828560, acc: 0.269531]\n",
      "1866: [D loss: 0.706822, acc: 0.523438]  [A loss: 1.068390, acc: 0.046875]\n",
      "1867: [D loss: 0.681650, acc: 0.558594]  [A loss: 0.756011, acc: 0.355469]\n",
      "1868: [D loss: 0.714027, acc: 0.503906]  [A loss: 1.028610, acc: 0.074219]\n",
      "1869: [D loss: 0.676238, acc: 0.585938]  [A loss: 0.749707, acc: 0.402344]\n",
      "1870: [D loss: 0.687733, acc: 0.550781]  [A loss: 1.073342, acc: 0.042969]\n",
      "1871: [D loss: 0.688870, acc: 0.542969]  [A loss: 0.697971, acc: 0.507812]\n",
      "1872: [D loss: 0.721937, acc: 0.521484]  [A loss: 1.073034, acc: 0.058594]\n",
      "1873: [D loss: 0.680395, acc: 0.574219]  [A loss: 0.729051, acc: 0.445312]\n",
      "1874: [D loss: 0.701244, acc: 0.546875]  [A loss: 0.939173, acc: 0.132812]\n",
      "1875: [D loss: 0.675502, acc: 0.580078]  [A loss: 0.785535, acc: 0.339844]\n",
      "1876: [D loss: 0.691789, acc: 0.517578]  [A loss: 0.864362, acc: 0.210938]\n",
      "1877: [D loss: 0.693490, acc: 0.539062]  [A loss: 0.910732, acc: 0.121094]\n",
      "1878: [D loss: 0.674143, acc: 0.585938]  [A loss: 0.761310, acc: 0.351562]\n",
      "1879: [D loss: 0.683117, acc: 0.578125]  [A loss: 0.958030, acc: 0.121094]\n",
      "1880: [D loss: 0.686520, acc: 0.570312]  [A loss: 0.817729, acc: 0.246094]\n",
      "1881: [D loss: 0.682287, acc: 0.560547]  [A loss: 0.943259, acc: 0.132812]\n",
      "1882: [D loss: 0.675243, acc: 0.562500]  [A loss: 0.808713, acc: 0.343750]\n",
      "1883: [D loss: 0.686549, acc: 0.554688]  [A loss: 0.945001, acc: 0.121094]\n",
      "1884: [D loss: 0.673786, acc: 0.595703]  [A loss: 0.832924, acc: 0.261719]\n",
      "1885: [D loss: 0.674871, acc: 0.589844]  [A loss: 0.917729, acc: 0.128906]\n",
      "1886: [D loss: 0.687516, acc: 0.546875]  [A loss: 0.935706, acc: 0.121094]\n",
      "1887: [D loss: 0.690228, acc: 0.542969]  [A loss: 0.930725, acc: 0.109375]\n",
      "1888: [D loss: 0.700112, acc: 0.527344]  [A loss: 0.846160, acc: 0.218750]\n",
      "1889: [D loss: 0.677238, acc: 0.566406]  [A loss: 0.844074, acc: 0.261719]\n",
      "1890: [D loss: 0.701810, acc: 0.535156]  [A loss: 1.084904, acc: 0.046875]\n",
      "1891: [D loss: 0.682728, acc: 0.583984]  [A loss: 0.659415, acc: 0.597656]\n",
      "1892: [D loss: 0.732477, acc: 0.519531]  [A loss: 1.097932, acc: 0.023438]\n",
      "1893: [D loss: 0.675998, acc: 0.580078]  [A loss: 0.705041, acc: 0.488281]\n",
      "1894: [D loss: 0.716652, acc: 0.523438]  [A loss: 0.899871, acc: 0.210938]\n",
      "1895: [D loss: 0.687533, acc: 0.556641]  [A loss: 0.876359, acc: 0.167969]\n",
      "1896: [D loss: 0.678836, acc: 0.562500]  [A loss: 0.894566, acc: 0.160156]\n",
      "1897: [D loss: 0.686487, acc: 0.550781]  [A loss: 0.886393, acc: 0.144531]\n",
      "1898: [D loss: 0.683566, acc: 0.552734]  [A loss: 0.815825, acc: 0.265625]\n",
      "1899: [D loss: 0.685155, acc: 0.583984]  [A loss: 0.931394, acc: 0.144531]\n",
      "1900: [D loss: 0.675526, acc: 0.578125]  [A loss: 0.869388, acc: 0.171875]\n",
      "1901: [D loss: 0.669777, acc: 0.589844]  [A loss: 0.871225, acc: 0.230469]\n",
      "1902: [D loss: 0.681904, acc: 0.564453]  [A loss: 0.854704, acc: 0.226562]\n",
      "1903: [D loss: 0.672914, acc: 0.591797]  [A loss: 0.923549, acc: 0.140625]\n",
      "1904: [D loss: 0.678096, acc: 0.556641]  [A loss: 0.828184, acc: 0.257812]\n",
      "1905: [D loss: 0.678526, acc: 0.554688]  [A loss: 1.041840, acc: 0.085938]\n",
      "1906: [D loss: 0.675135, acc: 0.576172]  [A loss: 0.672007, acc: 0.546875]\n",
      "1907: [D loss: 0.724750, acc: 0.535156]  [A loss: 1.117110, acc: 0.042969]\n",
      "1908: [D loss: 0.701467, acc: 0.525391]  [A loss: 0.708874, acc: 0.500000]\n",
      "1909: [D loss: 0.696818, acc: 0.542969]  [A loss: 0.949209, acc: 0.132812]\n",
      "1910: [D loss: 0.681045, acc: 0.585938]  [A loss: 0.807547, acc: 0.332031]\n",
      "1911: [D loss: 0.698717, acc: 0.541016]  [A loss: 0.924811, acc: 0.144531]\n",
      "1912: [D loss: 0.671013, acc: 0.607422]  [A loss: 0.808623, acc: 0.296875]\n",
      "1913: [D loss: 0.699748, acc: 0.517578]  [A loss: 0.923156, acc: 0.175781]\n",
      "1914: [D loss: 0.670639, acc: 0.572266]  [A loss: 0.820143, acc: 0.246094]\n",
      "1915: [D loss: 0.686536, acc: 0.562500]  [A loss: 0.872129, acc: 0.203125]\n",
      "1916: [D loss: 0.692940, acc: 0.558594]  [A loss: 0.875079, acc: 0.214844]\n",
      "1917: [D loss: 0.686392, acc: 0.574219]  [A loss: 0.874499, acc: 0.214844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1918: [D loss: 0.673916, acc: 0.589844]  [A loss: 0.915118, acc: 0.152344]\n",
      "1919: [D loss: 0.682523, acc: 0.558594]  [A loss: 1.075171, acc: 0.070312]\n",
      "1920: [D loss: 0.689544, acc: 0.548828]  [A loss: 0.793721, acc: 0.296875]\n",
      "1921: [D loss: 0.687709, acc: 0.564453]  [A loss: 1.015759, acc: 0.097656]\n",
      "1922: [D loss: 0.681408, acc: 0.578125]  [A loss: 0.768438, acc: 0.355469]\n",
      "1923: [D loss: 0.695465, acc: 0.560547]  [A loss: 1.011649, acc: 0.082031]\n",
      "1924: [D loss: 0.677566, acc: 0.574219]  [A loss: 0.786893, acc: 0.347656]\n",
      "1925: [D loss: 0.687088, acc: 0.542969]  [A loss: 1.003292, acc: 0.070312]\n",
      "1926: [D loss: 0.695677, acc: 0.544922]  [A loss: 0.840048, acc: 0.273438]\n",
      "1927: [D loss: 0.683160, acc: 0.554688]  [A loss: 0.947146, acc: 0.093750]\n",
      "1928: [D loss: 0.684554, acc: 0.550781]  [A loss: 0.767243, acc: 0.390625]\n",
      "1929: [D loss: 0.688776, acc: 0.550781]  [A loss: 0.997488, acc: 0.097656]\n",
      "1930: [D loss: 0.674344, acc: 0.546875]  [A loss: 0.790277, acc: 0.335938]\n",
      "1931: [D loss: 0.701784, acc: 0.523438]  [A loss: 1.010070, acc: 0.062500]\n",
      "1932: [D loss: 0.685050, acc: 0.558594]  [A loss: 0.710589, acc: 0.480469]\n",
      "1933: [D loss: 0.688447, acc: 0.558594]  [A loss: 1.010588, acc: 0.101562]\n",
      "1934: [D loss: 0.666576, acc: 0.583984]  [A loss: 0.772303, acc: 0.382812]\n",
      "1935: [D loss: 0.702088, acc: 0.537109]  [A loss: 0.995748, acc: 0.113281]\n",
      "1936: [D loss: 0.687877, acc: 0.562500]  [A loss: 0.881410, acc: 0.207031]\n",
      "1937: [D loss: 0.687189, acc: 0.548828]  [A loss: 0.877866, acc: 0.199219]\n",
      "1938: [D loss: 0.677262, acc: 0.558594]  [A loss: 0.871666, acc: 0.171875]\n",
      "1939: [D loss: 0.698042, acc: 0.527344]  [A loss: 0.868382, acc: 0.183594]\n",
      "1940: [D loss: 0.674444, acc: 0.595703]  [A loss: 0.871068, acc: 0.195312]\n",
      "1941: [D loss: 0.671994, acc: 0.593750]  [A loss: 0.927181, acc: 0.148438]\n",
      "1942: [D loss: 0.698853, acc: 0.548828]  [A loss: 0.874975, acc: 0.195312]\n",
      "1943: [D loss: 0.694273, acc: 0.527344]  [A loss: 0.850656, acc: 0.222656]\n",
      "1944: [D loss: 0.687400, acc: 0.539062]  [A loss: 0.945878, acc: 0.136719]\n",
      "1945: [D loss: 0.670499, acc: 0.599609]  [A loss: 0.838173, acc: 0.269531]\n",
      "1946: [D loss: 0.684244, acc: 0.580078]  [A loss: 0.964082, acc: 0.117188]\n",
      "1947: [D loss: 0.686799, acc: 0.539062]  [A loss: 0.907845, acc: 0.203125]\n",
      "1948: [D loss: 0.693325, acc: 0.523438]  [A loss: 0.858272, acc: 0.242188]\n",
      "1949: [D loss: 0.697188, acc: 0.542969]  [A loss: 0.849413, acc: 0.281250]\n",
      "1950: [D loss: 0.686470, acc: 0.574219]  [A loss: 0.980160, acc: 0.101562]\n",
      "1951: [D loss: 0.674148, acc: 0.582031]  [A loss: 0.773861, acc: 0.359375]\n",
      "1952: [D loss: 0.674424, acc: 0.585938]  [A loss: 1.087996, acc: 0.042969]\n",
      "1953: [D loss: 0.689876, acc: 0.548828]  [A loss: 0.779288, acc: 0.371094]\n",
      "1954: [D loss: 0.714383, acc: 0.501953]  [A loss: 1.122458, acc: 0.019531]\n",
      "1955: [D loss: 0.687916, acc: 0.556641]  [A loss: 0.730491, acc: 0.464844]\n",
      "1956: [D loss: 0.726456, acc: 0.517578]  [A loss: 1.014712, acc: 0.070312]\n",
      "1957: [D loss: 0.680703, acc: 0.554688]  [A loss: 0.705714, acc: 0.500000]\n",
      "1958: [D loss: 0.725860, acc: 0.523438]  [A loss: 0.990976, acc: 0.078125]\n",
      "1959: [D loss: 0.683200, acc: 0.562500]  [A loss: 0.781783, acc: 0.343750]\n",
      "1960: [D loss: 0.681432, acc: 0.531250]  [A loss: 0.892812, acc: 0.187500]\n",
      "1961: [D loss: 0.681762, acc: 0.583984]  [A loss: 0.834518, acc: 0.238281]\n",
      "1962: [D loss: 0.683393, acc: 0.554688]  [A loss: 0.874258, acc: 0.214844]\n",
      "1963: [D loss: 0.686840, acc: 0.531250]  [A loss: 0.863462, acc: 0.242188]\n",
      "1964: [D loss: 0.692977, acc: 0.529297]  [A loss: 0.875701, acc: 0.203125]\n",
      "1965: [D loss: 0.690712, acc: 0.539062]  [A loss: 0.861439, acc: 0.203125]\n",
      "1966: [D loss: 0.682217, acc: 0.570312]  [A loss: 0.876498, acc: 0.242188]\n",
      "1967: [D loss: 0.684494, acc: 0.560547]  [A loss: 0.935552, acc: 0.128906]\n",
      "1968: [D loss: 0.678190, acc: 0.574219]  [A loss: 0.911599, acc: 0.160156]\n",
      "1969: [D loss: 0.684840, acc: 0.587891]  [A loss: 0.880572, acc: 0.203125]\n",
      "1970: [D loss: 0.698444, acc: 0.542969]  [A loss: 0.871663, acc: 0.199219]\n",
      "1971: [D loss: 0.696169, acc: 0.556641]  [A loss: 0.811722, acc: 0.328125]\n",
      "1972: [D loss: 0.684268, acc: 0.568359]  [A loss: 0.918883, acc: 0.187500]\n",
      "1973: [D loss: 0.721716, acc: 0.507812]  [A loss: 0.881663, acc: 0.238281]\n",
      "1974: [D loss: 0.703302, acc: 0.562500]  [A loss: 1.018570, acc: 0.085938]\n",
      "1975: [D loss: 0.674002, acc: 0.576172]  [A loss: 0.698256, acc: 0.484375]\n",
      "1976: [D loss: 0.712386, acc: 0.517578]  [A loss: 1.085181, acc: 0.054688]\n",
      "1977: [D loss: 0.683631, acc: 0.542969]  [A loss: 0.691472, acc: 0.523438]\n",
      "1978: [D loss: 0.704642, acc: 0.562500]  [A loss: 0.967656, acc: 0.085938]\n",
      "1979: [D loss: 0.671322, acc: 0.593750]  [A loss: 0.760477, acc: 0.355469]\n",
      "1980: [D loss: 0.690613, acc: 0.572266]  [A loss: 0.919259, acc: 0.148438]\n",
      "1981: [D loss: 0.696922, acc: 0.533203]  [A loss: 0.822783, acc: 0.285156]\n",
      "1982: [D loss: 0.683439, acc: 0.548828]  [A loss: 0.838474, acc: 0.273438]\n",
      "1983: [D loss: 0.680540, acc: 0.576172]  [A loss: 0.884995, acc: 0.203125]\n",
      "1984: [D loss: 0.688542, acc: 0.558594]  [A loss: 0.825619, acc: 0.281250]\n",
      "1985: [D loss: 0.692810, acc: 0.546875]  [A loss: 0.868107, acc: 0.183594]\n",
      "1986: [D loss: 0.679624, acc: 0.564453]  [A loss: 0.819029, acc: 0.343750]\n",
      "1987: [D loss: 0.702129, acc: 0.517578]  [A loss: 1.010306, acc: 0.082031]\n",
      "1988: [D loss: 0.682690, acc: 0.572266]  [A loss: 0.808099, acc: 0.316406]\n",
      "1989: [D loss: 0.693952, acc: 0.564453]  [A loss: 0.950062, acc: 0.144531]\n",
      "1990: [D loss: 0.683248, acc: 0.544922]  [A loss: 0.833655, acc: 0.265625]\n",
      "1991: [D loss: 0.681353, acc: 0.568359]  [A loss: 0.955170, acc: 0.128906]\n",
      "1992: [D loss: 0.678954, acc: 0.564453]  [A loss: 0.819448, acc: 0.273438]\n",
      "1993: [D loss: 0.696990, acc: 0.560547]  [A loss: 0.973442, acc: 0.132812]\n",
      "1994: [D loss: 0.681728, acc: 0.556641]  [A loss: 0.725559, acc: 0.500000]\n",
      "1995: [D loss: 0.721582, acc: 0.515625]  [A loss: 1.157807, acc: 0.023438]\n",
      "1996: [D loss: 0.678338, acc: 0.548828]  [A loss: 0.695768, acc: 0.507812]\n",
      "1997: [D loss: 0.704408, acc: 0.527344]  [A loss: 1.005366, acc: 0.074219]\n",
      "1998: [D loss: 0.672572, acc: 0.595703]  [A loss: 0.727279, acc: 0.460938]\n",
      "1999: [D loss: 0.705987, acc: 0.558594]  [A loss: 0.915811, acc: 0.132812]\n",
      "2000: [D loss: 0.681403, acc: 0.542969]  [A loss: 0.804843, acc: 0.265625]\n",
      "2001: [D loss: 0.689067, acc: 0.570312]  [A loss: 0.901107, acc: 0.183594]\n",
      "2002: [D loss: 0.689040, acc: 0.554688]  [A loss: 0.835112, acc: 0.269531]\n",
      "2003: [D loss: 0.692460, acc: 0.515625]  [A loss: 0.860646, acc: 0.203125]\n",
      "2004: [D loss: 0.670564, acc: 0.576172]  [A loss: 0.832088, acc: 0.273438]\n",
      "2005: [D loss: 0.700403, acc: 0.531250]  [A loss: 1.029021, acc: 0.078125]\n",
      "2006: [D loss: 0.674023, acc: 0.591797]  [A loss: 0.750688, acc: 0.402344]\n",
      "2007: [D loss: 0.703081, acc: 0.531250]  [A loss: 0.985187, acc: 0.082031]\n",
      "2008: [D loss: 0.674933, acc: 0.589844]  [A loss: 0.752157, acc: 0.394531]\n",
      "2009: [D loss: 0.703307, acc: 0.533203]  [A loss: 0.990319, acc: 0.101562]\n",
      "2010: [D loss: 0.685222, acc: 0.539062]  [A loss: 0.749259, acc: 0.429688]\n",
      "2011: [D loss: 0.689208, acc: 0.550781]  [A loss: 0.938091, acc: 0.121094]\n",
      "2012: [D loss: 0.697126, acc: 0.519531]  [A loss: 0.786054, acc: 0.371094]\n",
      "2013: [D loss: 0.695692, acc: 0.541016]  [A loss: 0.921159, acc: 0.121094]\n",
      "2014: [D loss: 0.678574, acc: 0.566406]  [A loss: 0.816435, acc: 0.234375]\n",
      "2015: [D loss: 0.695495, acc: 0.517578]  [A loss: 0.939681, acc: 0.117188]\n",
      "2016: [D loss: 0.671541, acc: 0.568359]  [A loss: 0.787713, acc: 0.308594]\n",
      "2017: [D loss: 0.706131, acc: 0.542969]  [A loss: 0.999212, acc: 0.070312]\n",
      "2018: [D loss: 0.672003, acc: 0.552734]  [A loss: 0.796711, acc: 0.308594]\n",
      "2019: [D loss: 0.701718, acc: 0.519531]  [A loss: 0.973617, acc: 0.093750]\n",
      "2020: [D loss: 0.680011, acc: 0.580078]  [A loss: 0.780386, acc: 0.347656]\n",
      "2021: [D loss: 0.687280, acc: 0.560547]  [A loss: 0.988679, acc: 0.117188]\n",
      "2022: [D loss: 0.687314, acc: 0.558594]  [A loss: 0.797328, acc: 0.312500]\n",
      "2023: [D loss: 0.690584, acc: 0.554688]  [A loss: 0.897739, acc: 0.152344]\n",
      "2024: [D loss: 0.684340, acc: 0.574219]  [A loss: 0.831986, acc: 0.257812]\n",
      "2025: [D loss: 0.699384, acc: 0.517578]  [A loss: 0.900793, acc: 0.136719]\n",
      "2026: [D loss: 0.668675, acc: 0.583984]  [A loss: 0.830672, acc: 0.261719]\n",
      "2027: [D loss: 0.680048, acc: 0.558594]  [A loss: 0.847715, acc: 0.226562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2028: [D loss: 0.680848, acc: 0.583984]  [A loss: 0.836701, acc: 0.238281]\n",
      "2029: [D loss: 0.689901, acc: 0.552734]  [A loss: 0.914929, acc: 0.152344]\n",
      "2030: [D loss: 0.669767, acc: 0.593750]  [A loss: 0.813922, acc: 0.269531]\n",
      "2031: [D loss: 0.688380, acc: 0.591797]  [A loss: 0.958419, acc: 0.113281]\n",
      "2032: [D loss: 0.686396, acc: 0.548828]  [A loss: 0.784930, acc: 0.375000]\n",
      "2033: [D loss: 0.696911, acc: 0.550781]  [A loss: 0.937421, acc: 0.148438]\n",
      "2034: [D loss: 0.682650, acc: 0.537109]  [A loss: 0.856251, acc: 0.214844]\n",
      "2035: [D loss: 0.706367, acc: 0.537109]  [A loss: 0.959473, acc: 0.097656]\n",
      "2036: [D loss: 0.671911, acc: 0.589844]  [A loss: 0.814787, acc: 0.277344]\n",
      "2037: [D loss: 0.684520, acc: 0.552734]  [A loss: 0.985107, acc: 0.097656]\n",
      "2038: [D loss: 0.682741, acc: 0.568359]  [A loss: 0.788791, acc: 0.339844]\n",
      "2039: [D loss: 0.696777, acc: 0.537109]  [A loss: 1.023396, acc: 0.074219]\n",
      "2040: [D loss: 0.666496, acc: 0.603516]  [A loss: 0.729465, acc: 0.437500]\n",
      "2041: [D loss: 0.697403, acc: 0.568359]  [A loss: 0.985458, acc: 0.117188]\n",
      "2042: [D loss: 0.681033, acc: 0.552734]  [A loss: 0.726011, acc: 0.460938]\n",
      "2043: [D loss: 0.704357, acc: 0.531250]  [A loss: 0.953099, acc: 0.101562]\n",
      "2044: [D loss: 0.696566, acc: 0.535156]  [A loss: 0.778284, acc: 0.343750]\n",
      "2045: [D loss: 0.698386, acc: 0.525391]  [A loss: 0.902797, acc: 0.175781]\n",
      "2046: [D loss: 0.686717, acc: 0.558594]  [A loss: 0.888261, acc: 0.183594]\n",
      "2047: [D loss: 0.688995, acc: 0.541016]  [A loss: 0.904420, acc: 0.136719]\n",
      "2048: [D loss: 0.688763, acc: 0.585938]  [A loss: 0.886531, acc: 0.187500]\n",
      "2049: [D loss: 0.676966, acc: 0.550781]  [A loss: 0.862105, acc: 0.253906]\n",
      "2050: [D loss: 0.681156, acc: 0.587891]  [A loss: 0.826769, acc: 0.277344]\n",
      "2051: [D loss: 0.690465, acc: 0.552734]  [A loss: 1.002345, acc: 0.066406]\n",
      "2052: [D loss: 0.679077, acc: 0.589844]  [A loss: 0.791390, acc: 0.335938]\n",
      "2053: [D loss: 0.685911, acc: 0.537109]  [A loss: 1.031478, acc: 0.062500]\n",
      "2054: [D loss: 0.683025, acc: 0.558594]  [A loss: 0.766049, acc: 0.359375]\n",
      "2055: [D loss: 0.695541, acc: 0.533203]  [A loss: 0.966830, acc: 0.109375]\n",
      "2056: [D loss: 0.682858, acc: 0.544922]  [A loss: 0.705880, acc: 0.476562]\n",
      "2057: [D loss: 0.692457, acc: 0.548828]  [A loss: 0.968121, acc: 0.105469]\n",
      "2058: [D loss: 0.690078, acc: 0.539062]  [A loss: 0.764158, acc: 0.414062]\n",
      "2059: [D loss: 0.692898, acc: 0.564453]  [A loss: 0.962683, acc: 0.113281]\n",
      "2060: [D loss: 0.664138, acc: 0.609375]  [A loss: 0.792180, acc: 0.390625]\n",
      "2061: [D loss: 0.720171, acc: 0.500000]  [A loss: 0.947371, acc: 0.117188]\n",
      "2062: [D loss: 0.670149, acc: 0.570312]  [A loss: 0.838476, acc: 0.296875]\n",
      "2063: [D loss: 0.699729, acc: 0.537109]  [A loss: 1.007648, acc: 0.089844]\n",
      "2064: [D loss: 0.699362, acc: 0.533203]  [A loss: 0.741603, acc: 0.398438]\n",
      "2065: [D loss: 0.685176, acc: 0.541016]  [A loss: 0.937487, acc: 0.125000]\n",
      "2066: [D loss: 0.679812, acc: 0.550781]  [A loss: 0.781471, acc: 0.367188]\n",
      "2067: [D loss: 0.698325, acc: 0.539062]  [A loss: 0.912893, acc: 0.136719]\n",
      "2068: [D loss: 0.697973, acc: 0.529297]  [A loss: 0.760502, acc: 0.410156]\n",
      "2069: [D loss: 0.703161, acc: 0.548828]  [A loss: 1.060898, acc: 0.046875]\n",
      "2070: [D loss: 0.698605, acc: 0.496094]  [A loss: 0.746825, acc: 0.414062]\n",
      "2071: [D loss: 0.706616, acc: 0.521484]  [A loss: 0.952364, acc: 0.089844]\n",
      "2072: [D loss: 0.690341, acc: 0.541016]  [A loss: 0.801889, acc: 0.328125]\n",
      "2073: [D loss: 0.711951, acc: 0.527344]  [A loss: 1.005669, acc: 0.062500]\n",
      "2074: [D loss: 0.668875, acc: 0.558594]  [A loss: 0.741862, acc: 0.398438]\n",
      "2075: [D loss: 0.692560, acc: 0.541016]  [A loss: 0.950498, acc: 0.140625]\n",
      "2076: [D loss: 0.686105, acc: 0.535156]  [A loss: 0.742213, acc: 0.425781]\n",
      "2077: [D loss: 0.697729, acc: 0.554688]  [A loss: 0.938537, acc: 0.113281]\n",
      "2078: [D loss: 0.685833, acc: 0.550781]  [A loss: 0.780578, acc: 0.351562]\n",
      "2079: [D loss: 0.699443, acc: 0.541016]  [A loss: 0.920811, acc: 0.101562]\n",
      "2080: [D loss: 0.675210, acc: 0.583984]  [A loss: 0.766671, acc: 0.363281]\n",
      "2081: [D loss: 0.680727, acc: 0.564453]  [A loss: 0.872067, acc: 0.167969]\n",
      "2082: [D loss: 0.671992, acc: 0.621094]  [A loss: 0.791812, acc: 0.355469]\n",
      "2083: [D loss: 0.679649, acc: 0.570312]  [A loss: 0.854770, acc: 0.238281]\n",
      "2084: [D loss: 0.680814, acc: 0.568359]  [A loss: 0.875896, acc: 0.187500]\n",
      "2085: [D loss: 0.690113, acc: 0.529297]  [A loss: 0.799843, acc: 0.289062]\n",
      "2086: [D loss: 0.687277, acc: 0.562500]  [A loss: 0.887558, acc: 0.171875]\n",
      "2087: [D loss: 0.675217, acc: 0.566406]  [A loss: 0.849018, acc: 0.218750]\n",
      "2088: [D loss: 0.687647, acc: 0.542969]  [A loss: 0.963649, acc: 0.085938]\n",
      "2089: [D loss: 0.679616, acc: 0.566406]  [A loss: 0.820301, acc: 0.257812]\n",
      "2090: [D loss: 0.690923, acc: 0.562500]  [A loss: 0.882739, acc: 0.179688]\n",
      "2091: [D loss: 0.682265, acc: 0.554688]  [A loss: 0.784307, acc: 0.347656]\n",
      "2092: [D loss: 0.696574, acc: 0.570312]  [A loss: 0.989974, acc: 0.074219]\n",
      "2093: [D loss: 0.692357, acc: 0.556641]  [A loss: 0.727323, acc: 0.441406]\n",
      "2094: [D loss: 0.712386, acc: 0.523438]  [A loss: 0.973516, acc: 0.078125]\n",
      "2095: [D loss: 0.677602, acc: 0.583984]  [A loss: 0.758612, acc: 0.355469]\n",
      "2096: [D loss: 0.698422, acc: 0.535156]  [A loss: 0.921240, acc: 0.125000]\n",
      "2097: [D loss: 0.673427, acc: 0.560547]  [A loss: 0.790162, acc: 0.320312]\n",
      "2098: [D loss: 0.692736, acc: 0.548828]  [A loss: 0.877751, acc: 0.156250]\n",
      "2099: [D loss: 0.688140, acc: 0.564453]  [A loss: 0.851016, acc: 0.246094]\n",
      "2100: [D loss: 0.679383, acc: 0.539062]  [A loss: 0.879412, acc: 0.164062]\n",
      "2101: [D loss: 0.696329, acc: 0.578125]  [A loss: 0.868322, acc: 0.199219]\n",
      "2102: [D loss: 0.679711, acc: 0.566406]  [A loss: 0.860569, acc: 0.187500]\n",
      "2103: [D loss: 0.688694, acc: 0.564453]  [A loss: 0.893282, acc: 0.179688]\n",
      "2104: [D loss: 0.682506, acc: 0.542969]  [A loss: 0.829363, acc: 0.265625]\n",
      "2105: [D loss: 0.676368, acc: 0.578125]  [A loss: 0.870068, acc: 0.207031]\n",
      "2106: [D loss: 0.673306, acc: 0.572266]  [A loss: 0.836062, acc: 0.250000]\n",
      "2107: [D loss: 0.679747, acc: 0.570312]  [A loss: 0.857878, acc: 0.234375]\n",
      "2108: [D loss: 0.691098, acc: 0.527344]  [A loss: 0.968696, acc: 0.097656]\n",
      "2109: [D loss: 0.700312, acc: 0.519531]  [A loss: 0.727145, acc: 0.500000]\n",
      "2110: [D loss: 0.705612, acc: 0.505859]  [A loss: 1.186544, acc: 0.027344]\n",
      "2111: [D loss: 0.693051, acc: 0.525391]  [A loss: 0.623521, acc: 0.664062]\n",
      "2112: [D loss: 0.729775, acc: 0.513672]  [A loss: 0.995830, acc: 0.093750]\n",
      "2113: [D loss: 0.690408, acc: 0.544922]  [A loss: 0.794869, acc: 0.308594]\n",
      "2114: [D loss: 0.700637, acc: 0.537109]  [A loss: 0.862770, acc: 0.218750]\n",
      "2115: [D loss: 0.694374, acc: 0.546875]  [A loss: 0.862049, acc: 0.195312]\n",
      "2116: [D loss: 0.683026, acc: 0.548828]  [A loss: 0.834449, acc: 0.222656]\n",
      "2117: [D loss: 0.692282, acc: 0.539062]  [A loss: 0.792331, acc: 0.324219]\n",
      "2118: [D loss: 0.689962, acc: 0.544922]  [A loss: 0.870378, acc: 0.210938]\n",
      "2119: [D loss: 0.692410, acc: 0.535156]  [A loss: 0.822070, acc: 0.265625]\n",
      "2120: [D loss: 0.675666, acc: 0.576172]  [A loss: 0.878198, acc: 0.195312]\n",
      "2121: [D loss: 0.683008, acc: 0.550781]  [A loss: 0.844303, acc: 0.218750]\n",
      "2122: [D loss: 0.674530, acc: 0.576172]  [A loss: 0.822355, acc: 0.285156]\n",
      "2123: [D loss: 0.697649, acc: 0.541016]  [A loss: 0.930156, acc: 0.136719]\n",
      "2124: [D loss: 0.695601, acc: 0.513672]  [A loss: 0.829716, acc: 0.218750]\n",
      "2125: [D loss: 0.685062, acc: 0.560547]  [A loss: 0.903250, acc: 0.164062]\n",
      "2126: [D loss: 0.678744, acc: 0.580078]  [A loss: 0.849982, acc: 0.238281]\n",
      "2127: [D loss: 0.683762, acc: 0.570312]  [A loss: 0.931641, acc: 0.109375]\n",
      "2128: [D loss: 0.698182, acc: 0.546875]  [A loss: 0.845183, acc: 0.261719]\n",
      "2129: [D loss: 0.672927, acc: 0.599609]  [A loss: 0.909183, acc: 0.156250]\n",
      "2130: [D loss: 0.678719, acc: 0.582031]  [A loss: 0.777608, acc: 0.332031]\n",
      "2131: [D loss: 0.706622, acc: 0.511719]  [A loss: 0.923405, acc: 0.113281]\n",
      "2132: [D loss: 0.677962, acc: 0.558594]  [A loss: 0.828594, acc: 0.285156]\n",
      "2133: [D loss: 0.676419, acc: 0.593750]  [A loss: 0.948746, acc: 0.132812]\n",
      "2134: [D loss: 0.679762, acc: 0.562500]  [A loss: 0.883015, acc: 0.222656]\n",
      "2135: [D loss: 0.706117, acc: 0.542969]  [A loss: 0.920109, acc: 0.156250]\n",
      "2136: [D loss: 0.692706, acc: 0.552734]  [A loss: 0.954908, acc: 0.089844]\n",
      "2137: [D loss: 0.680992, acc: 0.570312]  [A loss: 0.813657, acc: 0.320312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2138: [D loss: 0.695466, acc: 0.535156]  [A loss: 0.954253, acc: 0.097656]\n",
      "2139: [D loss: 0.683557, acc: 0.570312]  [A loss: 0.806529, acc: 0.308594]\n",
      "2140: [D loss: 0.688632, acc: 0.539062]  [A loss: 1.017289, acc: 0.050781]\n",
      "2141: [D loss: 0.674810, acc: 0.556641]  [A loss: 0.777344, acc: 0.343750]\n",
      "2142: [D loss: 0.676043, acc: 0.572266]  [A loss: 0.924209, acc: 0.164062]\n",
      "2143: [D loss: 0.681699, acc: 0.599609]  [A loss: 0.780453, acc: 0.375000]\n",
      "2144: [D loss: 0.693947, acc: 0.542969]  [A loss: 0.851920, acc: 0.214844]\n",
      "2145: [D loss: 0.695465, acc: 0.546875]  [A loss: 0.997465, acc: 0.117188]\n",
      "2146: [D loss: 0.679723, acc: 0.570312]  [A loss: 0.700288, acc: 0.535156]\n",
      "2147: [D loss: 0.722314, acc: 0.503906]  [A loss: 1.014814, acc: 0.070312]\n",
      "2148: [D loss: 0.682932, acc: 0.570312]  [A loss: 0.764110, acc: 0.351562]\n",
      "2149: [D loss: 0.688553, acc: 0.576172]  [A loss: 0.902104, acc: 0.128906]\n",
      "2150: [D loss: 0.695307, acc: 0.496094]  [A loss: 0.784372, acc: 0.343750]\n",
      "2151: [D loss: 0.705272, acc: 0.525391]  [A loss: 0.948190, acc: 0.148438]\n",
      "2152: [D loss: 0.701950, acc: 0.521484]  [A loss: 0.797771, acc: 0.292969]\n",
      "2153: [D loss: 0.695802, acc: 0.552734]  [A loss: 0.866506, acc: 0.171875]\n",
      "2154: [D loss: 0.678917, acc: 0.576172]  [A loss: 0.791593, acc: 0.324219]\n",
      "2155: [D loss: 0.683062, acc: 0.542969]  [A loss: 0.907196, acc: 0.167969]\n",
      "2156: [D loss: 0.686258, acc: 0.544922]  [A loss: 0.780748, acc: 0.351562]\n",
      "2157: [D loss: 0.682463, acc: 0.591797]  [A loss: 0.837570, acc: 0.277344]\n",
      "2158: [D loss: 0.685297, acc: 0.554688]  [A loss: 0.884823, acc: 0.171875]\n",
      "2159: [D loss: 0.694406, acc: 0.533203]  [A loss: 0.816365, acc: 0.304688]\n",
      "2160: [D loss: 0.683747, acc: 0.578125]  [A loss: 0.953476, acc: 0.093750]\n",
      "2161: [D loss: 0.678727, acc: 0.583984]  [A loss: 0.776206, acc: 0.382812]\n",
      "2162: [D loss: 0.691694, acc: 0.562500]  [A loss: 0.941398, acc: 0.117188]\n",
      "2163: [D loss: 0.661574, acc: 0.591797]  [A loss: 0.768410, acc: 0.394531]\n",
      "2164: [D loss: 0.701296, acc: 0.529297]  [A loss: 1.006536, acc: 0.097656]\n",
      "2165: [D loss: 0.664955, acc: 0.601562]  [A loss: 0.767675, acc: 0.378906]\n",
      "2166: [D loss: 0.685263, acc: 0.558594]  [A loss: 0.902079, acc: 0.167969]\n",
      "2167: [D loss: 0.688257, acc: 0.576172]  [A loss: 0.784479, acc: 0.324219]\n",
      "2168: [D loss: 0.684312, acc: 0.531250]  [A loss: 0.893981, acc: 0.171875]\n",
      "2169: [D loss: 0.672949, acc: 0.582031]  [A loss: 0.831259, acc: 0.261719]\n",
      "2170: [D loss: 0.678491, acc: 0.560547]  [A loss: 0.880060, acc: 0.203125]\n",
      "2171: [D loss: 0.691846, acc: 0.546875]  [A loss: 0.862325, acc: 0.199219]\n",
      "2172: [D loss: 0.679333, acc: 0.566406]  [A loss: 0.831832, acc: 0.273438]\n",
      "2173: [D loss: 0.674485, acc: 0.597656]  [A loss: 0.888137, acc: 0.214844]\n",
      "2174: [D loss: 0.683958, acc: 0.541016]  [A loss: 0.798160, acc: 0.339844]\n",
      "2175: [D loss: 0.711789, acc: 0.542969]  [A loss: 0.956407, acc: 0.082031]\n",
      "2176: [D loss: 0.686274, acc: 0.574219]  [A loss: 0.846694, acc: 0.210938]\n",
      "2177: [D loss: 0.674330, acc: 0.576172]  [A loss: 0.902292, acc: 0.136719]\n",
      "2178: [D loss: 0.700078, acc: 0.541016]  [A loss: 0.818252, acc: 0.312500]\n",
      "2179: [D loss: 0.688310, acc: 0.539062]  [A loss: 1.009076, acc: 0.109375]\n",
      "2180: [D loss: 0.672396, acc: 0.576172]  [A loss: 0.714616, acc: 0.488281]\n",
      "2181: [D loss: 0.698875, acc: 0.552734]  [A loss: 1.111081, acc: 0.039062]\n",
      "2182: [D loss: 0.695385, acc: 0.515625]  [A loss: 0.697755, acc: 0.511719]\n",
      "2183: [D loss: 0.719612, acc: 0.501953]  [A loss: 0.967996, acc: 0.140625]\n",
      "2184: [D loss: 0.704936, acc: 0.523438]  [A loss: 0.784854, acc: 0.308594]\n",
      "2185: [D loss: 0.684894, acc: 0.576172]  [A loss: 0.926511, acc: 0.152344]\n",
      "2186: [D loss: 0.680813, acc: 0.568359]  [A loss: 0.758190, acc: 0.414062]\n",
      "2187: [D loss: 0.693529, acc: 0.539062]  [A loss: 0.911128, acc: 0.136719]\n",
      "2188: [D loss: 0.685782, acc: 0.537109]  [A loss: 0.863920, acc: 0.199219]\n",
      "2189: [D loss: 0.687957, acc: 0.564453]  [A loss: 0.875063, acc: 0.183594]\n",
      "2190: [D loss: 0.680486, acc: 0.585938]  [A loss: 0.848093, acc: 0.207031]\n",
      "2191: [D loss: 0.674712, acc: 0.576172]  [A loss: 0.866486, acc: 0.218750]\n",
      "2192: [D loss: 0.683019, acc: 0.558594]  [A loss: 0.863752, acc: 0.226562]\n",
      "2193: [D loss: 0.683266, acc: 0.529297]  [A loss: 0.820900, acc: 0.273438]\n",
      "2194: [D loss: 0.679803, acc: 0.564453]  [A loss: 0.904947, acc: 0.156250]\n",
      "2195: [D loss: 0.696539, acc: 0.541016]  [A loss: 0.820817, acc: 0.277344]\n",
      "2196: [D loss: 0.694598, acc: 0.533203]  [A loss: 0.905379, acc: 0.152344]\n",
      "2197: [D loss: 0.678427, acc: 0.576172]  [A loss: 0.805144, acc: 0.312500]\n",
      "2198: [D loss: 0.705724, acc: 0.529297]  [A loss: 1.037887, acc: 0.039062]\n",
      "2199: [D loss: 0.669960, acc: 0.591797]  [A loss: 0.714601, acc: 0.503906]\n",
      "2200: [D loss: 0.701566, acc: 0.521484]  [A loss: 1.049567, acc: 0.050781]\n",
      "2201: [D loss: 0.692095, acc: 0.533203]  [A loss: 0.720492, acc: 0.484375]\n",
      "2202: [D loss: 0.688763, acc: 0.544922]  [A loss: 0.946811, acc: 0.097656]\n",
      "2203: [D loss: 0.683418, acc: 0.568359]  [A loss: 0.782317, acc: 0.351562]\n",
      "2204: [D loss: 0.694406, acc: 0.533203]  [A loss: 0.913623, acc: 0.160156]\n",
      "2205: [D loss: 0.691654, acc: 0.552734]  [A loss: 0.822785, acc: 0.273438]\n",
      "2206: [D loss: 0.691288, acc: 0.556641]  [A loss: 0.952859, acc: 0.093750]\n",
      "2207: [D loss: 0.686903, acc: 0.552734]  [A loss: 0.789134, acc: 0.281250]\n",
      "2208: [D loss: 0.710638, acc: 0.527344]  [A loss: 0.888162, acc: 0.156250]\n",
      "2209: [D loss: 0.678564, acc: 0.583984]  [A loss: 0.781088, acc: 0.328125]\n",
      "2210: [D loss: 0.684541, acc: 0.568359]  [A loss: 0.889122, acc: 0.167969]\n",
      "2211: [D loss: 0.673855, acc: 0.589844]  [A loss: 0.795911, acc: 0.363281]\n",
      "2212: [D loss: 0.700284, acc: 0.525391]  [A loss: 0.939263, acc: 0.105469]\n",
      "2213: [D loss: 0.673618, acc: 0.582031]  [A loss: 0.759585, acc: 0.378906]\n",
      "2214: [D loss: 0.681723, acc: 0.552734]  [A loss: 0.956683, acc: 0.156250]\n",
      "2215: [D loss: 0.691166, acc: 0.525391]  [A loss: 0.757719, acc: 0.371094]\n",
      "2216: [D loss: 0.702132, acc: 0.550781]  [A loss: 0.901704, acc: 0.160156]\n",
      "2217: [D loss: 0.683837, acc: 0.560547]  [A loss: 0.774585, acc: 0.320312]\n",
      "2218: [D loss: 0.692331, acc: 0.558594]  [A loss: 0.922086, acc: 0.136719]\n",
      "2219: [D loss: 0.666861, acc: 0.609375]  [A loss: 0.834485, acc: 0.238281]\n",
      "2220: [D loss: 0.683469, acc: 0.541016]  [A loss: 0.967889, acc: 0.097656]\n",
      "2221: [D loss: 0.684615, acc: 0.544922]  [A loss: 0.783807, acc: 0.359375]\n",
      "2222: [D loss: 0.681847, acc: 0.582031]  [A loss: 0.958470, acc: 0.093750]\n",
      "2223: [D loss: 0.688194, acc: 0.535156]  [A loss: 0.792074, acc: 0.328125]\n",
      "2224: [D loss: 0.699430, acc: 0.535156]  [A loss: 0.916071, acc: 0.125000]\n",
      "2225: [D loss: 0.684518, acc: 0.544922]  [A loss: 0.793656, acc: 0.316406]\n",
      "2226: [D loss: 0.701374, acc: 0.484375]  [A loss: 0.957236, acc: 0.074219]\n",
      "2227: [D loss: 0.690236, acc: 0.550781]  [A loss: 0.705296, acc: 0.507812]\n",
      "2228: [D loss: 0.700032, acc: 0.537109]  [A loss: 1.011057, acc: 0.046875]\n",
      "2229: [D loss: 0.673935, acc: 0.572266]  [A loss: 0.732577, acc: 0.425781]\n",
      "2230: [D loss: 0.689692, acc: 0.562500]  [A loss: 0.972252, acc: 0.089844]\n",
      "2231: [D loss: 0.679404, acc: 0.580078]  [A loss: 0.758002, acc: 0.406250]\n",
      "2232: [D loss: 0.683715, acc: 0.560547]  [A loss: 0.859067, acc: 0.203125]\n",
      "2233: [D loss: 0.681999, acc: 0.556641]  [A loss: 0.886515, acc: 0.175781]\n",
      "2234: [D loss: 0.687159, acc: 0.531250]  [A loss: 0.811985, acc: 0.292969]\n",
      "2235: [D loss: 0.689273, acc: 0.556641]  [A loss: 0.886635, acc: 0.167969]\n",
      "2236: [D loss: 0.678808, acc: 0.582031]  [A loss: 0.852123, acc: 0.210938]\n",
      "2237: [D loss: 0.681126, acc: 0.574219]  [A loss: 0.831917, acc: 0.253906]\n",
      "2238: [D loss: 0.683503, acc: 0.560547]  [A loss: 0.820281, acc: 0.261719]\n",
      "2239: [D loss: 0.682194, acc: 0.570312]  [A loss: 0.812909, acc: 0.281250]\n",
      "2240: [D loss: 0.692747, acc: 0.562500]  [A loss: 0.964953, acc: 0.093750]\n",
      "2241: [D loss: 0.680518, acc: 0.566406]  [A loss: 0.808960, acc: 0.285156]\n",
      "2242: [D loss: 0.700346, acc: 0.550781]  [A loss: 0.796379, acc: 0.304688]\n",
      "2243: [D loss: 0.702640, acc: 0.552734]  [A loss: 1.068550, acc: 0.050781]\n",
      "2244: [D loss: 0.690495, acc: 0.539062]  [A loss: 0.723218, acc: 0.453125]\n",
      "2245: [D loss: 0.695232, acc: 0.523438]  [A loss: 0.935430, acc: 0.101562]\n",
      "2246: [D loss: 0.683542, acc: 0.554688]  [A loss: 0.786415, acc: 0.328125]\n",
      "2247: [D loss: 0.681578, acc: 0.564453]  [A loss: 0.895603, acc: 0.160156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248: [D loss: 0.675444, acc: 0.589844]  [A loss: 0.842280, acc: 0.218750]\n",
      "2249: [D loss: 0.679892, acc: 0.546875]  [A loss: 0.892090, acc: 0.195312]\n",
      "2250: [D loss: 0.680325, acc: 0.552734]  [A loss: 0.863493, acc: 0.199219]\n",
      "2251: [D loss: 0.691494, acc: 0.529297]  [A loss: 0.834850, acc: 0.203125]\n",
      "2252: [D loss: 0.685713, acc: 0.544922]  [A loss: 0.891213, acc: 0.171875]\n",
      "2253: [D loss: 0.678677, acc: 0.560547]  [A loss: 0.764675, acc: 0.347656]\n",
      "2254: [D loss: 0.689819, acc: 0.568359]  [A loss: 0.862345, acc: 0.222656]\n",
      "2255: [D loss: 0.673603, acc: 0.589844]  [A loss: 0.831793, acc: 0.253906]\n",
      "2256: [D loss: 0.680282, acc: 0.576172]  [A loss: 0.892747, acc: 0.167969]\n",
      "2257: [D loss: 0.686023, acc: 0.556641]  [A loss: 0.810336, acc: 0.281250]\n",
      "2258: [D loss: 0.700601, acc: 0.552734]  [A loss: 0.888649, acc: 0.160156]\n",
      "2259: [D loss: 0.676869, acc: 0.570312]  [A loss: 0.848757, acc: 0.218750]\n",
      "2260: [D loss: 0.679662, acc: 0.550781]  [A loss: 0.914767, acc: 0.160156]\n",
      "2261: [D loss: 0.690130, acc: 0.529297]  [A loss: 0.822784, acc: 0.308594]\n",
      "2262: [D loss: 0.700151, acc: 0.544922]  [A loss: 0.913282, acc: 0.144531]\n",
      "2263: [D loss: 0.689938, acc: 0.548828]  [A loss: 0.804246, acc: 0.308594]\n",
      "2264: [D loss: 0.699577, acc: 0.517578]  [A loss: 0.920497, acc: 0.160156]\n",
      "2265: [D loss: 0.695372, acc: 0.519531]  [A loss: 0.796036, acc: 0.289062]\n",
      "2266: [D loss: 0.682792, acc: 0.568359]  [A loss: 0.987167, acc: 0.085938]\n",
      "2267: [D loss: 0.688537, acc: 0.566406]  [A loss: 0.690777, acc: 0.550781]\n",
      "2268: [D loss: 0.695450, acc: 0.539062]  [A loss: 1.002227, acc: 0.105469]\n",
      "2269: [D loss: 0.695653, acc: 0.539062]  [A loss: 0.765056, acc: 0.382812]\n",
      "2270: [D loss: 0.695010, acc: 0.546875]  [A loss: 0.890191, acc: 0.160156]\n",
      "2271: [D loss: 0.686465, acc: 0.550781]  [A loss: 0.839970, acc: 0.210938]\n",
      "2272: [D loss: 0.692846, acc: 0.552734]  [A loss: 0.854363, acc: 0.214844]\n",
      "2273: [D loss: 0.692812, acc: 0.558594]  [A loss: 0.841429, acc: 0.230469]\n",
      "2274: [D loss: 0.678542, acc: 0.562500]  [A loss: 0.863683, acc: 0.183594]\n",
      "2275: [D loss: 0.686579, acc: 0.558594]  [A loss: 0.884593, acc: 0.136719]\n",
      "2276: [D loss: 0.675469, acc: 0.591797]  [A loss: 0.878479, acc: 0.210938]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-21af4e4ea01f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0mmnist_dcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNIST_DCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0mtimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElapsedTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m \u001b[0mmnist_dcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-21af4e4ea01f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_steps, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0ma_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversarial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mlog_mesg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%d: [D loss: %f, acc: %f]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mlog_mesg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s  [A loss: %f, acc: %f]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog_mesg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m    953\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m    954\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (WF+2P)/S+1\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "#         with tf.device('/gpu:1'):\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        # In: 28 x 28 x 1, depth = 1\n",
    "        # Out: 14 x 14 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "#         with tf.device('/gpu:1'):\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "#         with tf.device('/gpu:1'):\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "#         with tf.device('/gpu:1'):\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = input_data.read_data_sets(\"mnist\",\\\n",
    "                one_hot=True).train.images\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\n",
    "                self.img_cols, 1).astype(np.float32)\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0, self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "\n",
    "\n",
    "mnist_dcgan = MNIST_DCGAN()\n",
    "timer = ElapsedTimer()\n",
    "mnist_dcgan.train(train_steps=10000, batch_size=256, save_interval=500)\n",
    "timer.elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
