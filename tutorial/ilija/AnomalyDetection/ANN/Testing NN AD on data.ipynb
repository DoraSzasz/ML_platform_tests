{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Neural Network based Anomaly Detection on actual data\n",
    "\n",
    "  This code reads PerfSONAR measured packet loss rates between a specified endpoint and all other endpoints in a selected time range. It tries to train neural network to distinguish measurements belonging to the timebin under investigation from measurements in a reference time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "from pandas.tseries.offsets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = 20 # not use all the series for the tutorial\n",
    "\n",
    "start_date = '2017-05-13 00:00:00'\n",
    "end_date = '2017-05-16 23:59:59'\n",
    "\n",
    "# tuning parameters\n",
    "ref = 24\n",
    "sub = 1\n",
    "chance = ref/(sub+ref)\n",
    "cut = chance + (1-chance) * 0.05\n",
    "print('chance:',chance, '\\tcut:', cut)\n",
    "ref = ref * Hour()\n",
    "sub = sub * Hour()\n",
    "\n",
    "\n",
    "srcSiteOWDServer = \"128.142.223.247\" # CERN site\n",
    "\n",
    "# destSiteOWDServer = \"193.109.172.188\"  # pic site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data from ES\n",
    "\n",
    "we connect to elasticsearch, create query and execute scan. Query requires three things: data must be in the given timerange, must be measured by the selected endpoint and be packet loss data. Actual data access does not happen here but in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(['atlas-kibana.mwt2.org:9200'],timeout=60)\n",
    "indices = \"network_weather-2017.*\"\n",
    "\n",
    "start = pd.Timestamp(start_date)\n",
    "end   = pd.Timestamp(end_date)\n",
    "\n",
    "my_query = {\n",
    "    'query': { \n",
    "       'bool':{\n",
    "            'must':[\n",
    "                    {'range': {'timestamp': {'gte': start.strftime('%Y%m%dT%H%M00Z'), 'lt': end.strftime('%Y%m%dT%H%M00Z')}}},\n",
    "                    {'term': {'src': srcSiteOWDServer}},\n",
    "#                     {'term': {'dest': destSiteOWDServer}},\n",
    "                    {'term': {'_type': 'packet_loss_rate'}}\n",
    "                    ]\n",
    "               }\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "scroll = scan(client=es, index=indices, query=my_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "This is the slowest part. It reads ~5k documents per second and will load 1M documents. Expect wait time of ~1 minutes. Actual time might vary depending on your connection and how busy is the Elasticsearch cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "allData={} # will be like this: {'dest_host':[[timestamp],[value]], ...} \n",
    "for res in scroll:\n",
    "#     if count<2: print(res) \n",
    "    if not count%100000: print(count)\n",
    " #   if count>1000000: break\n",
    "    dst = res['_source']['dest'] # old data - dest, new data - dest_host\n",
    "    if dst not in allData: allData[dst]=[[],[]]\n",
    "    allData[dst][0].append(res['_source']['timestamp'] )\n",
    "    allData[dst][1].append(res['_source']['packet_loss'])\n",
    "    \n",
    "    count=count+1\n",
    "\n",
    "dfs=[]\n",
    "for dest,data in allData.items():\n",
    "    ts=pd.to_datetime(data[0],unit='ms')\n",
    "    df=pd.DataFrame({dest:data[1]}, index=ts )\n",
    "    df.sort_index(inplace=True)\n",
    "    df.index = df.index.map(lambda t: t.replace(second=0))\n",
    "    df = df[~df.index.duplicated(keep='last')]\n",
    "    dfs.append(df)\n",
    "    #print(df.head(2))\n",
    "\n",
    "print(count, \"\\nData loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puts together data from different links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)\n",
    "full_df.head()\n",
    "#print(full_df.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot timeseries\n",
    "\n",
    "only a subset of all the links will be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.iloc[:,0:n_series].plot(figsize=(20,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Network Model\n",
    "\n",
    "only class is defined, no output is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    def __init__(self, n_series):\n",
    "        self.n_series = n_series\n",
    "        self.df = None\n",
    "        self.auc_df = None\n",
    "        \n",
    "        self.nn = Sequential()\n",
    "        self.nn.add(Dense(units=n_series*2, input_shape=(n_series,), activation='relu' ))\n",
    "#       self.nn.add(Dropout(0.5))\n",
    "        self.nn.add(Dense(units=n_series, activation='relu'))\n",
    "#       self.nn.add(Dropout(0.5))\n",
    "        self.nn.add(Dense(units=1, activation='sigmoid'))\n",
    "#       self.nn.compile(loss='hinge', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "#       self.nn.compile(loss='mse',optimizer='rmsprop', metrics=['accuracy'])\n",
    "        self.nn.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "#       self.nn.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "        self.nn.summary()\n",
    "    \n",
    "    def set_data(self, df, auc_df):\n",
    "        self.df = df\n",
    "        self.auc_df = auc_df\n",
    "    \n",
    "    def plot_hist(self, hist):\n",
    "        es=len(hist.history['loss'])\n",
    "        x = np.linspace(0,es-1,es)\n",
    "        plt.plot(x, hist.history['loss'], '--', linewidth=2, label='loss')\n",
    "        plt.plot(x, hist.history['acc'], '-', linewidth=2, label='acc')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def check_for_anomaly(self,ref, sub, count):\n",
    "    \n",
    "        y_ref = pd.Series([0] * ref.shape[0])\n",
    "        X_ref = ref\n",
    "    \n",
    "        y_sub = pd.Series([1] * sub.shape[0])\n",
    "        X_sub = sub\n",
    "        \n",
    "        # separate Reference and Subject into Train and Test\n",
    "        X_ref_train, X_ref_test, y_ref_train, y_ref_test = train_test_split(X_ref, y_ref, test_size=0.3, random_state=42)\n",
    "        X_sub_train, X_sub_test, y_sub_train, y_sub_test = train_test_split(X_sub, y_sub, test_size=0.3, random_state=42)\n",
    "    \n",
    "        # combine training ref and sub samples\n",
    "        X_train = pd.concat([X_ref_train, X_sub_train])\n",
    "        y_train = pd.concat([y_ref_train, y_sub_train])\n",
    "\n",
    "        # combine testing ref and sub samples\n",
    "        X_test = pd.concat([X_ref_test, X_sub_test])\n",
    "        y_test = pd.concat([y_ref_test, y_sub_test])\n",
    "    \n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "    \n",
    "        X_train_s, y_train_s = shuffle(X_train, y_train)\n",
    "    \n",
    "        hist = self.nn.fit(X_train_s.values, y_train_s.values, epochs=100, verbose=0, shuffle=True, batch_size=10)\n",
    "        loss_and_metrics = self.nn.evaluate(X_test.values, y_test.values)#, batch_size=256)\n",
    "        #print(loss_and_metrics)\n",
    "    \n",
    "        if loss_and_metrics[1] > cut or not count%5: \n",
    "            self.plot_hist(hist)\n",
    "        \n",
    "        return scaled_accuracy(loss_and_metrics[1], ref.shape[0], sub.shape[0])\n",
    "    \n",
    "    \n",
    "    def loop_over_intervals(self):\n",
    "        lstart = self.df.index.min()\n",
    "        lend = self.df.index.max()\n",
    "\n",
    "        #round start \n",
    "        lstart.seconds=0\n",
    "        lstart.minutes=0\n",
    "\n",
    "        # loop over them\n",
    "        ti = lstart + ref + sub\n",
    "        count = 0\n",
    "        while ti < lend + 1 * Minute():\n",
    "            print(count)\n",
    "            startt = time()\n",
    "            ref_start = ti-ref-sub\n",
    "            ref_end = ti-sub\n",
    "            ref_df = self.df[(self.df.index >= ref_start) & (self.df.index < ref_end)]\n",
    "            sub_df = self.df[(self.df.index >= ref_end) & (self.df.index < ti)]\n",
    "            score = self.check_for_anomaly(ref_df, sub_df, count)\n",
    "            self.auc_df.loc[(self.auc_df.index >= ref_end) & (self.auc_df.index < ti), ['score']]  = score\n",
    "            print('\\n',ti,\"\\trefes:\" , ref_df.shape, \"\\tsubjects:\", sub_df.shape, '\\tacc:', score)\n",
    "            ti = ti + sub\n",
    "            print(\"took:\", time()-startt)\n",
    "            count = count + 1\n",
    "            #if count>2: break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaled_accuracy(accuracy, ref_samples, sub_samples):\n",
    "    chance = float(ref_samples)/(ref_samples+sub_samples)\n",
    "    rescale = 1/(1 - chance)\n",
    "    return (accuracy-chance)*rescale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full_df.interpolate(method='nearest', axis=0, inplace=True)\n",
    "full_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select part of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = full_df.iloc[:,0:n_series]\n",
    "auc_df = pd.DataFrame(np.nan, index=df.index, columns=['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually create the object, give it a data, run anomally detection. \n",
    "This part can take significant time. It takes 10-30 seconds per hour of data analyzed. Total number of steps will be equal to number of subject intervals in the period tested. For every 5th step and  intervals where anomaly has been detected ROC curve will be shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = ANN(n_series)\n",
    "ann.set_data(df, auc_df)\n",
    "ann.loop_over_intervals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot again full timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.plot(figsize=(20,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shade regions where an anomaly has been dected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "auc_df['Detected'] = 0\n",
    "auc_df.loc[auc_df.score>0.05, ['Detected']]=1\n",
    "ax.plot( auc_df.score,'g')\n",
    "ax.fill( auc_df.Detected, 'b', alpha=0.3)\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
